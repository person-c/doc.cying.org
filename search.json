[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ".",
    "section": "",
    "text": "This is a Quarto website. see Notes here\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "note/2023-10-15-data-transformation/index.html",
    "href": "note/2023-10-15-data-transformation/index.html",
    "title": "Data Tricks",
    "section": "",
    "text": "对数变换：即将原始数据\\(X\\)的对数值作为新的分析数据：\n\\[\nX' = lgX\n\\]\n当原始数据中有小于1及零的数据时，亦可取\n\\[\nX' = lg(X + 1)\n\\]\n还可根据需要选用\n\\[\nX' = lg(X + k) \\ 或 \\ X' = lg(k - X)\n\\]\n对数变化常用于：1. 使服从对数正态的数据正态化。如环境中某些污染物的分布，人体中某些微量元素的分布等，可用对数比变换改善其正态性。2. 使数据达到方差齐性，特别是各样本的标准差与均数成比例或变异系数CV接近一个常数时。\n平方根变换: 即将原始数据\\(X\\)的平方根作为新的分析数据：\n\\[\nX' = \\sqrt{X}\n\\]\n当原始数据有小值或零值时，亦可用\n\\[\nX' = \\sqrt{X + 0.5}\n\\]\n平方根变换常用于：1. 使服从Possion分布的计数资料或轻度偏态的资料正态化，例如放射性物质在单位时间内的放射次数，某些发病率较低的疾病在时间或地域上的发病例数等，可用平方根变换使其正态化。2. 当样本的方差与均数正相关时，可使资料达到方差齐性。\n倒数变换: 即将原始数据\\(X\\)的倒数作为新的分析数据：\n\\[\nX' = \\frac{1}{X}\n\\]\n倒数变换常用于数据两端波动较大的资料，可使极端值的影响减小。"
  },
  {
    "objectID": "note/2023-10-15-data-transformation/index.html#删除元组",
    "href": "note/2023-10-15-data-transformation/index.html#删除元组",
    "title": "Data Tricks",
    "section": "删除元组",
    "text": "删除元组\n也就是将存在遗漏信息属性值的对象（元组，记录）删除，从而得到一个完备的信息表 。这种方法简单易行， 在对象有多个属性缺失值、被删除的含缺失值的对象与初始数据集的数据量相比非常小的情况下非常有效，类标号缺失时通常使用该方法。\n然而，这种方法却有很大的局限性。 它以减少历史数据来换取信息的完备，会丢弃大量隐藏在这些对象中的信息。 在初始数据集包含的对象很少的情况下，删除少量对象足以严重影响信息的客观性和结果的正确性；因此，当缺失数据所占比例较大，特别当遗漏数据非随机分布时，这种方法可能导致数据发生偏离，从而引出错误的结论。\n说明:删除元组，或者直接删除该列特征，有时候会导致性能下降。"
  },
  {
    "objectID": "note/2023-10-15-data-transformation/index.html#数据补齐",
    "href": "note/2023-10-15-data-transformation/index.html#数据补齐",
    "title": "Data Tricks",
    "section": "数据补齐",
    "text": "数据补齐\n这类方法是用一定的值去填充空值，从而使信息表完备化。通常基于统计学原理， 根据初始数据集中其余对象取值的分布情况来对一个缺失值进行填充 。数据挖掘中常用的有以下几种补齐方法：\n人工填写（filling manually) 由于最了解数据的还是用户自己，因此这个方法产生数据偏离最小，可能是填充效果最好的一种。然而一般来说，该方法很费时，当数据规模很大、空值很多的时候，该方法是不可行的。\n特殊值填充（Treating Missing Attribute values as Special values） 将空值作为一种特殊的属性值来处理，它不同于其他的任何属性值 。如所有的空值都用“unknown”填充。这样将形成另一个有趣的概念， 可能导致严重的数据偏离，一般不推荐使用 。\n平均值填充（Mean/Mode Completer) 将初始数据集中的属性分为数值属性和非数值属性来分别进行处理 。 如果空值是数值型的，就根据该属性在其他所有对象的取值的平均值来填充该缺失的属性值； 如果空值是非数值型的，就根据统计学中的众数原理 ，用该属性在其他所有对象的取值次数最多的值(即出现频率最高的值)来补齐该缺失的属性值。与其相似的另一种方法叫条件平均值填充法（Conditional Mean Completer）。在该方法中，用于求平均的值并不是从数据集的所有对象中取，而是从与该对象具有相同决策属性值的对象中取得。 这两种数据的补齐方法，其基本的出发点都是一样的，以最大概率可能的取值来补充缺失的属性值，只是在具体方法上有一点不同。与其他方法相比，它是用现存数据的多数信息来推测缺失值。\n热卡填充（Hot deck imputation，或就近补齐) 对于一个包含空值的对象，热卡填充法在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。不同的问题可能会选用不同的标准来对相似进行判定。该方法概念上很简单，且利用了数据间的关系来进行空值估计。这个方法的缺点在于难以定义相似标准，主观因素较多。\nK最近距离邻法（K-means clustering) 先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。\n使用所有可能的值填充（Assigning All Possible values of the Attribute） 用空缺属性值的所有可能的属性取值来填充，能够得到较好的补齐效果。但是，当数据量很大或者遗漏的属性值较多时，其计算的代价很大，可能的测试方案很多。\n组合完整化方法（Combinatorial Completer） 用空缺属性值的所有可能的属性取值来试，并从最终属性的约简结果中选择最好的一个作为填补的属性值。这是以约简为目的的数据补齐方法，能够得到好的约简结果；但是，当数据量很大或者遗漏的属性值较多时，其计算的代价很大。\n回归（Regression） 基于完整的数据集，建立回归方程。对于包含空值的对象，将已知属性值代入方程来估计未知属性值，以此估计值来进行填充。当变量不是线性相关时会导致有偏差的估计。\n期望值最大化方法（Expectation maximization，EM） EM算法是一种在不完全数据情况下计算极大似然估计或者后验分布的迭代算法。在每一迭代循环过程中交替执行两个步骤：E步（Excepctaion step,期望步），在给定完全数据和前一次迭代所得到的参数估计的情况下计算完全数据对应的对数似然函数的条件期望；M步（Maximzation step，极大化步），用极大化对数似然函数以确定参数的值，并用于下步的迭代。算法在E步和M步之间不断迭代直至收敛，即两次迭代之间的参数变化小于一个预先给定的阈值时结束。该方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。\n多重填补（Multiple Imputation，MI） 多重填补方法分为三个步骤： 为每个空值产生一套可能的填补值，这些值反映了无响应模型的不确定性；每个值都被用来填补数据集中的缺失值，产生若干个完整数据集合。 每个填补数据集合都用针对完整数据集的统计方法进行统计分析。 对来自各个填补数据集的结果进行综合，产生最终的统计推断，这一推断考虑到了由于数据填补而产生的不确定性。该方法将空缺值视为随机样本，这样计算出来的统计推断可能受到空缺值的不确定性的影响。该方法的计算也很复杂。\nC4.5方法 通过寻找属性间的关系来对遗失值填充。它寻找之间具有最大相关性的两个属性，其中没有遗失值的一个称为代理属性，另一个称为原始属性，用代理属性决定原始属性中的遗失值。这种基于规则归纳的方法只能处理基数较小的名词型属性。\n就几种基于统计的方法而言，删除元组法和平均值法差于热卡填充法、期望值最大化方法和多重填充法；回归是比较好的一种方法，但仍比不上hot deck和EM；EM缺少MI包含的不确定成分。值得注意的是，这些方法直接处理的是模型参数的估计而不是空缺值预测本身。它们合适于处理无监督学习的问题，而对有监督学习来说，情况就不尽相同了。譬如，你可以删除包含空值的对象用完整的数据集来进行训练，但预测时你却不能忽略包含空值的对象。另外，C4.5和使用所有可能的值填充方法也有较好的补齐效果，人工填写和特殊值填充则是一般不推荐使用的。"
  },
  {
    "objectID": "note/2023-10-15-data-transformation/index.html#不处理",
    "href": "note/2023-10-15-data-transformation/index.html#不处理",
    "title": "Data Tricks",
    "section": "不处理",
    "text": "不处理\n补齐处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实，在对不完备信息进行补齐处理的同时，我们或多或少地改变了原始的信息系统。而且，对空值不正确的填充往往将新的噪声引入数据中，使挖掘任务产生错误的结果。因此，在许多情况下，我们还是希望在保持原始信息不发生变化的前提下对信息系统进行处理。\n不处理缺失值，直接在包含空值的数据上进行数据挖掘的方法包括贝叶斯网络和人工神经网络等。\n贝叶斯网络提供了一种自然的表示变量间因果信息的方法，用来发现数据间的潜在关系。在这个网络中，用节点表示变量，有向边表示变量间的依赖关系。贝叶斯网络仅适合于对领域知识具有一定了解的情况，至少对变量间的依赖关系较清楚的情况。否则直接从数据中学习贝叶斯网的结构不但复杂性较高（随着变量的增加，指数级增加），网络维护代价昂贵，而且它的估计参数较多，为系统带来了高方差，影响了它的预测精度。\n人工神经网络可以有效的对付缺失值，但人工神经网络在这方面的研究还有待进一步深入展开。\n知乎上的一种方案：\n4.把变量映射到高维空间。比如性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。 而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。"
  },
  {
    "objectID": "note/2023-10-15-data-transformation/index.html#总结",
    "href": "note/2023-10-15-data-transformation/index.html#总结",
    "title": "Data Tricks",
    "section": "总结",
    "text": "总结\n大多数数据挖掘系统都是在数据挖掘之前的数据预处理阶段采用第一、第二类方法来对空缺数据进行处理。并不存在一种处理空值的方法可以适合于任何问题。无论哪种方式填充，都无法避免主观因素对原系统的影响，并且在空值过多的情形下将系统完备化是不可行的。从理论上来说，贝叶斯考虑了一切，但是只有当数据集较小或满足某些条件（如多元正态分布）时完全贝叶斯分析才是可行的。而现阶段人工神经网络方法在数据挖掘中的应用仍很有限。值得一提的是，采用不精确信息处理数据的不完备性已得到了广泛的研究。不完备数据的表达方法所依据的理论主要有可信度理论、概率论、模糊集合论、可能性理论，D-S的证据理论等。"
  },
  {
    "objectID": "note/2022-08-10-linear-dimension-reduction/index.html",
    "href": "note/2022-08-10-linear-dimension-reduction/index.html",
    "title": "Liner Dimension Reduction",
    "section": "",
    "text": "线性降维方法主要就是PCA(principal componet analysis)以及SVD(sigular value decomposition),以及PCA的扩展MDS(Multtidimensional Scaling)。PCA与MDS的差别在于PCA考虑的是样本的特征，寻求在低维空间下保留方差较大的特征的信息，所以通过对特征之间的协方差矩阵进行特征值分解矩阵，得到在低维空间下能够保留方差较大特征的正交基，是对特征的线性组合，而MDS考虑的是样本之间的相似度矩阵，通过对相似度矩阵矩阵进行特征分解找到在低维空间下能够保留样本最大距离的正交基。"
  },
  {
    "objectID": "note/2022-08-10-linear-dimension-reduction/index.html#pcoa",
    "href": "note/2022-08-10-linear-dimension-reduction/index.html#pcoa",
    "title": "Liner Dimension Reduction",
    "section": "PCoA",
    "text": "PCoA\n考虑一个样本的dissimilarity matrix,也就是一个含有样本之间距离或不相似度量的矩阵,记为D。\n\nThe Torgerson method\n\n首先对D进行double centering得到double-centered matrix,记为B。然后对矩阵B进行奇异值分解。\ndouble centering1): \\[\nD^2 = \\left[d_{ij}^2 \\right]\n\\] double centering2): \\[\nB = -\\frac{1}{2}CD^{2}C\n\\] 其中\\(C = I - \\frac{1}{n}J_n\\)\n\nThe iterative method\n该方法更加实用，可以用于非欧式距离矩阵。\n\n\\[\nStress_D(x_1, x_2, ..., x_N) = \\left(\\sum_{i\\neq j\\neq 1, ..., N} \\left(d_{ij} - ||x_i - x_j|| \\right)^2 \\right)^{\\frac{1}{2}}\n\\]"
  },
  {
    "objectID": "note/2022-08-10-linear-dimension-reduction/index.html#空间变换",
    "href": "note/2022-08-10-linear-dimension-reduction/index.html#空间变换",
    "title": "Liner Dimension Reduction",
    "section": "空间变换",
    "text": "空间变换\n考虑一个向量\\(a\\)，\\(\\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}\\)，左乘一个矩阵\\(M\\)，\\(\\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}\\)\n首先从空间变化的角度理解矩阵相乘。所谓的空间变化，就是对原空间的映射, 即原空间\\(A\\)经过变化\\(M\\)，变为\\(B\\)。这里假设的是，无论是这个向量，还是矩阵都是从同一组基的视角来表示的，我们将这组基记为\\(\\begin{bmatrix} i & j \\end{bmatrix}\\)，向量\\(\\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}\\)在基\\(i\\)上的分量就是\\(-1\\)，在基\\(j\\)上的分量就是\\(2\\)。这组基下的所有向量都是依赖于该基的，所以空间的变换直接用基的变换表示即可。矩阵\\(M\\)的列向量表示的含义就是经过空间变化以后，原来的一组基向量在基\\(i, j\\)下的表示。因为变化是整个空间的变化，所有向量的变化都是一致的，所以我们只需要用原向量在原基的每个分量乘以新的基即可:\n\\[\n-1 * \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} +\n2 * \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\\]\n结论：\n\n对于左乘的矩阵\\(A\\)的每一列，我们可以认为是原基在空间变换\\(A\\)以后用原基表示的原基的映射。\n\n\n行列式\n\n行列式\\(det(A)\\)给出了由\\(A\\)表示的映射引起的缩放因子和方向。数值代表缩放因子，正负号代表变换的方向。当行列式等于1时，由\\(A\\)定义的线性变换是等值的和保持方向的。\n如果一个n阶方阵的行列式为0，也就证明这个行列式不满秩。也就是说组成行列式的列向量是线性相关的，那么这n个列向量张成的空间便是n维空间的一个投影，维度等于秩。\n\\(det(A) = 0\\), 变换A使得空间的维度被压缩了。\n\n非方阵\n\n非方阵没有行列式。按照空间变化，非方阵一定使得空间的维度发生了变化，无法衡量变换的大小，因此没有行列式。例如一个\\(3*2\\)的矩阵可以将一个二维平面的向量映射为三维空间中的一个平面上的向量。而一个\\(2*3\\)的矩阵可以将一个三维空间的向量映射为二维平面上的一个向量，考虑整个三维空间就是将原空间做了一个投影。\n\n矩阵的逆\n\n矩阵不一定存在逆。可以求逆的矩阵叫做可逆矩阵，也叫非奇异矩阵。矩阵为非奇异矩阵的充要条件是矩阵存在行列式且不为0。可以这样理解：矩阵的作用是空间变换，其实就类似于一个函数。（这里说映射更标准些）矩阵求逆就类似于求反函数。当矩阵为不存在行列式或者行列式为0时，代表变换发生了降维，其映射关系不是一对一的关系，是多对一的关系，因此无法求逆或没有意义。\n\\(A^{-1}\\)所代表的变换恰好是\\(A\\)变换的逆过程。\n求解\\(Ax = b\\)的几何意义，就是找到一个向量\\(x\\)使得在\\(A\\)的变换下，\\(x\\)被映射为\\(b\\)。如果\\(A\\)为满秩矩阵，则有唯一解\\(x = A^{-1}B\\) ，也就是对\\(B\\)施加逆变换即可找到\\(x\\)"
  },
  {
    "objectID": "note/2022-08-10-linear-dimension-reduction/index.html#基变换",
    "href": "note/2022-08-10-linear-dimension-reduction/index.html#基变换",
    "title": "Liner Dimension Reduction",
    "section": "基变换",
    "text": "基变换\n矩阵相乘同样可以从基变化的角度看。假设一个向量\\(a\\)，其在基\\(u, v\\)下的表示为 \\(\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\)。如果我们想从另一组基\\(i, j\\)来表示应该怎么做呢？实际上我们只需要把基\\(u, v\\)用基\\(i,j\\)表示即可。假设基\\(u\\)用\\(i, j\\)表示为 \\(\\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}\\)，\\(v\\)用\\(i, j\\)表示为\\(\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\)，构成矩阵\\(P\\)。那么把向量变化到\\(i, j\\)的视角下，就是简单的矩阵左乘向量（这里用到了空间变化的思想，即左乘矩阵的列是基向量）：\n\\[\n2 * \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix} +\n1 * \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n\\]\n如果我们想把基\\(i, j\\)下的向量从基\\(u, v\\)的视角来观察呢？答案是对于\\(i, j\\)视角下的向量我们只需要左乘\\(P^{-1}\\)，即逆矩阵可以理解为两种视角的相互转换。\n既然从不同的视角看向量有不同的表示，从不同的视角看空间变化，也有不同的表示。考虑在基\\(u, v\\)下的一个空间变化表示为\\(M\\)，那么在基\\(i, j\\)下，该变化怎么表示呢。我们可以考虑先变换到基\\(i, j\\)的视角，然后进行变换，最后回到\\(u, v\\)的视角。即：\n\\[\nM = P^{-1}AP \\rightarrow A = PMP^{-1}\n\\]\n结论：\n\n对于左乘矩阵\\(A\\)，我们可以认为其每一列是用新基视角表示原基的表示。\n这说明对于形如\\(A = PMP^{-1}\\)的式子，我们可以直观的认为是视角转换下的同一变换。\n\n符号约定： 小写字母\\(x, i, j, u, v\\)代表列向量。\n基变化，如何把基\\((i, j)\\)下的坐标转化为另一组基\\((u, v)\\)下的坐标，实际上只需要将基\\((i, j)\\)用基\\((u, v)\\)进行表示，然后左乘向量\\(x\\)即可。"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html",
    "href": "note/2022-07-17-statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "图形描述\n数字描述\n\n\n数据集中的描述： 均值与中位数，数据对称时均值与中位相等，均值向数据分布（右偏或左偏）偏离。\n数据分散的描述：interquartile range = third quartile - 1st quartile。标准差是用于描述数据分散的常用数值。\n均值和标准差都对少部分很大或很小的值很敏感，可以考虑使用中位数和interquartile range\n\n\n\n\n总体（population): entire group we want information\n参数（parameter)：quantity about the population we are interested in. 样本（sample): part of population from which we collect information.\n统计量（estimate，statistic）：the quantity we are interested in as measured in the sample.\n关键：即使一个很小的样本也能产生一个于总体参数相近的估计。\n抽样方法：\n\n不放回的简单随机抽样（a simple random sampling)。\n分层的随机抽样(a stratified random sample)：相似的为一层，然后，在每一层进行一个简单的随机抽样，然后把这些样本组合起来。\n\nBias and chance error:\n\nBias(systematic error)\n\n\nselection bias： a sample of convenience make it more likely to sample certain subjects than others\n\nnon-response bias: less likely to answer a question at a special situation\n\nvoluntary response bias: websites that post reviews of businenss are more likely to get response from customer who had very bad or very good experiences.\n\n\nChance error(sampling error): 随机抽样，估计和总体的偏差，每一次的抽样有不同的chance error。抽样的随机性。\n\n\\[\nestimate = parameter + bias(systematic\\ error) + chance\\ error(sampling\\ error)\n\\]\nNote: 增大样本可以减少chance error，并且我们可以计算chance error具体有多大。但是增大样本只是让bias在一个更大的规模上重复，并且我们不能知道bias的大小。\n因果分析与关联分析\nobservation study: 测量一个感兴趣的事的结果，用于观察关联关系。\nAssociation is not causation, there may be confounding factors\nCausation experiment(randomized controlled experiments): 1. Subjects are assigned into treatment and control groups. at random.\n2. Subjects in controlled group get a placebo to ensure both groups equally affected by the placebo effect: the didea of being treated may have an effect by itself.\n3. Double-blind.\nMore：\nThe wired power of placebo effect, explained"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第一章-描述性统计学探索数据",
    "href": "note/2022-07-17-statistics/index.html#第一章-描述性统计学探索数据",
    "title": "Statistics",
    "section": "",
    "text": "图形描述\n数字描述\n\n\n数据集中的描述： 均值与中位数，数据对称时均值与中位相等，均值向数据分布（右偏或左偏）偏离。\n数据分散的描述：interquartile range = third quartile - 1st quartile。标准差是用于描述数据分散的常用数值。\n均值和标准差都对少部分很大或很小的值很敏感，可以考虑使用中位数和interquartile range"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第二章-数据产生抽样",
    "href": "note/2022-07-17-statistics/index.html#第二章-数据产生抽样",
    "title": "Statistics",
    "section": "",
    "text": "总体（population): entire group we want information\n参数（parameter)：quantity about the population we are interested in. 样本（sample): part of population from which we collect information.\n统计量（estimate，statistic）：the quantity we are interested in as measured in the sample.\n关键：即使一个很小的样本也能产生一个于总体参数相近的估计。\n抽样方法：\n\n不放回的简单随机抽样（a simple random sampling)。\n分层的随机抽样(a stratified random sample)：相似的为一层，然后，在每一层进行一个简单的随机抽样，然后把这些样本组合起来。\n\nBias and chance error:\n\nBias(systematic error)\n\n\nselection bias： a sample of convenience make it more likely to sample certain subjects than others\n\nnon-response bias: less likely to answer a question at a special situation\n\nvoluntary response bias: websites that post reviews of businenss are more likely to get response from customer who had very bad or very good experiences.\n\n\nChance error(sampling error): 随机抽样，估计和总体的偏差，每一次的抽样有不同的chance error。抽样的随机性。\n\n\\[\nestimate = parameter + bias(systematic\\ error) + chance\\ error(sampling\\ error)\n\\]\nNote: 增大样本可以减少chance error，并且我们可以计算chance error具体有多大。但是增大样本只是让bias在一个更大的规模上重复，并且我们不能知道bias的大小。\n因果分析与关联分析\nobservation study: 测量一个感兴趣的事的结果，用于观察关联关系。\nAssociation is not causation, there may be confounding factors\nCausation experiment(randomized controlled experiments): 1. Subjects are assigned into treatment and control groups. at random.\n2. Subjects in controlled group get a placebo to ensure both groups equally affected by the placebo effect: the didea of being treated may have an effect by itself.\n3. Double-blind.\nMore：\nThe wired power of placebo effect, explained"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第一章-大数定理与中心极限定理",
    "href": "note/2022-07-17-statistics/index.html#第一章-大数定理与中心极限定理",
    "title": "Statistics",
    "section": "第一章 大数定理与中心极限定理",
    "text": "第一章 大数定理与中心极限定理\nThree histograms: 1. Probability histogram for producing the data. 2. The histogram of 100 observed tosses. 3. The probability of the statistic.\nLaw of large numbers: When sample size is large enough, the \\(\\bar{x}_N\\) will be likely close to \\(\\mu\\) . 1) applied for averages and percentages, but not for sums. 2) sampling with replacement from a population or for simulating data from a probability histogram.\nMore advanced large number laws: the empirical histaogram will be close to probability to histogram producing the data.\nCentral limit theory: the sample sum statistic(averages and percentages are sums in disguise) distribution is normal distribution\n应用条件：放回抽样，或者每次都从同一个概率分布函数抽样（其实不同的也可以？）\nSample size is large enough.(if no strong skewness, n &gt; 15 is sufficient)"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第二章-概率",
    "href": "note/2022-07-17-statistics/index.html#第二章-概率",
    "title": "Statistics",
    "section": "第二章 概率",
    "text": "第二章 概率\nStandard definition: proportion of times this event occurs in many repetitions.\nSubjective probability: not based on experiments, different people assign different subjective probabilities to the same event.\nFour basic rules\n\nComplement rule: P(A does not occur) = 1 - P(A)\n\n\nRules for equally likely outcomes: \\(P(A) = \\frac{\nnumber\\ of\\ outcomes\\ in\\ A}{n}\\)\n\n\nAddition rule: \\(A\\) and \\(B\\) are mutually exclusive(don’t occure at the same time), then: \\[ P(A or B) = P(A) + P(B)\\]\n\n\nMultiplication rule: A and B are independent(one occures doesn’t change the probability that the other occurs),then: \\[P(A and B) = P(A)(B)\\]\n\n条件概率（conditional probability) \\[ P(B|A) = \\frac{P(A and B)}{P(A)} \\] General multiplication rule: \\(P(A and B) = P(A)P(B|A)\\), special case where A and B are independent: \\(P(A and B) = P(A)P(B)\\).\nBayes’s rule Bayesian analysis\nFalse positives case warner’s randomized response model"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第三章-正态分布与二项分布",
    "href": "note/2022-07-17-statistics/index.html#第三章-正态分布与二项分布",
    "title": "Statistics",
    "section": "第三章 正态分布与二项分布",
    "text": "第三章 正态分布与二项分布\nNormal curve: bell-shaped.\nEmpirical rule:\nAbout 2/3(68%) fall within one sd of the mean.\nAbout 95% fall within 2 sd of the mean.\nAbout 99.7 fall within 3 sd of the mean.\nStandardize data: \\(z = \\frac{height - \\bar{x}}{s}\\)\nz meas how many sd the height away from the mean. no unites.\nNormal approximation: 1. Finding areas under teh normal curve.(we can look up area to the left of a given value) the empirical rule is a special case of normal approximation.\n2. Computing percentiles for normal data: 30% data for normal curve, the height is z sd away from mean.\nBinomial probability \\[\n\\frac{X(success\\ count) - np}{\\sqrt{np(1 - p)}} \\sim\nN(0,1)\n\\]\nNote: 简单随机抽样是不放回的抽样，不是二项分布设定，因为每取出一个, 概率P就，改变了;但是如果总体size远大于样本size，那么放回抽样和不放回抽样就是大致一致的，服从于二项分布，服从于正态曲线。"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第四章-样本分布",
    "href": "note/2022-07-17-statistics/index.html#第四章-样本分布",
    "title": "Statistics",
    "section": "第四章 样本分布",
    "text": "第四章 样本分布\nExpected value of the sample average , E(\\(\\bar{x}_N\\)) is the population average. Standard error: statistic’s sd(其实就是样本统计量的标准差), tells us roughly how far off the statistic will be from it expected value.\nExpected value and SE for average &gt; E(\\(\\bar{x}_N\\)) = \\(\\mu\\) (Square root law), \\(SE(\\bar{x}_n) = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\nMore lager sample size n, more smaller SE, it can be used to determine sample size to get desired accuarcy\nSE don’t depend on the size of the population, only on the size of the sample.\n\n\nExpected value and SE for sum &gt; $E(S_n) = n$ , \\(SE(S_n) = \\sqrt{n}\\sigma\\)\nExpected value and SE for percentages Framework for counting and classifying: \\[\nE(percentage\\ of\\ 1s) = 1\\mu100%\n\\frac{\\sigma}{\\sqrt{n}} 100%\n\\]\nExpected value and SE when simulating A random variable X that is simulated has K possible outcomes , \\(\\mu = \\sum_{i=1}^k x_i P(X = x_i),  \\sigma^2 =\n\\sum{i=1}^k (x_i - \\mu)^2 P(X = x_i)\\)"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第一章-线性回归",
    "href": "note/2022-07-17-statistics/index.html#第一章-线性回归",
    "title": "Statistics",
    "section": "第一章 线性回归",
    "text": "第一章 线性回归\nScatter plot three element: direction(slope up or down), form(points cluster around a line or other), strength(how closethe points follow the form)\nSummary of pair data: \\(\\bar{x}\\), \\(s_x\\), \\(\\bar{y}\\), \\(s_y\\), \\(r\\)\nHow to quantify the strength?\nIf it is liner former, the correlation coefficient r is a good choice. standardized \\(x*y\\) ,not affected by the scale of either variable. its sign gives the direction and its absolute value gives the strength.\nNote: r is only useful for measuring linear association.and correlation does not mean causation\nHow to get the regression line?\nTo minimize the MSE(mean squared error), the method of least squares gives the analytic answer: \\(b = r\\frac{s_y}{s_x}\\) and \\(a = \\bar{y} - b\\bar{x}\\). This line \\(y = a + bx\\) is called the regression line.\nAnother interpretation of the regression line:\n&gt; Computes the average value of y when the first coordinate is near x.\nNote: The average often times is the best estimate when no extra information is provided.\n向均值回归? regression effect(回归效应)?\n因为 1）\\(\\bar{x}\\) 的预测值是 \\(\\bar{y}\\)， 2）\\(b = r\\frac{s_y}{s_x}\\) ，也就是说当\\(x\\)偏离 \\(\\bar{x}\\) 一个sd时， \\(y\\)只向 \\(\\bar{y}\\) 偏离 \\(r*sd\\) 个单位，也就是\\(y\\) is fewer sd away from \\(\\bar{y}\\) than \\(x\\) is from \\(\\bar{x}\\).\ni.e. Football shaped scatter, exam scores. my becaused by regression fallacy.\nNote: \\(x\\) to \\(y\\) and \\(y\\) to \\(x\\) are two different regression line, cannot predict each other.\n回归中的正态估计？\n在回归线（football shape scatter)中的某一点\\(x\\)处，\\(y\\)服从于正态分布 即： \\(\\frac{Y - y(predict)|x}{\\sqrt{1 - r^2}s_y}\\)\n如何检查回归使用是否正确？\nResidual plot. 残差就真实值与预测值的差. it should be a unstructured horizontal band. curved plot: not liner;but the data can be \\(\\sqrt{}\\) or log transformation to liner to analyze.\nscatter arises: heteroscedastic(may produce homoscedastic by y variable transformation, and it may result in a non-liner scatter , which require a second transformation in of x to fix)\n离群值 outliers? 离\\(x\\)均值很远的\\(x\\)可能会对回归线的构建有很大的影响（influential point）,会使得回归线向它偏离，无法用残差图检验。\n一些问题： - 预测\\(y\\)时，\\(x\\)应该在其范围之中，超出\\(x\\)的取值范围以后可能就不是线性关系。\n- 对总结数据注意，比如平均值，它们的变化更小，相关性？\n\\(R^2\\),可以被回归线解释的部分，\\(1 - r^2\\)就是不能解释的，就是残差。"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第一章-confidence-interval",
    "href": "note/2022-07-17-statistics/index.html#第一章-confidence-interval",
    "title": "Statistics",
    "section": "第一章 Confidence interval",
    "text": "第一章 Confidence interval\nSE gives the chance error, confidence interval give a more precise statement.\n已知一个样本统计量的分布，那么每一次的抽样我们可以说有95%几率该样本的统计量大小不会偏离该总体参数的2个SE(如果该统计量服从于正态分布)，也就是可以说每一次抽样得到到统计量，我们都有95%的把握说总体参数不会偏离超过2SE于该统计量。每一次的抽样都可以得到一个置信区间。\n注意：置信区间随着每一次抽样变化而变化，但是总体参数是一个固定的值。\nconfidence = estimate +/- zSE(if statistic ~ normal distribution)\n总体方差差未知？\nbootstrap：用样本方差代替得到一个估计置信区间。\nMore：置信区间的大小由zSE决定，称作margin of error,因为 \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\) ， 所以可以通过增大样本size减少区间。 同样也可减少z减少区间，比如80%区间。\n百分数的 95%置信区间： estimated percentage +/- \\(\\sqrt{n}\\)\n因为 \\(\\sigma = \\sqrt{p(1 - p)}\\) &lt; \\(\\frac{1}{2}\\)"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第二章-假设检验原理",
    "href": "note/2022-07-17-statistics/index.html#第二章-假设检验原理",
    "title": "Statistics",
    "section": "第二章 假设检验原理",
    "text": "第二章 假设检验原理\n假设检验的逻辑？\n设定零假设，备择假设，收集数据并评估该数据是否满足零假设从而接受零假设或拒绝，零假设一般是什么都没有发生，所以我们想要拒绝它，导致假设检验的逻辑不是很直接。\n检验统计量? test statistic\nA test statistic measures how far away the data are from what we would expect if H0 is true. i.e z-statistic: \\[\nz = \\frac{observed - expected}{SE}\n\\]\n\n观测值是一个用于评估H0的统计量，expected and SE are the expected value and SE of the this statistic, computed under the assumption H0 is true.\n\np value is the probability of getting a value of z as extreme or more extreme than the observed z, assuming HO is true.\nNote:Ho 是否正确是 一个确定的事，p值只是给出了在H0为真的假设下,观察到如此极端值得概率大小。\n实际上当我们进行z检验时，我们用观测统计量 - 期望统计量，然后除以统计量的SE，计算出观测统计量在零假设的情况下偏离期望值多少个sd。实际上样本的统计量的SE需要用总体方差进行计算，在sample size &gt; 20的情况下可以直接用样本sd代替总体sd，进行近似求解sample SE。如果sample size &lt; 20，用样本sd代替总体sd计算那么其服从于t(n-1), 置信区间：\\(\\bar{x} +/- t_{n-1}SE\\)\n其他： 1. 统计学上的显著不能说明效应大小很重要，因为大的样本数目可以减少SE,使得样本统计量的分布更加集中，那么只要一个很小的偏离就可以具有统计学上的显著。\n2. 95%的置信区间包括了所有零假设不会被拒绝的值，对于一个双端检验p值为0.05。\n3. 两类错误：H0为真,拒绝了type1 false positive,H0为假，接受了type2 error false negative"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第三章-假设检验之z检验以及t检验",
    "href": "note/2022-07-17-statistics/index.html#第三章-假设检验之z检验以及t检验",
    "title": "Statistics",
    "section": "第三章 假设检验之z检验以及t检验",
    "text": "第三章 假设检验之z检验以及t检验\nTwo sample z-test \\[\nz = \\frac{observed\\ difference - expected\\ difference}{SE\\ of\\ difference} = \\frac{(\\hat{p}_2 - \\hat{p}_1) - (p_2 - p_1)}{SE\\ of\\ difference}\n\\] If the two sample are independent: \\[\nSE(\\bar{x}_2 - \\bar{x}_1) = \\sqrt{(SE(\\bar{x}_1))^2 + (SE(\\bar{x}_2))^2 }\n\\]\nand \\(SE(\\bar{x}_1) = \\frac{\\sigma_1}{\\sqrt{n_1}}\\) is estimated by \\(\\frac{s_1}{\\sqrt{n_1}}\\) if sample size n1, n2 are not large, then the p-value neeed to computed from the t-distribution.\nif assuming \\(\\sigma_1 = \\sigma_2\\),then pooled estimate for \\(\\sigma_1 = \\sigma_2\\) ,given by \\[\ns_{pooled}^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\nPaired-difference test the independent assumption is in the sampling of the couples H0: population difference is zero \\(t = \\frac{\\bar{d} - 0}{SE(\\bar{d})}\\), where \\(d_i\\) is the difference of the ith couple. \\(SE(\\bar{d}) = \\frac{\\sigma_d}{\\sqrt{n}}\\), estimate \\(\\sigma_d\\) by \\(s_d\\)\nThe sign test"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第四章-假设检验之卡方检验类别变量研究",
    "href": "note/2022-07-17-statistics/index.html#第四章-假设检验之卡方检验类别变量研究",
    "title": "Statistics",
    "section": "第四章 假设检验之卡方检验——类别变量研究",
    "text": "第四章 假设检验之卡方检验——类别变量研究\nTesting of goodness-of-fit:研究一个分类变量的分布和已知分布是否一致。 H0: the color distribution is given by that table \\[\n\\chi^2_{n-1} = \\sum_{all\\ categories} \\frac{(observed - expected)^2}{expected}\n\\] 期望值来自于已知分布\nTesting homogeneity：\\(\\chi^2 -test\\ of \\ homogeneity\\) tests that the distribution of a categorical variable(color) is the same for serval populations(milk, peanut,caramel);检验一个分类变量在不同的总体中的分布是否一致。 \\[\n\\chi^2 (no.\\ of\\ columns -1 )(no.\\ of\\ rows - 1) = \\sum_{all\\ cells} \\frac{(observed - expected)^2}{expected}\n\\] 期望值来自于把不同总体合并为一个总体，计算该分类变量在所有总体中概率分布。\nTesting independence computed exactly as in the case of testing homogeneity. 比较图（待粘贴）"
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第五章-假设检验之f检验方差分析",
    "href": "note/2022-07-17-statistics/index.html#第五章-假设检验之f检验方差分析",
    "title": "Statistics",
    "section": "第五章 假设检验之F检验——方差分析",
    "text": "第五章 假设检验之F检验——方差分析\n通过比值比较组内差异和组间差异的大小。 Compare the sample variance of the means to the sample variance within the groups. Analysis of Variance(ANOVA)\nk groups and the \\(j\\)th group has \\(n_j\\) observations:\nThere are total \\(N = n_1 + ... + n_k\\) observations. Sample meen of jth group :\n\\(\\bar{y_j} = \\frac{1}{n_j}\\sum_{i=1}^{n_j}y_{ij}\\)\nOverall sampeld mean:\n\\(\\bar{y} = \\frac{1}{N}\\sum_{j=1}^{k}\\sum_{i=1}^{n_j}y_{ij}\\)\nThe treatment sum of squares : \\[\nSST = \\sum_j\\sum_i(\\bar{y_j} - \\bar{y})^2\n\\]\nhas k-1 degrees of freedom.\nThe treatment mean square: \\[\nMST = \\frac{SST}{k - 1}\n\\] Measures the variability of the treatment mean \\(\\bar{y_j}\\)\nThe error sum of suqares : \\[\nSSE = \\sum_j\\sum_i(\\bar{y}_ij -  \\bar{y_j})^2\n\\] has \\(N - k\\) degress of freedom. the error mean mean square : \\[\nMSE = \\frac{SSE}{N - k}\n\\] Measures the varibility within the groups.\nCompare the variation between the groups to the varition within the groups: \\[\nF = \\frac{MST}{MSE}\n\\] follows F-distribution with k - 1 and N - K degress of freedom. under null hypothesis ,it shoud be close to 1(not exactly for chane error.)\nthe ANOVA table: (待粘贴)\nThe one-way ANOVA model: \\[\ny_{ij} = \\mu_j + \\epsilon_{ij} (\\mu_j: mean\\ of\\ jth\\  group, \\epsilon_{ij} \\sim N(0, \\sigma^2))\n\\]\nso the null hypothesis: \\[\n\\mu_1 = \\mu_2 = ...\\mu_k\n\\]\ngroup mean’s devation away from the overall mean: \\(\\tau_j = \\mu_j - \\mu\\)\nso the the model: \\[\ny_{ij} = \\mu + \\tau_j + \\epsilon_{ij}\n\\] where \\(\\tau\\) called treatment effect of group j. Then the null hypothesis is \\[\nH_0 : \\tau_1 = \\tau_2 = ... = \\tau_k = 0\n\\]\nestimate overall mean \\(\\mu\\) by the ‘grand mean’ \\(\\bar{y}\\), then the estimate of \\(\\tau_j = \\mu_j - \\bar{y}\\). the estimate of \\(\\epsilon\\) is the residual \\(y_{ij} - y_j\\)\ncorresonding to the model \\(y_{ij} = \\mu + \\tau + \\epsilon_ij\\) wecan write \\(y_{ij}\\) as the sum of the corresponding estimates: \\[\ny_{ij} = \\bar{y} + (\\bar{y_j} - \\bar{y}) + (y_{ij} - \\bar{y_{j}})\n\\]\nit turns out that such a decomposition is also true for the sum of squares: \\[\n\\sum_j\\sum_i(y_{ij} - \\bar{y})^2 = \\sum_j\\sum_i(y_j - \\bar{y})^2 + \\sum_j\\sum_i(y_ij - bar{y_j})^2\n\\] \\[\nTSS = SST             +            SSE\n\\] it split the total variation into weo ‘sources’: SST and SSE.\nMORE: The F-test assumes that all group have the same \\(\\sigma^2\\), it can be roughly checked with side-by-side boxplots, and there are also formal test. another assumption: data are independent within and across group. it would be the case if the data were assigned to treatment at random . F-test give conclusion not eual,how thy differ, examine all paires of means of a two sample t-test using \\(s_{pooled} = \\sqrt{MSE}\\), multiple tests, adjustment is necessary such as Bonferroni adjustment."
  },
  {
    "objectID": "note/2022-07-17-statistics/index.html#第六章-假设检验之多重假设检验",
    "href": "note/2022-07-17-statistics/index.html#第六章-假设检验之多重假设检验",
    "title": "Statistics",
    "section": "第六章 假设检验之多重假设检验",
    "text": "第六章 假设检验之多重假设检验\np value &lt; 1% \\(\\rightarrow{}\\) test is ‘highly significant’ interpretation: If there is no effect, then there is only 1% chance to get such a highly significant result.\nbut if we do 800 tests, then enen their id no effect at all we expect to see 800* 1% = 8 highly results just by chance.\nthis is called multiple testing fallacy or look-elsewhere effect.(leads to data snooping or in other words, data dredging.)\nData snooping and other problems have lead to a crisis with regard to replicability (getting similar conclusions with different samples, procedures and data analysis methods) and reproducibility (getting the same results when using the same data and methods of analysis.)\nBonferroni correction: If there are m tests, multiply the p-values by m\nFalse Discovery Proportion (FDP): \\[\nFDP = \\frac{number\\ of\\ false\\ discoveries}\n{total\\ number\\ of\\ discoveries}\n\\] where a ‘discovery’ occurs when a test rejects the null hypothesis.\nFalse discovery rate (FDR): Controls the expected proportion of discoveries that are false. Benjamini-Hochberg procedure to control the FDR at level α = 5% (say): 1. Sort the p-values: p(1) ≤ . . . ≤ p(m) 2. Find the largest k such that p(k) ≤ k m \\(\\alpha\\) 3. Declare discoveries for all tests i from 1 to k\nUsing a validation set: Split the data into a model-building set and a validation set before the analysis"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html",
    "href": "note/2022-07-10-cluster-algorithm/index.html",
    "title": "Cluster Algorithm",
    "section": "",
    "text": "Note:该文章基本只是对几篇文章的整合，只有细微的细改，原文章在参考下。\n参考：\n相似性衡量，聚类算法以及data reduction的整体介绍\n距离计算的详细内容，每种聚类方法的实现过程，example\n聚类结果的内部价指标\n聚类结果的外部评价指标\n聚类分析：根据样本特征计算样本距离。需要考虑的点，聚类算法，相似度的衡量。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#相似系数核函数kx-y以及dtw",
    "href": "note/2022-07-10-cluster-algorithm/index.html#相似系数核函数kx-y以及dtw",
    "title": "Cluster Algorithm",
    "section": "相似系数，核函数\\(K(x, y)\\)以及DTW",
    "text": "相似系数，核函数\\(K(x, y)\\)以及DTW\n相似系数，包括夹角余弦和相关系数。其主要优势在于不受原线性变换的影响，可以轻松转化为距离，但其运算速度比距离法慢的多，当维数很高的时候。\n核函数\\(K(x, y)\\)， 定义在\\(R^d * R^d\\)的二元函数，本质也是距离。核函数的功能是把数据从低维到高维进行投影。\nDTW(dynamic time wraping)。可以用于计算两个不同长度向量的距离，也可以对两对向量中不同时间段的数据做匹配。主要用于时间序列分析。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#距离",
    "href": "note/2022-07-10-cluster-algorithm/index.html#距离",
    "title": "Cluster Algorithm",
    "section": "距离",
    "text": "距离\n\n数值变量\n\nMinkowski距离：其实就是\\(L_p\\) norm。考虑两个向量， \\(X = (x_1, x_2, ..., x_p)\\), \\(Y = (y_1, y_2, ..., y_p)\\). \\[\nd(X, Y) = \\sqrt[q]{\\lvert(x_1 - y_1)\\rvert^q + \\lvert(x_2 - y_2)\\rvert^q + ... + \\lvert(x_p - y_p)\\rvert^q}\n\\]\nManhattan距离：Minkowski，q = 1的特例。\n\n\\[\nd(X, Y) = \\lvert(x_1 - y_1)\\rvert + \\lvert(x_2 - y_2)\\rvert + ... + \\lvert(x_p - y_p)\\rvert\n\\]\n\nEuclidean距离：Minkowski, q = 2的特例。 \\[\nd(X, Y) = \\sqrt[2]{\\lvert(x_1 - y_1)\\rvert^2 + \\lvert(x_2 - y_2)\\rvert^2 + ... + \\lvert(x_p - y_p)\\rvert^2}\n\\]\nsupremum距离,也叫切比雪夫距离， \\(q \\rightarrow +\\infty\\)\n\nMahalanobis距离：权重向量 \\(W = (\\omega_1, \\omega_2, ...,\n\\omega_p)\\)，主要用于Gaussian Mixture Model(GMM)\n\\[\nd(X, Y) = \\sqrt[q]{\\omega_1*\\lvert(x_1 - y_1)\\rvert^q + \\omega_2*\\lvert(x_2 - y_2)\\rvert^q + ... +\\omega_p* \\lvert(x_p - y_p)\\rvert^q}\n\\]\nNote: 考虑到不同特征的尺度不一致，对特征做标准化处理。\\(Z_f = \\frac{X_f - mean_f}{S_f}\\)\n\n\n二元变量\n\\[d(X, Y) = \\frac{unpaired\\ features}{unpaired\\ feature + paired\\ positive}\n\\]\nNote: 为什么分母没有考虑\\(paired\\ negativae\\),因为\\(paired\\ negative\\) 说明是两者都没有的属性，那这样的属性可以说是无穷多的，计算上也没什么意义，所以不考虑，该说法由Jaccard提出，所以该距离称为Jaccard distance。\n\n\n分类变量\n\n简单匹配\n\n\\[\nd(X, Y) = \\frac{paired\\ feature}{category\\ number}\n\\]\n\n分类变量二值化，即将多类归为两类。\n\n\n\n有序变量\n考虑 \\(Level \\in{low, middle, high, ...}\\)\n\n用\\({1, 2, ..., N}\\)定义 \\(level\\)排序。\n对\\(level\\)进行\\(z\\ score\\)标准化。\n计算\\(level\\)的Minkowski距离。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#hierarchical-methods",
    "href": "note/2022-07-10-cluster-algorithm/index.html#hierarchical-methods",
    "title": "Cluster Algorithm",
    "section": "Hierarchical methods",
    "text": "Hierarchical methods\n主要有两种路径： agglomerative 和divisive, 也可以理解为自下而上法(bottom-up) 和自上而下法(top-down) 。自下而上法，就是一开始每个个体(object) 都是一个类， 然后根据巧linkage寻找同类， 最后形成一个”类” 。自上而下法就是反过来， 一开始所有个体都属于一个” 类， 然后根据linkage排除异己，最后每个个体都成为一个” 类” 。这两种路径本质上没有优劣之分，只是在实际应用的时候要根据数据特点以及你想要的” 类” 的个数，来考虑是自上而下更快还是自下而上更快。至于根据Linkage 判断” 类” 的方法就是楼上所提到的最短距离法、最长距离法、中间距离法、类平均法等等（其中类平均往往被认为是最常用也最好用的方法， 一方面因为其良好的单调性，另一方面因为其空间扩张/ 浓缩的程度适中），HierarchicaI methods 中比较新的算法有BIRCH (BaIanced lterative Reducing and clustering Using Hierarchies) 主要是在数据体量很大的时候使用，而且数据类型是numerical; ROCK (A HierarchicaI CIustering Algorithm for CategoricaI Attri butes）主用在categorical 的数据类型上； ChameIeon (A HierarchicaI CIustering Algorithm Using Dynamic ModeIing) 里用到的linkage是kNN (k-nearest-neighbor) 算法，并以此构建一个graph。Chameleon的聚类效果被认为非常强大，比BIRCH 好用，但运算复杂度很高。\n\nexample\n\n把每一个单个的观测都视为一个类，而后计算各类之间的距离，选取最相近的两个类，将它们合并为一个类。新的这些类再继续计算距离，合并到最近的两个类。如此往复，最后就只有一个类。然后用树状图记录这个过程，这个树状图就包含了我们所需要的信息。类的数量取决于你从树状图哪里剪。\n\n计算类与类之间的距离，用邻近度矩阵记录。\n将最近的两个类合并为一个新的类。\n根据新的类，更新邻近度矩阵。\n重复2. 3。\n到只只剩下一个类的时候，停止。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#partition-based-methods",
    "href": "note/2022-07-10-cluster-algorithm/index.html#partition-based-methods",
    "title": "Cluster Algorithm",
    "section": "Partition-based methods",
    "text": "Partition-based methods\n其原理简单来说就是，想象你有一堆散点需要聚类，想要的聚类效果就是”类内的点都足够近，类间的点都足够远” 。首先你要确定这堆散点最后聚成几类，然后挑选几个点作为初始中心点，再然后依据预先定好的启发式算法(heuristic algorithms)给数据点点做迭代重置（iterative relocation),直到最后到达”类内的点都足够近，类间的点都足够远” 的目标效果。也正是根据所渭的”启发式算法”,形成了k-means算法及其变体包括k-medoids 、k-modes 、k-medians、kernel k-means 等算法。k-means 对初始值的设置很敏感，所以有了k-means 十十、intelligent k-means 、genetic k-means; k-means 对噪声和离群值非常敏感，所以有了k-medoids 和k-medians; k-means只用于numerical 类型数据，不适用 于categorical 类型数据，所以k-modes; k-means不能解决非凸(non-convex) 数据，所以有了kernel k-meanso 另外，很多教程都告诉我们Partition-based methods 聚类多适用于中等体量的数据集，但我们也不知道中等’ 到底有多’ 中” ，所以不妨理解成， 数据集越大，越有可能陷入局部最小。\n\nexample k-means\n\n\n选择 K 个初始质心，初始质心随机选择即可，每一个质心为一个类。\n把每个观测指派到离它最近的质心，与质心形成新的类。\n重新计算每个类的质心，所谓质心就是一个类中的所有观测的平均向量（这里称为向量，是因为每一个观测都包含很多变量，所以我们把一个观测视为一个多维向量，维数由变量数决定）。\n重复2. 和 3。\n直到质心不在发生变化时或者到达最大迭代次数时。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#density-based-methods",
    "href": "note/2022-07-10-cluster-algorithm/index.html#density-based-methods",
    "title": "Cluster Algorithm",
    "section": "Density-based methods",
    "text": "Density-based methods\nk-means解决不了不规则形状的聚类。于是就有了Density-based methods来系统解决这个问题。该方法同时也对噪声数据的处理比较好。其原理简单说画圈，其中要定义两个参数，一个是圈的最大半径，一个是圈里最少应容纳几个点。最后在一个圈里的，就是一个类。DBSCAN (Density-Based SpatiaI Clustering of Applications with Noise) 就是其中的典型，可惜参数设置也是个问题，对这两个参数的设置非常敏感。DBSCAN 的扩展叫OPTICS (Ordering Points To ldentify Clustering Structure) 通过优先对高密度(high density) 进行搜索，然后根据高密度的特点设置参数，改善了DBSCAN的不足。\n\nexample\n\n其核心思想是在数据空间中找到分散开的密集区域，简单来说就是画圈，其中要定义两个参数，一个是圈的最大半径，一个是一个圈里面最少应该容纳多少个点。\n\n从数据集中随机选择核心点。\n\n以核心点为圆心，做半径为V的圆，圆内圈入点的个数满足密度阈值的核心点称为核心对象,每一个核心对象的对应的圈都是一个簇。\n\n合并这些相互重合的簇。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#grid-based-method",
    "href": "note/2022-07-10-cluster-algorithm/index.html#grid-based-method",
    "title": "Cluster Algorithm",
    "section": "Grid-based method",
    "text": "Grid-based method\n\nexample\n\n根据网格的聚类其原理是将数据空间划分为网格单元，将数据对象映射到网格单元中，并计算每个单元的密度。根据预设阈值来判断每个网格单元是不是高密度单元，由邻近的稠密单元组成“类”。\n1.将数据空间划分为网格单元。\n2.依照设置的阈值，判定网格单元是否稠密。\n3.合并相邻稠密的网格单元为一类。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#model-based-methods",
    "href": "note/2022-07-10-cluster-algorithm/index.html#model-based-methods",
    "title": "Cluster Algorithm",
    "section": "Model-based methods",
    "text": "Model-based methods\n这一类方去主要是指基于概率模型的方法和基于神经网络模型的方法，尤其以基于概率模型的方法居多。这里的概率模型主要指概率生成模型(generative Model)，同一”类” 的数据属于同一种概率分布。这种方法的优点就是对” 类” 的划分不那么”坚硬”，而是以概率形式表现，每一类的特征也可以用参数来表达；但缺点就是执行效率不高，特别是分布数量很多并且数据量很少的时候。其中最典型、也最常用的方法就是高斯混合模型(GMM, Gaussian Mixture Models)。基于神经网络模型的方法主要就是指SO(SeIfOrganized Maps) 了。"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#外部评价指标",
    "href": "note/2022-07-10-cluster-algorithm/index.html#外部评价指标",
    "title": "Cluster Algorithm",
    "section": "外部评价指标",
    "text": "外部评价指标\n\n聚类纯度(purity) 与准确率异曲同工，其总体思想就是用聚类正确的数目除以总的样本数，因此常被称为准确率。\n\n\\[\nP = \\sum_k\\frac{max\\ groud\\ truth\\ number\\ in\\ cluster\\ k }{total\\ number}\n\\]\n\nRand Index\n\n定义簇(cluster，cluster result), 类(classification, groud truth)。\nprecision,你发现的阳性有多少是真的阳性。\nRecall,放出去的阳性，你找回了多少。\nTP = 同簇同类的数目\nTN = 不同簇不同类的数目\nFP = 同簇不同类的数目\nFN = 不同簇同类的数目\n\\[ RI = \\frac{TP + TN}{TP + TN + FP + FN} \\] \\[ Precision = \\frac{TP}{TP + FP} \\] \\[ Recall = \\frac{TP}{TP + FN} \\] \\[ F_\\beta = \\frac{(\\beta^2 + 1)}{\\beta^2}\\frac{(Precision * Recall)}{Precision + Recall} \\]\n在这里 \\(RI\\) 和 \\(F_\\beta\\) 的取值范围均为\\([0,1]\\), 越大表示聚类效果越好。一般用得较多的是\\(F1\\)，这里 \\(\\beta =1\\)\n\n调整兰德系数(adjusted Rand Index)\n\n\\[\nARI = \\frac{\\sum_i\\sum_j\\tbinom{n_{ij}}{2} - \\left[ \\sum_i\\tbinom{a_i}{2} \\sum_b\\tbinom{b_j}{2} \\right] \\bigg/ \\tbinom{n}{2}} {\\frac{1}{2} * \\left[ \\sum_i\\tbinom{a_i}{2} \\sum_j\\tbinom{b_j}{2} \\right] - \\left[ \\sum_i\\tbinom{a_i}{2} \\sum_j\\tbinom{b_j}{2} \\right] \\bigg/ \\tbinom{n}{2}}\n\\]\n考虑\\(A\\)在三个cluster中数量分别为5, 1, 2. \\(B\\)在三个cluster中数量分别为1, 4, 0. \\(C\\)在三个cluster中的数量分别为0, 1, 3.\n\\[\n\\sum_i\\sum_j\\tbinom{n_{ij}}{2} = \\tbinom{5}{2} + \\tbinom{2}{2} + \\tbinom{4}{2} + \\tbinom{3}{2} = 20  \n\\] \\[\n\\sum_i\\tbinom{a_i}{2} = \\tbinom{8}{2} + \\tbinom{5}{2} + \\tbinom{4}{2} = 44  \n\\] \\[\n\\sum_j\\tbinom{b_j}{2} = \\tbinom{6}{2} + \\tbinom{6}{2} + \\tbinom{5}{2} = 40\n\\]\n\\[\nARI = \\frac{20 - 44 * 40 / 136 }{0.5 * (44 + 40) - 44 * 40 / 136 } = 0.24\n\\]"
  },
  {
    "objectID": "note/2022-07-10-cluster-algorithm/index.html#内部评价指标",
    "href": "note/2022-07-10-cluster-algorithm/index.html#内部评价指标",
    "title": "Cluster Algorithm",
    "section": "内部评价指标",
    "text": "内部评价指标\n内部评价指标基本都基于簇内距离，与簇间距离的比值，只是这些距离的含义不同。\n\n轮廓系数(Sihouette Coefficient Index) \\[\ns(i) = \\frac{b(i) - a(i)}{max\\{a(i), b(i) \\}}\n\\]\n\n\\[\ns = \\frac{1}{n} \\sum_i^n s_i\n\\]\n\\(i\\)代表一个样本点，其中\\(a(i)\\)是该样本点在簇类与其它点的均值距离，\\(b(i)\\)是该样本点与其最近的簇的样本点的距离均值，这里所谓的距离最近的簇是该点与其它簇的中心点的距离最近的簇。可以看出\\(s\\)的取值范围为\\(\\left[-1, 1 \\right]\\)\n\nCalinski-Harabasz Index(方差比准则) \\[\nW = \\sum_{k=1}^{k}\\omega_k = \\sum_{k=1}^{K}\\sum_{x\\in C_k} (x - c_k)^2\n\\]\n\n\\[\nB = \\sum_{k=1}^{k} b_k = \\sum_{k=1}^{k} n_k(c_k - c)^2\n\\]\n\\[\ns = \\frac{B}{K - 1} \\bigg/ \\frac{W}{n - K} = \\frac{B}{W} \\cdot \\frac{n - K}{k - 1}\n\\]\n其中\\(\\omega_k\\)是簇\\(k\\)中所有点与该簇中心点的距离和，\\(b_k\\)是簇\\(k\\)的中心点与所有样本中心点距离乘以簇\\(k\\)的样本点数目。其中\\(c_k\\)为簇\\(k\\)的簇中心，\\(c\\)为所有样本点的中心。\\(K\\)为聚类得到的簇总数，\\(n\\)为样本数目，\\(n_k\\)为簇\\(k\\)的样本数目。\n\nDavies-Bouldin Index\n\n簇内直径与簇间距离的比值。\n首先定义簇内直径\\(s_i\\)等于簇\\(i\\)中所有点与簇中心点的距离均值，簇\\(i\\)与簇\\(j\\)之间的距离为\\(d_{ij}\\),等于簇\\(i\\)与簇\\(j\\)中心点之间的距离。\n\\[\nR_{ij} = \\frac{s_i + s_j}{d_{ij}}\n\\]\n\\[\nDB = \\frac{1}{K} \\sum_{i,j=1}^K max_{i\\neq j} R_{ij}\n\\]\n\\(DB\\)指数的取值范围在\\(\\left[0, +\\infty\\right]\\)，结果越小聚类效果越好。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#数值变量",
    "href": "note/2023-10-01-health-statistics/index.html#数值变量",
    "title": "Health Statistics",
    "section": "数值变量",
    "text": "数值变量\n频数表的编制：\n\n求全距。\n确定组数, \\(range / n\\) 取整 \\([x, x + n)\\)。\n列表划计。\n\n\n对称或者正态分布数据选用算术均值描述其均值，方差/标准差/变异系数描述其离散程度。\n\n\n标准差，变异系数是同单位的。变异系数用于不同尺度，均值相差较大的数据直接相互比较。 方差/标准差/变异系数的计算都依赖于均值的计算。\n\n\\(CV = \\frac{S}{\\bar{X}} \\cdot 100\\%\\)\n\n\n\n对数正态分布数据, 例如抗体的几何滴度，细菌计数等，选用几何均值描述其分布，全距/四分位数描述其离散趋势。\n\n\n\\(G  = (\\prod{X})^{\\frac{1}{n}} \\rightarrow 10^{\\frac{\\sum{lgX}}{n}}\\)\n\n\n任意其它分布选用中位数，全距/四分位数\n\n\n\\(M = L + \\frac{i}{f_X}(n \\cdot X\\% - \\sum{f_L})\\)"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#分类变量",
    "href": "note/2023-10-01-health-statistics/index.html#分类变量",
    "title": "Health Statistics",
    "section": "分类变量",
    "text": "分类变量\n分类变量数据主要依赖于各种相对数描述，包括比例（proportion），速率（rate）， 相对比（ratio）. 其中rate主要涉及到时间的概念，需要注意孕妇死亡率等是相对比指标。\n数据的标准化法：\n\n直接化法，按总人口统一人口数，等价于对每组分层数据的率求均值-即每组中每个分层的权重都一致。\n间接法， 依据标准化率计算不同组的理论值，实际值与理论值之比即得到标准化死亡比（standard mortality ration, SMR）, 进而求得当地的标准化死亡率 \\(p' = p \\cdot SMR\\).\n\n动态数列：定基/环比，变化/增长（-1），平均发展速度/平均变化速度(-1)。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#实验设计",
    "href": "note/2023-10-01-health-statistics/index.html#实验设计",
    "title": "Health Statistics",
    "section": "实验设计",
    "text": "实验设计\n医学实验的特点：最终对象是人\n\n人具有生物学和社会学属性。\n人的个体变异性较大，实验单位的一致性较差，观察结果的离散程度较大。\n一般不允许在人体上直接实验，需先进行动物实验。\n\n医学实验设计要素：\n\n处理因素。\n受试对象。受试对象要求对处理因素敏感，以及反应稳定。\n实验效应。\n\n实验设计的基本原则：\n\n对照。\n随机化。包括抽取随机，分配随机，相同机会接受不同的实验顺序三层含义。\n重复。\n知情同意。\n\n常用的设计方案包括完全随机化，区组化设计，析因设计以及被试内设计。被试内设计通过将个体的差异转化为重复测量的差异，而减少了误差。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#调查研究",
    "href": "note/2023-10-01-health-statistics/index.html#调查研究",
    "title": "Health Statistics",
    "section": "调查研究",
    "text": "调查研究\n调查研究最大特点在于其只能被动观察，以及需要更大样本进而有更大的误差。\n调查设计的原则需要完整，可行，经济，时效。\n可行性分析可以通过逻辑分析，或者经验判断，或者试调查。\n抽样的基本程序包括界定研究总体和调查总体，设计抽样方法，编制抽样框架，抽取样本，评估样本。\n调查技术包括问卷，电话，访谈，观察法以及敏感问题调查技术-随机化应答技术。\n非抽样误差包括抽样框误差，无回答误差，以及计量误差。\n非抽样误差的估计有以下方法：\n\n调查质量控制措施的完善程度和落实情况。\n调查的应答率。\n比较不同来源的资料。\n进行抽样复查。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#sampling-distribution-for-chi2-f-t",
    "href": "note/2023-10-01-health-statistics/index.html#sampling-distribution-for-chi2-f-t",
    "title": "Health Statistics",
    "section": "Sampling Distribution for \\(\\chi^2, F, t\\)",
    "text": "Sampling Distribution for \\(\\chi^2, F, t\\)\n首先给出各分布构造的定义：\n\n若 \\(\\left\\{  X_i \\right\\}_{i=1}^n\\) 独立同分布于 \\(N(0, 1)\\)， 那么 \\(\\sum{X_i^2} \\sim \\chi^2(n)\\), 其 \\(E(\\chi^2) = n, Var(\\chi^2) = 2n\\).\n若有 \\(\\chi^2_1(m)\\), \\(\\chi^2_2(n)\\), 那么 \\(\\frac{\\frac{\\chi_1^2}{m}}{\\frac{\\chi_2^2}{n}} \\sim F(m, n).\\)\n若有 \\(X \\sim \\mathcal{N}(0, 1)\\)， 以及 \\(\\chi^2(n)\\), 那么 \\(\\frac{X}{\\sqrt{\\frac{\\chi^2(n)}{n}}} \\sim t(n)\\)\n\n\nTheorem 1 设 \\(\\left\\{ x_{i} \\right\\}_{i=1}^{n}\\) 是来自正态总体 \\(\\mathcal{N}(\\mu, \\sigma^2)\\) 的样本，其样本均值和方差分别为:\n\n\\(\\bar{x} = \\frac{1}{n}\\sum{x_i}\\)\n\\(s^2 = \\frac{1}{n-1}\\sum{(x - \\bar{x})^2}\\)\n\n则：\n\n\\(\\bar{x} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)\\)\n\\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{(n-1)}\\)\n\\(\\bar{x}, s^2\\) 相互独立\n\n\n\n这里描述的是正态总体样本的均值服从于正态分布，而其样本的方差 \\(s\\) 服从于卡方分布。\n注意 \\(s\\) 是样本的方差，而不是属于样本均值正态分布的方差，样本均值的正态分布的方差为 \\(\\frac{\\sigma^2}{n}\\)\n\n\n\n\nTheorem 2 设 \\(\\left\\{ x_i \\right\\}_{i=1}^m\\) 是来自 \\(\\mathcal{N}(\\mu_1, \\sigma_1)\\) 的样本，\\(\\left\\{ y_i \\right\\}_{i=1}^n\\) 是来自 \\(\\mathcal{N}(\\mu_2, \\sigma_2)\\) 的样本，那么：\n\n\\(\\bar{x} = \\frac{1}{m}\\sum{x}, \\bar{y} = \\frac{1}{n}\\sum{y}\\)\n\\(s_x^2 = \\frac{1}{m-1}\\sum{(x - \\bar{x})^2}, s_y^2 = \\frac{1}{n -1}\\sum{(y - \\bar{y})^2}\\)\n\n则：\n\n\\(\\frac{s_x^2/\\sigma_1^2}{s_y^2/\\sigma_2^2} \\sim F(m - 1, n - 1)\\)\n\n证明：\n\n\\(\\frac{(m - 1)s_x^2}{\\sigma^1} \\sim \\chi^2(m - 1), \\frac{(n - 1)s_y^2}{\\sigma_2^2} \\sim \\chi^2(n-1)\\)\n\\(\\frac{(1.1)/(m - 1)}{(1.2)/(n-1)} \\sim F(m - 1, n - 1)\\)\n\n\n\nTheorem 3 设 \\(\\left\\{ x_{i} \\right\\}_{i=1}^{n}\\) 是来自正态总体 \\(\\mathcal{N}(\\mu, \\sigma^2)\\) 的样本，其样本均值和方差分别为:\n\n\\(\\bar{x} = \\frac{1}{n}\\sum{x_i}\\)\n\\(s^2 = \\frac{1}{n-1}\\sum{(x - \\bar{x})^2}\\)\n\n则：\n\n\\(\\frac{\\bar{x} - \\mu}{\\sigma \\cdot \\sqrt{\\frac{1}{n}}} \\sim t(n-1)\\)\n\n证明：\n\n\\(\\frac{\\bar{x} - \\mu}{\\sigma \\cdot \\sqrt{\\frac{1}{n}}} \\sim \\mathcal{N}(0, 1)\\)\n\\(\\frac{(n - 1)s^2}{\\sigma^2} \\sim \\chi^2(n - 1)\\)\n\\(\\frac{(1)}{\\sqrt{(2)/(n - 1)}} \\rightarrow  \\frac{\\bar{x} - \\mu}{s\\cdot \\sqrt{\\frac{1}{n}}} \\sim t(n-1)\\)\n\n\n\nTheorem 4 设 \\(\\left\\{ x_i \\right\\}_{i=1}^m\\) 是来自 \\(\\mathcal{N}(\\mu_1, \\sigma_1^2)\\) 的样本，\\(\\left\\{ y_i \\right\\}_{i=1}^n\\) 是来自 \\(\\mathcal{N}(\\mu_2, \\sigma_2^2)\\) 的样本，那么：\n\n\\(\\bar{x} = \\frac{1}{m}\\sum{x}, \\bar{y} = \\frac{1}{n}\\sum{y}\\)\n\\(s_x^2 = \\frac{1}{m-1}\\sum{(x - \\bar{x})^2}, s_y^2 = \\frac{1}{n -1}\\sum{(y - \\bar{y})^2}\\)\n\n设 \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\), 则：\n\n\\(\\frac{(\\bar{x} - \\bar{y}) - (\\mu_1 - \\mu_2)}{s_p \\cdot \\sqrt{\\frac{1}{m} + \\frac{1}{n}}} \\sim t(m + n - 2), \\text{Where } s_p^2 = \\frac{(m - 1)s_x^2 + (n - 1)s_y^2}{m + n - 2}\\)\n\n证明：\n由两样本独立且正态分布：\n\n\\((\\bar{x} - \\bar{y}) \\sim \\mathcal{N}(\\mu_1 - \\mu_1, (\\frac{1}{m} + \\frac{1}{n})\\sigma^2)\\)\n\\(\\frac{(\\bar{x} - \\bar{y}) - (\\mu_1 - \\mu_2)}{\\sigma \\cdot \\sqrt{\\frac{1}{m} + \\frac{1}{n}}} \\sim \\mathcal{N}(0, 1)\\)\n\n\n由卡方变量可加性：\n\n\\(\\frac{(m - 1)s_x^2}{\\sigma_1^2} \\sim \\chi^2(m - 1), \\frac{(n-1)s_y^2}{\\sigma_2^2} \\sim \\chi^2(n-1)\\)\n\\(\\frac{(m - 1)s_x^2}{\\sigma^2} + \\frac{(n-1)s_y^2}{\\sigma^2} \\sim \\chi^2(m + n - 2)\\rightarrow \\frac{s_p^2 \\cdot (m + n - 2)}{\\sigma^2} \\sim \\chi^2(m + n - 2)\\)"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#tz-text-test",
    "href": "note/2023-10-01-health-statistics/index.html#tz-text-test",
    "title": "Health Statistics",
    "section": "\\(t/z \\text{ Test }\\)",
    "text": "\\(t/z \\text{ Test }\\)\n\nAssumptions and Its Application Case\nFrom wikipedia:\n\nFor exactness, the t-test and Z-test require normality of the sample means, and the t-test additionally requires that the sample variance follows a scaled \\(\\chi^2\\) distribution, and that the sample mean and sample variance be statistically independent. Normality of the individual data values is not required if these conditions are met. By the central limit theorem, sample means of moderately large samples are often well-approximated by a normal distribution even if the data are not normally distributed. For non-normal data, the distribution of the sample variance may deviate substantially from a \\(\\chi^2\\) distribution.\nHowever, if the sample size is large, Slutsky’s theorem implies that the distribution of the sample variance has little effect on the distribution of the test statistic. That is as sample size\n\n\\({\\displaystyle {\\sqrt {n}}({\\bar {X}}-\\mu )\\xrightarrow {d} N\\left(0,\\sigma ^{2}\\right)}\\) as per the Central limit theorem.\n\\({\\displaystyle s^{2}\\xrightarrow {p} \\sigma ^{2}}\\) as per the Law of large numbers.\n\\({\\displaystyle \\therefore {\\frac {{\\sqrt {n}}({\\bar {X}}-\\mu )}{s}}\\xrightarrow {d} N(0,1)}\\)\n\n\n我们知道 \\(t\\) 分布的构造定义如下：\n\\[\n\\frac{X}{\\sqrt{\\chi_{(n - 1)}/{(n-1)}}}, \\text{Where} X \\sim \\mathcal{N}(0, 1)\n\\]\n从 \\(t\\) 的构造中可知，我们需要一个 \\(X \\sim \\mathcal{N}(0, 1)\\) 的正态变量；而在大样本情况下， 依据中心极限定理，样本均值总是符合正态分布。 进一步地，依据大数定理，\\(s^2 \\approx \\sigma^2\\)， 所以直接用 \\(s\\) 替代 \\(\\sigma\\) 可以进行 \\(z\\) 检验。实际上，在大样本下如下公式总是成立的：\n\n\\(\\bar{x} \\sim \\mathcal{N}(\\mu, \\frac{1}{n}\\sigma^2) \\rightarrow \\text{Central Limit Theory}\\)\n\n\nSpecially, 设二项分布 \\(X \\sim B(n, p)\\):\n则：\n\n\\(\\bar{x} \\sim \\mathcal{N}(np, np(1-p)), \\text{When} x \\rightarrow \\infty\\)\n\\(\\hat{p} \\sim \\mathcal{N}(p, \\frac{1}{n}p(1-p))\\) 对于二项分布而言，其总体的期望以及方差实际上描述的就是多次独立的伯努利实验的期望与方差。\n\n\n而大样本下 \\(t\\) 分布近似于 \\(z\\) 分布。所以 \\(t\\) 分布可以应用于小样本下的正态总体，以及大样本下的任意总体的均值的检验。\n在应用 \\(t\\) 检验时，我们往往只有一个或两个样本的均值与方差，计算的关键就在于如何利用样本方差计算得到均值分布的方差。\n\n\nOne Smple \\(t\\)-Test\n\n\\(\\bar{x} \\sim \\mathcal{N}(\\mu, \\frac{1}{n}\\sigma^2)  \\rightarrow \\frac{\\bar{x} - \\mu}{\\sigma \\cdot \\sqrt{\\frac{1}{n}}} \\sim \\mathcal{N}(0, 1)\\)\n\\(\\frac{\\bar{x} - \\mu}{s \\cdot \\sqrt{\\frac{1}{n}}} \\sim t(n-1)\\)\n\n应用条件：\n\n小样本正态分布，大样本。\n\n\n\nPaired \\(t\\)-Test\n\\(t(\\upsilon) = \\frac{\\left|\\bar{d} - 0\\right|}{s_{\\bar{d}}} = \\frac{\\bar{d}}{s_{\\bar{d}}}, \\text{ where } \\upsilon = \\text{对子数} - 1, s_{\\bar{d}} = s_d \\cdot \\sqrt{\\frac{1}{n}}\\)\n证明：\n\n\\(\\frac{\\bar{d} - \\mu_d}{\\sigma_d \\cdot \\sqrt{\\frac{1}{n}}} \\sim \\mathcal{N}(0, 1)\\)\n\n应用条件：\n\n小样本下， \\(d\\) 服从于正态分布。\n大样本。\n\n\n\nIndependent Two Sample \\(t\\)-Test\n两个总体的比较需要对方差齐性进行检验：\n\n\\(F = \\frac{S_1^2}{S_2^2}, \\upsilon_1 = n_1 - 1, \\upsilon_2 = n_2 - 1\\), 其中 \\(S_1^2\\) 是比较大的那个。\n\nWhen \\(\\sigma_1 = \\sigma_2 = \\sigma\\):\n\n\\(\\frac{(\\bar{x} - \\bar{y}) - (\\mu_1 - \\mu_2)}{s_p \\cdot \\sqrt{\\frac{1}{m} + \\frac{1}{n}}} \\sim t(m + n - 2), \\text{Where } s_p^2= \\frac{(m-1)s_x^2 + (n - 1)s_y^2}{m + n - 2}\\)\n\nWhen \\(\\sigma_1 \\neq \\sigma_2 \\text{ or Unknown for it} \\rightarrow \\text{ Welch's t-Test}\\):\n\n\\(t^\\prime(\\upsilon) = \\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{s_1^2}{m} + \\frac{s_2^2}{n}}}\\)\n\n通过 \\(\\text{Satterhwaite}\\) 法，对自由度进行校正\n\n\\(\\upsilon = \\frac{(\\frac{s_1^2}{m} + \\frac{s_2^2}{n})^2}{\\frac{s_1^4}{m^2}/(m-1) + \\frac{s_2^4}{n^2}/(n-1)}\\)"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#bn-ppolambda-text-test",
    "href": "note/2023-10-01-health-statistics/index.html#bn-ppolambda-text-test",
    "title": "Health Statistics",
    "section": "\\(B(n, p)/Po(\\lambda) \\text{ Test}\\)",
    "text": "\\(B(n, p)/Po(\\lambda) \\text{ Test}\\)\n设事件 \\(A\\) 发生的概率为 \\(\\pi\\), 那么在 \\(n\\) 次伯努利试验中，该事件发生次数 \\(k\\):\n\n\\(P(k) = \\binom{n}{k} \\pi^k \\cdot(1-\\pi)^{n - k}, \\text{Where } \\binom{n}{k} = \\frac{n!}{k!\\cdot(n-k)!}\\)\n\n其中 \\(\\binom{n}{k}\\) 正好是 牛顿二项展开式 \\(\\left[ (1 - \\pi) + \\pi \\right]^n\\) 第 \\(k + 1\\) 项。\n设 \\(X \\sim B(n, \\pi)\\), 则：\n\n\\(X \\sim \\mathcal{N}(n\\pi, n\\pi(1 - \\pi), \\text{When } n\\pi &gt; 5 \\text{ and } n(1 - \\pi) &gt; 5\\)\n\\(Z = \\frac{X - n\\pi}{\\sqrt{n\\pi{(1 - \\pi)}}} = \\frac{p - \\pi}{\\sqrt{\\pi(1 - \\pi)/n}} \\sim \\mathcal{N}(0, 1)\\)\n\\(p \\sim \\mathcal{N}(\\pi, \\frac{\\pi(1-\\pi)}{n})\\)\n\n当 \\(\\pi\\) 特别小时，设 \\(\\lambda = n\\pi\\), 则当 \\(n \\rightarrow \\infty\\) ：\n\n\\(P(X) = \\frac{e^{-\\lambda}\\lambda^X}{X!}\\)\n\n\n\n\\(n\\) 足够大，以至于每 \\(\\frac{1}{n}\\) 中只有发生与不发生两种情况。\n每一份 \\(\\frac{1}{n}\\) 中其概率都为 \\(\\frac{\\pi}{n}\\)\n每一份中之间是相互独立的。\n\n\n则 \\(X \\sim Po(\\lambda)\\)\n\n\\(X \\sim \\mathcal{N}(\\lambda, \\lambda) \\rightarrow \\frac{X - \\lambda}{\\sqrt{\\lambda}} \\sim \\mathcal{N}(0, 1),\\text{When } \\lambda &gt; 20\\)\n\\(Z = \\frac{X_1/n_1 - X_2/n_2}{\\sqrt{X_1/n_1^2 + X_2/n_2^2}}\\)\n\n泊松分布最大的特征就是其均值与方差都等于 \\(\\lambda\\), 我们常用这一点来判断一个分布是否属于泊松分布。此外，泊松分布具有可加性。考虑 \\(X_1 \\sim Po(\\lambda_1), X_2 \\sim Po(\\lambda_2)\\), 且互相独立：\n\n\\(X_1 + X_2 \\sim Po(\\lambda_1 + \\lambda_2)\\)"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#aanlysis-of-variance",
    "href": "note/2023-10-01-health-statistics/index.html#aanlysis-of-variance",
    "title": "Health Statistics",
    "section": "Aanlysis of Variance",
    "text": "Aanlysis of Variance\n\nOne Way ANNOVA\n该假设的原理如下：\n\nIf the group means are drawn from populations with the same mean values, the variance between the group means should be lower than the variance of the samples, following the central limit theorem - wikipedia\n\n其需要满足的假设如下：\n\n正态总体\n方差齐性\n样本独立性\n\n\nTiku (1971) found that “the non-normal theory power of F is found to differ from the normal theory power by a correction term which decreases sharply with increasing sample size.” The problem of non-normality, especially in large samples, is far less serious than popular articles would suggest - wikipedia\n\n\n\\(Y_{ij} - \\bar{Y} = (Y_{ij} - Y_i) + (Y_i - \\bar{Y})\\)\n\\(\\sum_{i=1}^a\\sum_{j=1}^{n_i} \\left( Y_{ij} - \\bar{Y} \\right)^2 = \\sum_{i=1}^a\\sum_{j=1}^{n_i} \\left(Y_{ij} - Y_i \\right)^2 + \\sum_{i=1}^a n_i\\left( {Y_i - \\bar{Y}} \\right)^2\\)\n\\(F = \\frac{\\sum_{i=1}^a n_i\\left( {\\bar{Y_i} - \\bar{Y}} \\right)^2 \\bigg{/} (a - 1)}{\\sum_{i=1}^a\\sum_{j=1}^{n_i} \\left(Y_{ij} - \\bar{Y_i} \\right)^2 \\bigg{/} (N - a)} \\sim F_{(a-1, N-a)}\\)\n\n从公式的推导之中可以看到，其核心在于对变异(SS, sum of squares)的分解 \\(SS_{\\text{total}} = SS_{\\text{within group}} + SS_{\\text{between group}}\\)。方差分析需要进行Levene’s方差齐性检验，该检验实际上是对每个值减去该组均值的差异做单因素方差分析：\n\n\\(Z_{ij} = |Y_{ij} - \\bar{Y_i}|\\)\n\\(L = \\frac{\\sum_{i=1}^a n_i\\left( {\\bar{Z_i}- \\bar{Z}} \\right)^2 \\bigg{/} (a - 1)}{\\sum_{i=1}^a\\sum_{j=1}^{n_i} \\left(Z_{ij} - \\bar{Z_i} \\right)^2 \\bigg{/} (N - a)} \\sim F_{(a-1, N-a)})\\)\n\n\n\nSpecial Two Way ANNOVA - Random Block Design\n当观测变量有多个时，进行方差分析，不仅要考虑每个变量的观测值的影响，还需要考虑变量之间的交互作用对观测值的影响。而随机区块设计通过分组以后再进行随机化，使得我们可以不考虑两者的交互作用，只考虑两个变量分别对结果的影响；\n\n\\(Y_{ij} - \\bar{Y} = (\\bar{Y_i} - \\bar{Y}) + (\\bar{Y_j} - \\bar{Y}) + (Y_{ij} - \\bar{Y_i} - \\bar{Y_j} + \\bar{Y})\\)\n\n从上面的公式可以看出，随机区组设计将变异分为两个变量来源（区组，实验因素），以及个体本身的误差，其中误差项的自由度为 \\((n-1)(\\upsilon - 1).\\)\n对于混杂因素的控制还有将线性回归与方差分析结合起来的协方差分析，其对个体值的分解如下：\n\n\\(Y_{ij} = \\bar{Y_i} + b(\\bar{X}_{ij} - \\bar{X}) + e_{ij}\\)\n\n其中 \\(X\\) 是混杂因素。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#mutiple-hypothesis-test",
    "href": "note/2023-10-01-health-statistics/index.html#mutiple-hypothesis-test",
    "title": "Health Statistics",
    "section": "Mutiple Hypothesis Test",
    "text": "Mutiple Hypothesis Test\n方差分析对各处理组均数是否相等总的检验，在 \\(H_0\\) 被拒绝以后，需要确定究竟是哪些处理组之间存在差异，此时需要进行均数之间的多重比较，这就涉及到累计I型错误率。\n当 \\(a\\) 个处理组均数需要两两比较时候，共需要比较 \\(c = a!/[2!(a-2)!]\\)。 设每次检验的检验水准为 \\(\\alpha\\) ,累积I型错误概率为 \\('\\alpha\\), 则\n\n\\('\\alpha = 1 - (1 - \\alpha)^c\\)\n\n\n\\(q\\) -Test/student-Newman-Keuls\n\\(q \\text{Test}\\) 用于任意两组之间的相互比较，SNK法的检验效能介于Bonferroni和Tukey法之间的；当比较均值的组数较多时，Tukey法更有效，组数较少时，ferroni法更有效。\n其计算过程如下：\n\n将各组的平均值按由小到大的顺序排列。\n计算两个平均之间的差值以及组间跨度 \\(r\\)\n\n则 \\(q\\) 统计量：\n\n\\(q = \\frac{{\\bar{Y_i} - \\bar{Y_h}}}{\\sqrt{\\frac{MS_{within \\: group}}{2}(\\frac{1}{n_i} + \\frac{1}{n_h})}}\\)\n\n其中 \\(\\bar{Y_i}, \\bar{Y_h}\\) 及 \\(n_i, n_h\\) 分别是两个比较组的均数以及样本例数， \\(MS_{\\text{within group}}\\) 为进行方差分析得到的组内均方。\n\n\nDunnett-\\(t\\) Test\n\\(t_D\\) 统计量处理组与对照组的比较，该统计量的计算如下：\n\n\\(t_D = \\frac{\\bar{Y_i} - \\bar{Y_c}}{\\sqrt{MS_{within \\: group} \\times (\\frac{1}{n}_i + \\frac{1}{n_c})}}\\)\n\n其中 \\(\\bar{Y_i}, \\bar{Y_c}\\) 及 \\(n_i, n_c\\) 分别是实验组与对照组的均数以及样本例数， \\(MS_\\text{within group}\\) 为进行方差分析得到的组内均方。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#chi2-text-test",
    "href": "note/2023-10-01-health-statistics/index.html#chi2-text-test",
    "title": "Health Statistics",
    "section": "\\(\\chi^2 \\text{ Test}\\)",
    "text": "\\(\\chi^2 \\text{ Test}\\)\n卡方检验用于观测变量为无序分类变量时， 用于检验零假设下观测频数与理论频数(将所有组合并为一个组计算出的每个结局的频率分布，然后乘以原组的频数)之间偏离如此大范围的概率（如果观测变量为有序的分类变量，则该使用秩相关的检验）。该方法应用的条件如下：\n\n\n不宜有 \\(\\frac{1}{5}\\) 的格子数的理论频数小于 \\(5\\), 或有一个格子的理论频数小于1，否则将导致分析的偏性。可采取扩大样本含量，或者合并或者删除不符合条件的数据。后两者会损失信息，样本随机性，可能会影响推断结论。\n多个样本率（即多分组，观测变量为无序的二分类变量）的比较，显著的差异只能推断出这几组之间有总体的差异，但是不能推出其中两者之间是否存在差异。此时可将不同组的数据两两组合重新进行卡方检验进行推断，此时的检验水准的计算公式为 \\(\\frac{\\alpha}{k(k-1)/2 + 1}\\)\n\n\n考虑一个 \\(R \\cdot  C\\) 的列联表\n\n\\(\\chi^2 = \\sum\\frac{(O - E)^2}{E}, \\text{Where} E_{rc}= \\frac{n_{r}n_{c}}{n} \\rightarrow \\chi^2 = n(\\sum\\frac{O^2}{n_{r}n_{c}} - 1)\\)\n\n\n\n如果为单变量，按样本分组；如果观测变量是无序二分类的，则是率的比较；如果观测变量为无序多分类，则为频数分布的比较。\n如果为两个无序的观测变量，则为两个观测变量关联性检验，此时还需要进一步计算关联系数 \\(C \\text{ (contigency coefficient)} = \\sqrt{\\frac{\\chi^2}{n + \\chi^2}}\\)\n\n\n对于称为四格表的资料,即 \\(2*2\\) 的列联表，其计算的简化形式为：\n\n\\(\\chi^2 = \\frac{(ad - bc)^2n}{(a+b)(c+d)(a+c)(b+d)}\\)\n\n特别地， 当理论频数存在 \\(1 &lt; E &lt; 5\\) 时有如下的 Yate correction for continuity：\n\n\\(\\chi^2 = \\sum\\frac{(\\mid O - E \\mid - 0.5)^2}{E} = \\frac{(|ad - bc| - \\frac{n}{2})^2n}{(a+b)(c+d)(a+c)(b+d)}\\)\n\n\n实际上Pearson卡方值是正态总体中一种连续性的变量，四格表资料的卡方值为不连续的值，卡方分布仅仅是对表格资料的统计量分布的近似分布。 当四格表中有小于5的期望值时，其卡方值偏大，减去 \\(0.5\\) 进行Yate 的连续性校正。\n\n若 \\(n \\le 40 \\text{ or } E \\le 1\\) 则采用Fisher确切概率法。\n\n\\(P = \\frac{\\binom{a + c}{a}\\binom{b + d}{b}}{\\binom{n}{a + b}} = \\frac{(a+b)!(a+c)!(b+c)!(b+d)!}{a!b!c!d!n!}\\)\n\n\n实际上fisher确切概率法给出了 \\(R * C\\) 列联表的确切概率，可以用于任意情况的检验；但是这里给出的是某一种情况的概率，为了求得比当前情况都更为极端的差值，在四格表资料中我们比较的两个样本率的比较，因此对于所有率的差值大于原假设的极端情况，我们需要计算所有的概率累加，得到发生如此极端值的概率。\n\n当四格表资料为配对设计时，该检验称为 Mc-Nemar Test, 检验两者阳性率是否一致，该值完全由两组中阳性的频数决定，则：\n\n\\(\\chi^2 = \\frac{(b - c)^2}{b + c}, \\upsilon = 1\\)\n\n若 \\(b + c \\le 40\\), 则：\n\n\\(\\chi^2 = \\frac{(|b-c|-1)^2}{b+c}, \\upsilon = 1\\)\n\n因为 Pearson \\(\\chi^2\\) 能反映实际频数和理论频数的的吻合程度，所以 \\(\\chi^2\\) 检验可以用作频数分布的拟合优度检验（goodness of fit test)，用于判断样本是否符合正态分布，二项分布，Possion分布等。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#rank-based-test",
    "href": "note/2023-10-01-health-statistics/index.html#rank-based-test",
    "title": "Health Statistics",
    "section": "Rank Based Test",
    "text": "Rank Based Test\n用于总体分布未知，且观测变量为数值变量或者有序分类变量情况，需要关注的是不同的秩和检验的秩和 \\(T\\) 是如何计算的以及遇到相等的数据该如何处理（只有数值变量会遇到相同的数据，对于观测变量为有序的等级数据，因为用的是其每一类结果的平均秩次，不存在该问题）。\n\nWilcoxon Signed Rank Test for Paired Sample\n依差值的绝对值从小到大编秩。编秩时遇到差值为 \\(0\\) 的舍去不计，同时样本例数 \\(n - 1\\) ; 遇到绝对值差值相等差数，符号相同则顺次编秩，符号相反则取平均秩次，再给秩次冠以原差值的正负号。分别计算出正负秩次 \\(T_+, T_-,\\)任取其中一个作为统计量秩和 \\(T\\)\n\n\\(T \\sim \\mathcal{N}(\\frac{n(n+1)}{4}, \\frac{n(n+1)(2n+1)}{24}), \\text{When } n &gt; 25\\)\n\\(Z = \\frac{\\mid T - n(n + 1)/4  \\mid - 0.5}{\\sqrt{n(n+1)(2n+1)/24}} \\text{ 其中 0.5 为连续性校正常数。}\\)\n\n当相同差值数较多时（不包括差值为0的值)，校正式\n\n\\(Z = \\frac{\\mid T - n(n + 1)/4  \\mid - 0.5}{\\sqrt{n(n+1)(2n+1)/24 - \\frac{\\sum(t_j^3 - t_j)}{48}}} \\text{ 其中 $t_j$ 是第 $j$ 个相同差值的个数。}\\)\n\n\n\nWilcoxon Rank Sum Test/Mann Whitney Test for Independent Two Samples\n\n若观测变量为数值变量：将两组原始数据分别从小到大排队，再将两组数据由小到大统一编秩，若有同组相同数据则顺序编秩，若有不同组别相同数据则取平均秩次。记两组中样本例数较小的为 \\(n_1,\\) 其秩和为统计量\\(T\\).\n若观测变量为有序的多分类变量：将每个观测单位按观测变量等级排序，则观测变量各个等级的平均秩次为该组观测单位秩次和的均值，则可求得每个分组的秩和，取观测单位数较小的组的秩和作为统计量 \\(T\\).\n\n则统计量 \\(T\\):\n\n\\(T \\sim \\mathcal{N}(\\frac{n_1(N+1)}{2}, \\frac{n_1n_2(N+1)}{12}), \\text{Where} N = n_1 + n_2\\)\n\\(Z = \\frac{|T - \\frac{n_1(N+1)}{2}| - 0.5}{\\sqrt{\\frac{n_1n_2(N+1)}{12}}}\\)\n\n当相同秩较多时，有如下校正：\n\n\\(Z_c = Z\\sqrt{C}, \\text{Where } C = 1 - \\sum{(t_j^3 - t_j)}/(N^3 - N)\\)\n\n\n\nANNOVA for Rank\n单因素的方差分析对应Kruskal-Wallis Test, 随机区组设计的方差分析对应Freidman Test.\nKruskal-Wallis Test\n构造\\(H\\)统计量：假设有 \\(a\\) 个组，第 \\(i\\) 组的样本量 \\(n_i\\), \\(N\\) 为各组样本量之和，将各组数据合并，编秩次，秩次相同的取平均值。\\(R_{ij}\\) 为第 \\(i\\) 个组的第 \\(j\\) 个个体的秩次，\\(\\bar{R_i}\\)为第 \\(i\\) 个组的平均秩次，\\(\\bar{R}\\) 为总平均秩次。\n\n\\(H = \\frac{\\sum_{i}^{a}n_i(\\bar{R_i} - \\bar{R})^2}{\\frac{1}{N - 1}\\sum_{i=1}^a\\sum_{j=1}^{n_i}(R_{ij} - \\bar{R})^2}\\)\n\n从上式可以看出 \\(H\\) 统计量实际上是组间变异与总的变异的比值。\n没有相同秩次时，秩次服从均匀分布, 上式可以简化为：\n\n\\(H =\\frac{12}{N(N + 1)}(\\sum\\frac{R_i^2}{n_i}) - 3(N+1).\\)\n\n相同秩次过多时，上述以均匀分布为基础推导的公式需要进行校正：\n\n\\(H_c = H/C\\)\n\n其中 \\(C = 1 - \\sum(t_j^3 - t_j) / (N^3 - N)\\)。\n\\(n_i\\) 与 \\(a\\) 较小时 直接计算或者查表。\n\\(n\\) 较大时, \\(H\\) 近似服从于 \\(\\chi^2_{a-1}.\\)\n实际上也可以对数据编秩，然后用数据的秩次代替原数据进行方差分析得到 \\(F\\) 统计量， \\(H\\) 统计量与 \\(F\\) 统计量有如下关系：\n\\(F = \\frac{H/(a-1)}{(N-1-H)/(N-a)}\\)\nFriedman Test\n在区组(行）内进行编秩，有相同的则取平均秩次。\\(i\\) 代表不同的区组，\\(j\\) 代表不同地处理水平。\n\n\\(M = \\frac{\\sum_{j=1}^{a}n(\\bar{R}_j - \\bar{R})^2}{\\sum_{j=1}^{a}\\sum_{i=1}^{n}(R_{ij} - \\bar{R})^2 / n(a-1)}\\)\n\n从上式可以看到 \\(M\\) 统计量是处理水平之间的变异与总的变异的比值。\n如果没有相同秩次时，上式可以简化为：\n\n\\(M = \\frac{12}{na(a + 1)}\\sum_{j=1}^aR_j^2 - 3n(a+1)\\)\n\n当相同秩过多时可以进行校正，校正系数为\n\n\\(C = 1 - \\sum_{j=1}^{a}\\sum_{p=1}^{l_i}(t_{jp}^3 - t_{jp})/[na(a^2 -1 )]\\)\n\\(M_c = M/C\\)\n\n当 \\(n\\) 以及 \\(a\\) 较小时直接查表或精确计算。\\(n\\) 较大时 \\(M\\) 近似服从于 \\(\\chi^2_{a-1}.\\)\n同样地，我们也可以直接用编秩后数据代替原始数据的值进行随机区组设计的方差分析， 此时的 \\(F\\) 统计量与 \\(M\\) 统计量的关系如下：\n\n\\(F = \\frac{M/(a-1)}{(na-n - M)/(n-1)(a-1)}\\)"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#pearsonspearman-correlation",
    "href": "note/2023-10-01-health-statistics/index.html#pearsonspearman-correlation",
    "title": "Health Statistics",
    "section": "Pearson/Spearman Correlation",
    "text": "Pearson/Spearman Correlation\n考虑两个连续的正态分布变量 \\(X, Y\\), 那么其样本的 Pearson 积差相关系数(product-moment correlation coefficient):\n\n\\(r_p = \\frac{\\sum(X - \\bar{X})(Y - \\bar{Y})}{\\sqrt{\\sum{(X - \\bar{X})^2} \\sum{(Y - \\bar{Y})^2}}}\\)\n\\(t_r = \\frac{r - 0}{S_r},  \\upsilon = n -2 \\text{ Where } S_r = \\sqrt{\\frac{1-r^2}{n-2}}\\)\n\n对于相关有如下注意的问题：\n\n分层数据合并假象，合并分层不改变其相关性时，才可以合并。\n两个变量应该都是随机的，而不是控制一个变量，观察另一个变量的结果。\n\n对于非正态分布变量或者总体分布未知变量考虑使用 Spearman 等级相关(rank correlation)。将 \\(n\\) 对观察值 \\(X_i, Y_i\\) 由小到大编秩，则有：\n\n\\(r_s = 1 - \\frac{6\\sum_{i=1}^{n}d_{i}^2}{n(n^2-1)}\\)\n\\(t_r = \\frac{r - 0}{S_r},  \\upsilon = n -2 \\text{ Where } S_r = \\sqrt{\\frac{1-r^2}{n-2}} \\text{ When } n \\ge 20\\)"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#simple-linear-regression",
    "href": "note/2023-10-01-health-statistics/index.html#simple-linear-regression",
    "title": "Health Statistics",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n简单线性回归，即只有一个自变量的线性回归，其回归方程形式如下：\n\n\\(\\hat{Y} = a + bX\\)\n\\(b  = \\frac{\\sum{(X - \\bar{X})(Y - \\bar{Y})}}{\\sum{(X - \\bar{X})^2}}\\), \\(a = \\bar{Y} - b\\bar{X}\\)\n\n对参数 \\(b\\) 有如下 \\(F\\) 检验：\n\n\\(F = \\frac{MS_{\\text{regression}}}{MS_{\\text{residuals}}}, \\upsilon_{\\text{regression}} = 1, \\upsilon_{\\text{residuls}} = n - 2\\)\n\n对参数 \\(b\\) 同时有如下 \\(t\\) 检验：\n\n\\(t_b = \\frac{b-0}{S_b}, \\upsilon = n - 2 \\text{ Where } S_b = \\frac{\\sqrt{\\frac{SS_{\\text{residules}}}{n - 2}}}{\\sqrt{\\sum{(X - \\bar{X})^2}}}\\)\n\\(t_b \\pm t_{\\alpha, \\upsilon}(S_b)\\)\n\n对于拟合程度的判断可由决定系数确定：\n\n\\(R^2 = \\frac{SS_\\text{regressiion}}{SS_{\\text{total}}}\\)\n\n对于拟合的回归方程，确定数值 \\(X_0\\), 那么有：\n\n\\(\\mu_{Y|X_0} = \\hat{Y_0} \\pm t_{\\alpha, \\upsilon} S\\sqrt{\\frac{1}{n} + \\frac{(X - X_0)^2}{\\sum{(X-\\bar{X})^2}}}\\)\n\\(Y_0 = \\hat{Y_0} \\pm t_{\\alpha, \\upsilon}S\\sqrt{1 + \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{\\sum{(X - \\bar{X})^2}}}\\)\n\n利用上面的第二个式子可以进行统计控制，即如果想要将 \\(Y\\) 控制在一定范围，则可以通过控制\\(X\\) 将个体预测值控制在一定范围之中。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#假设检验原理",
    "href": "note/2023-10-01-health-statistics/index.html#假设检验原理",
    "title": "Health Statistics",
    "section": "假设检验原理",
    "text": "假设检验原理\n求得样本统计量的分布，则可求得 \\(H_0\\) 下样本统计量向假设的预期偏离如此大范围的机率，则依据小概率定律，选择拒绝或者接受原假设。\n但是实际上要求得统计量的分布并不容易，对于参数统计，我们常常需要假设其分布，然后推理出相关统计量的分布；非参数统计则需要对其编秩，然后推断出秩和的分布。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#假设检验与置信区间",
    "href": "note/2023-10-01-health-statistics/index.html#假设检验与置信区间",
    "title": "Health Statistics",
    "section": "假设检验与置信区间",
    "text": "假设检验与置信区间\n对于 \\(t/z \\space test\\) 而言，统计量 \\(t/z\\) 的大小表示为向均值偏离多少个标准差, 而置信区间的形式则为均值加上多少个标准差。\n\n\\(t/z = \\frac{\\bar{x} - \\mu_{\\bar{x}}}{\\sigma_{\\bar{x}}}\\)\n\\(\\mu_{95\\%} = \\bar{x} \\pm \\frac{t}{z_{95\\%}} \\cdot \\sigma_{\\bar{x}}\\)\n\n置信区间的含义在于用100个样本均值计算得到置信区间，其范围包括总体均值的个数为95，但是对个一个样本均值计算出来的区间，其是否包括该总体均值只有是或者不是，而不是有 95%的概率包括，因为该区间以及总体均值都是确定的值。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#i-and-ii-type-error-and-power",
    "href": "note/2023-10-01-health-statistics/index.html#i-and-ii-type-error-and-power",
    "title": "Health Statistics",
    "section": "I and II type error and power",
    "text": "I and II type error and power\n\\(H_0\\) 没有差异为阴性结果，\\(H_1\\) 为具有差异，阳性结果。I 型错误是拒绝 \\(H_0\\) 所犯的错误为 \\(\\alpha\\), II 型错误为不拒绝 \\(H_0\\) 所犯的错误为 \\(\\beta\\), 其含义为在 \\(H_1\\) 得到该统计量的概率，\\(1 - \\beta\\) 即为检验功效，即在 \\(H_1\\) 为真的情况下，在指定的 \\(\\alpha\\) 检出其存在差异的概率，即敏感度(sensitivity)."
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#统计量分布计算的bootstrap以及置换检验",
    "href": "note/2023-10-01-health-statistics/index.html#统计量分布计算的bootstrap以及置换检验",
    "title": "Health Statistics",
    "section": "统计量分布计算的Bootstrap以及置换检验",
    "text": "统计量分布计算的Bootstrap以及置换检验\n但是在实际的假设检验过程，倘若需要求得样本统计量的分布，往往需要知晓总体的概率分布。由此在假设检验过程之中，除了零假设以外，实际上在样本统计量的分布的计算之中，同时隐含了总体的分布以及方差齐性等假设，而这些假设也有对应的检验方法。因此，假设检验真正的概率应该所有假设概率的乘积，而不仅仅是在零假设之下所计算出的概率。以上所考虑的是一定要得到样本统计量的确切分布，进而得到精确的概率，例如正态总体假设下，方差齐性的 \\(t test\\), 但是很多情况下我们不必得到样本统计量的精确分布，得到一个大概估计即可，例如大样本下的正态检验，\\(R\\times C\\) 列表值用卡方分布近似，非参数检验-秩检验等。在计算机时代，我们可以直接利用Bootstrp模拟得到假设之下样本统计量的分布。"
  },
  {
    "objectID": "note/2023-10-01-health-statistics/index.html#假设检验与线性回归参数检验的等效性",
    "href": "note/2023-10-01-health-statistics/index.html#假设检验与线性回归参数检验的等效性",
    "title": "Health Statistics",
    "section": "假设检验与线性回归参数检验的等效性",
    "text": "假设检验与线性回归参数检验的等效性"
  },
  {
    "objectID": "note/2022-10-07-xgboost/index.html",
    "href": "note/2022-10-07-xgboost/index.html",
    "title": "Xgboost",
    "section": "",
    "text": "考虑一个二叉树，如何生长？最大化父节点的样本集合的损失值与其之下的两个子节点的样本集合损失值的差值。\n\n\n变量竞争准则：\n\\[\n\\Delta_{SST} = \\sum_{i \\in father\\ node}(y_i - \\overline{y})^2 - \\left\\{ \\sum_{i\\in son\\ node\\ 1}(y_i - \\overline{y})^2 + \\sum_{i\\in son\\ node\\ 2}(y_i - \\overline{y})^2 \\right\\}\n\\]\n拆分变量以及生长：\n\n变量内部寻找最佳分割。数值变量一般遍历样本之间的均值找到该变量使得\\(\\Delta_{SST}\\)最大的分割点\\(\\tau\\)，而分类变量遍历所有的组合找使得\\(\\Delta\\)最大的\\(\\mathcal{A}\\)以及其补集\\(\\mathcal{A}^c\\)。\n变量之间寻找最佳拆分变量。比较每个变量最佳分割的\\(\\Delta_{SST}\\)的大小，拥有最大\\(\\Delta\\)的变量，作为拆分分量，其\\(\\tau\\)或\\(\\mathcal{A}\\)作为拆分准则。\n重复1，2。根据不同的准则，停止生长。比如总\\(R^2\\)增长不会大于复杂性参数(complexity parameter, cp)的某个值时，或者到了分叉的限制点，或者某个节点的观测值太少。\n\nNote: 对于回归树而言，样本在某个节点的预测值就是该节点样本集合的均值；每个节点下面如果停止生长，那么序号会保留。\n\n\n\n决策树拆分变量以及生长的过程与回归树类似，只不过竞争的准则与回归有所不同。\n假定观测值一共有\\(K\\)类，在一个节点的观测值中属于第\\(i\\)类的比例为\\(p_k(k = 1, 2, ..., K)\\). 显然\\(\\sum_{k=1}^{K} p_k = 1\\). 常有的准则有下面几种：\n\n误分率 按照少数服从多数的原则，在树的某叉中，某一类\\(i\\)的数目是最多的，则类\\(i\\)被认为是分类正确的，那么误分率为\\(1 - p_i\\)\n熵 定义为 —— \\(\\sum_{k=1}^{K}p_{k}log_{2}p_{k}\\). 在所有的观测值都为一类的时候，熵为0. 因此所选择的拆分变量是使得父节点的熵和子节点的熵差别（称为信息增益(information gain)）最大的变量。子节点的熵应为各个子节点的数目比例对各个节点熵的加权平均。\nGini 不纯度（或Gini指数）定义为\n\\[\n\\sum_{k=1}^{K}p_{k}(1 -  p_k) = \\sum_{k=1}^{K}p_{k} - \\sum_{k=1}^{K}p_{k}^2 = 1 - \\sum_{i=1}^{K}p_{k}^{2}\n\\]\n在所有的观测值都为一类的时候，Gini不纯度的度量都为0.因此，所选择的拆分变量是使得父节点Gini不纯度和子节点的Gini不纯度差别最大的变量。子节点的Gini不纯度应该用各个子节点观测值数目比例对各个节点的Gini不纯度的加权平均来计算。\n\n何时停止树的生长按照某些确定的误判函数计算。例如控制参数cp，如果纯度改变达不到它的值，就不会再分叉了。另外还有一个复杂性（complexity parameter）的量 \\(\\alpha \\in [0, \\infty)\\), 计算每增加一个变量到模型中的损失。\n再如，从长成的树的终节点开始剪枝，每剪一次，看看误差是不是增加，如果超过要求则停止剪枝。\n另一种剪枝原则是使得下式最小：\n\\[\n\\frac{树T减去其子树t后的误差 - 树T的误差}{树T的的终节点数目 - 树T减去其子树t后的终节点数目}\n\\]\n其实本质就是最大化剪去一个节点能够减少的误差。"
  },
  {
    "objectID": "note/2022-10-07-xgboost/index.html#回归树",
    "href": "note/2022-10-07-xgboost/index.html#回归树",
    "title": "Xgboost",
    "section": "",
    "text": "变量竞争准则：\n\\[\n\\Delta_{SST} = \\sum_{i \\in father\\ node}(y_i - \\overline{y})^2 - \\left\\{ \\sum_{i\\in son\\ node\\ 1}(y_i - \\overline{y})^2 + \\sum_{i\\in son\\ node\\ 2}(y_i - \\overline{y})^2 \\right\\}\n\\]\n拆分变量以及生长：\n\n变量内部寻找最佳分割。数值变量一般遍历样本之间的均值找到该变量使得\\(\\Delta_{SST}\\)最大的分割点\\(\\tau\\)，而分类变量遍历所有的组合找使得\\(\\Delta\\)最大的\\(\\mathcal{A}\\)以及其补集\\(\\mathcal{A}^c\\)。\n变量之间寻找最佳拆分变量。比较每个变量最佳分割的\\(\\Delta_{SST}\\)的大小，拥有最大\\(\\Delta\\)的变量，作为拆分分量，其\\(\\tau\\)或\\(\\mathcal{A}\\)作为拆分准则。\n重复1，2。根据不同的准则，停止生长。比如总\\(R^2\\)增长不会大于复杂性参数(complexity parameter, cp)的某个值时，或者到了分叉的限制点，或者某个节点的观测值太少。\n\nNote: 对于回归树而言，样本在某个节点的预测值就是该节点样本集合的均值；每个节点下面如果停止生长，那么序号会保留。"
  },
  {
    "objectID": "note/2022-10-07-xgboost/index.html#决策树",
    "href": "note/2022-10-07-xgboost/index.html#决策树",
    "title": "Xgboost",
    "section": "",
    "text": "决策树拆分变量以及生长的过程与回归树类似，只不过竞争的准则与回归有所不同。\n假定观测值一共有\\(K\\)类，在一个节点的观测值中属于第\\(i\\)类的比例为\\(p_k(k = 1, 2, ..., K)\\). 显然\\(\\sum_{k=1}^{K} p_k = 1\\). 常有的准则有下面几种：\n\n误分率 按照少数服从多数的原则，在树的某叉中，某一类\\(i\\)的数目是最多的，则类\\(i\\)被认为是分类正确的，那么误分率为\\(1 - p_i\\)\n熵 定义为 —— \\(\\sum_{k=1}^{K}p_{k}log_{2}p_{k}\\). 在所有的观测值都为一类的时候，熵为0. 因此所选择的拆分变量是使得父节点的熵和子节点的熵差别（称为信息增益(information gain)）最大的变量。子节点的熵应为各个子节点的数目比例对各个节点熵的加权平均。\nGini 不纯度（或Gini指数）定义为\n\\[\n\\sum_{k=1}^{K}p_{k}(1 -  p_k) = \\sum_{k=1}^{K}p_{k} - \\sum_{k=1}^{K}p_{k}^2 = 1 - \\sum_{i=1}^{K}p_{k}^{2}\n\\]\n在所有的观测值都为一类的时候，Gini不纯度的度量都为0.因此，所选择的拆分变量是使得父节点Gini不纯度和子节点的Gini不纯度差别最大的变量。子节点的Gini不纯度应该用各个子节点观测值数目比例对各个节点的Gini不纯度的加权平均来计算。\n\n何时停止树的生长按照某些确定的误判函数计算。例如控制参数cp，如果纯度改变达不到它的值，就不会再分叉了。另外还有一个复杂性（complexity parameter）的量 \\(\\alpha \\in [0, \\infty)\\), 计算每增加一个变量到模型中的损失。\n再如，从长成的树的终节点开始剪枝，每剪一次，看看误差是不是增加，如果超过要求则停止剪枝。\n另一种剪枝原则是使得下式最小：\n\\[\n\\frac{树T减去其子树t后的误差 - 树T的误差}{树T的的终节点数目 - 树T减去其子树t后的终节点数目}\n\\]\n其实本质就是最大化剪去一个节点能够减少的误差。"
  },
  {
    "objectID": "note/2022-10-07-xgboost/index.html#xgboost",
    "href": "note/2022-10-07-xgboost/index.html#xgboost",
    "title": "Xgboost",
    "section": "xgboost",
    "text": "xgboost\n构造目标函数\n假设已经训练了\\(K\\)棵树，则对于第\\(i\\)个样本（最终)预测值为： \\[\\hat{y}_{i} = \\sum_{k=1}^{K}f_{k}(x_i), f_k \\in \\mathcal{F}\\] 目标函数： \\[\nobj = \\sum_{i=1}^{n}l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K}\\Omega(f_k)\n\\] 对于\\(K\\)个模型的优化是叠加式训练；给定\\(x_i\\)： \\(\\hat{y_i}^{(0)}= 0 \\leftarrow\\) (default case)\n\\(\\hat{y_i}^{(1)}= \\hat{y_i}^{(0)} + f_{1}(x_i)\\) \\(\\hat{y_i}^{(2)}= \\hat{y_i}^{(0)} + f_{1}(x_i) + f_{2}(x_i)\\)\n…… \\(\\hat{y_i}^{(k)}= \\hat{y_i}^{(0)} + f_{1}(x_i) + f_{2}(x_i) + ... + f_{k-1}(x_i) + f_{k}(x_i) \\rightarrow \\hat{y_i}^{(k)} = \\hat{y}^{(k-1)} + f_{k}(x_k)\\) 当训练第\\(k\\)树时，目标函数写作： \\[\nobj_k = \\sum_{i=1}^{n}l(y_i, \\hat{y}^{(k-1)} + f_{k}(x_k)) + \\sum_{j=1}^{K-1}\\Omega(f_j) + \\Omega(f_k) \\rightarrow  \\sum_{i=1}^{n}l(y_i, \\hat{y}^{(k-1)} + f_{k}(x_k)) +  \\Omega(f_k)\n\\]\n目标函数直接优化难，如何近似?（talor expansion)\n\\[obj_k = \\sum_{i}^{n} \\left[l(y_i, \\hat{y_i}^{(k-1)}) + \\partial_{\\hat{y_i}^{(k-1)})}l(y_i, \\hat{y_i}^{(k-1)}) *f_{k}(x_i) + \\frac{1}{2}\\partial_{\\hat{y_i}^{(k-1)})}^{2}l(y_i, \\hat{y_i}^{(k-1)}) *f_{k}^{2}(x_i)\\right] + \\Omega(f_k)  \\\\\n= \\sum_{i}^{n} \\left[ g_{i} *f_{k}(x_i) + \\frac{1}{2}h_i*f_{k}^{2}(x_i)\\right] + \\Omega(f_k)\n\\]\nNote: \\(g_i\\)以及\\(h_i\\)就是残差的一种形式，传递给第\\(k\\)个模型训练的方向；记住\\(g_i\\)以及\\(h_i\\)是样本\\(i\\)具有两个常量可以帮助后面的理解。\n如何把树的结构引入目标函数\n参数化一棵树： 定义： \\(q(x_i) \\rightarrow\\) 样本\\(x_i\\)在树的哪个叶节点。 \\(\\omega \\rightarrow\\) 叶节点的权重值（值） 那么\\(\\omega_{q(x_i)} \\rightarrow\\) 样本\\(x_i\\)的预测值\\(f_k(x_i) = \\omega_{q(x_i)}\\) \\(I_j \\rightarrow\\) 在叶子节点\\(j\\)的样本集合。 \\[\\Omega(f_k) = \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}\\omega_j\\]\n新的目标函数： \\[\n  obj_k = \\sum_{i}^{n} \\left[ g_{i} *f_{k}(x_i) + \\frac{1}{2}h_i*f_{k}^{2}(x_i)\\right] + \\Omega(f_k) \\\\ = \\sum_{i}^{n} \\left[ g_{i} *\\omega_{q(x_i)} + \\omega_{q(x_i)}^{2}(x_i)\\right] + \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}\\omega_j \\\\ = \\sum_{j}^T \\left[(\\sum_{i \\in I_j}g_i)\\omega_j + \\frac{1}{2}(\\sum_{i \\in I_j}h_i + \\lambda)\\omega_{j}^2 \\right] + \\gamma T\n\\]\nNote：这里的关键在于上面提到的\\(g_i\\)以及\\(h_i\\)是样本\\(i\\)所具有的一个常量，观察目标函数可以知道，其值就是每个样本的\\(g_i\\)和\\(h_i\\)与其叶子节点\\(\\omega_j\\)值的乘积之和。目标函数形式的改变就是从样本遍历其和变为从叶子节点遍历其和。 定义\\(G_j = \\sum_{i \\in I_j}g_{i}\\), \\(H_j = \\sum_{i \\in I_j}h_{i}\\): \\[\nobj_k = \\sum_{j=1}^T \\left[ G_{i}\\omega_j + \\frac{1}{2}(H_{i} + \\lambda)\\omega_{j}^2 \\right] + \\gamma T\n\\] Note: 可以看到最后的目标函数就是，某个叶子节点的样本集合一阶导数之和该节点权重值 + 二阶导数之和该节点权重值的平方。注意\\(G_i\\)以及\\(H_i\\)的都是常数。\n利用贪心算法求解树的生长过程。\n注意\\(G_i\\)以及\\(H_i\\)的都是常数，最后的目标函数实际熵就是一个形如\\(b + ax^2\\)的二次函数，二次函数为了取得最大值，其\\(x\\)取值为：\\(\\omega_{j}* = - \\frac{G_i}{H_i + \\lambda} + \\gamma T\\)，那么目标函数取值为\\(-\\frac{1}{2} \\sum_{j=1}^{T}\\frac{G_{i}^2}{H_i + \\lambda}\\)。如之前提到的决策树的生长，这里的生长，和其类似，不过分割的准则变成了\\(obj\\)；每一次分割就是使得\\(\\Delta_{obj}\\)最大的方向。"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html",
    "title": "Probability Distribution",
    "section": "",
    "text": "多重伯努利实验中，已知事件 \\(A\\) 成功的概率为 \\(p\\)，且实验次数 \\(n\\) 固定 ，那么随机变量 \\(X\\) —— 事件 \\(A\\) 发生次数 \\(X\\) ： \\[ P(X = k) = C_n^k p^k(1-p)^{n-k}, k = 0,1,...,n. \\]\n记为： \\[\nX \\sim b(n,p) \\text{ Where } E(X) = np, D(X) = np(1 - p)\n\\]\n\ncurve(dbinom(x, 100, 0.3), 0, 80, col = \"red\")\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 0.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 1.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 2.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 3.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 4.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 5.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 6.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 7.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 8.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 9.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 10.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 11.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 12.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 13.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 14.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 15.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 16.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 17.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 18.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 19.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 20.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 21.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 22.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 23.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 24.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 25.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 26.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 27.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 28.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 29.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 30.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 31.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 32.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 33.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 34.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 35.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 36.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 37.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 38.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 39.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 40.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 41.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 42.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 43.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 44.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 45.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 46.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 47.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 48.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 49.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 50.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 51.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 52.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 53.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 54.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 55.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 56.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 57.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 58.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 59.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 60.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 61.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 62.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 63.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 64.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 65.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 66.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 67.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 68.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 69.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 70.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 71.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 72.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 73.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 74.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 75.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 76.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 77.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 78.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 79.200000\n\ncurve(dbinom(x, 100, 0.5), 0, 80, col = \"blue\", add = TRUE)\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 0.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 1.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 2.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 3.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 4.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 5.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 6.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 7.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 8.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 9.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 10.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 11.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 12.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 13.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 14.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 15.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 16.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 17.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 18.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 19.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 20.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 21.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 22.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 23.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 24.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 25.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 26.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 27.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 28.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 29.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 30.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 31.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 32.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 33.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 34.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 35.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 36.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 37.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 38.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 39.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 40.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 41.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 42.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 43.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 44.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 45.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 46.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 47.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 48.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 49.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 50.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 51.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 52.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 53.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 54.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 55.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 56.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 57.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 58.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 59.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 60.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 61.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 62.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 63.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 64.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 65.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 66.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 67.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 68.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 69.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 70.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 71.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 72.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 73.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 74.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 75.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 76.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 77.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 78.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 79.200000\n\nlegend(\"topright\",\n  legend = paste0(\"probe = \", c(0.3, 0.5)),\n  text.col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\n\ncurve(pbinom(x, 100, 0.3), 0, 80, col = \"red\")\ncurve(pbinom(x, 100, 0.5), 0, 80, col = \"blue\", add = TRUE)\nlegend(\"topleft\",\n  legend = paste0(\"probe = \", c(0.3, 0.5)),\n  text.col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\n两点分布(Bernoulli Distribution) ，即一重伯努利实验,为二项分布的特殊分布。\n\n\n\n多重伯努利实验中，已知事件 \\(A\\) 发生的概率为 \\(p\\)，那么当事件 \\(A\\) 第 \\(r\\) 次发生，那么随机变量 \\(X\\) —— 伯努利实验次数： \\[\nP(X = K) = C_{k-1}^{r-1}p^r(1-p)^{k-r}, k = r,r+1,...\n\\]\n记作： \\[\nX \\sim Nb(r,p), \\text{ Where } E(X) = \\frac{r}{p}, D(X) = \\frac{r(1-p)}{p^2}\n\\]\n几何分布(Geometric Distrirution)为负二项分布的特殊分布 ，即当 \\(r = 1\\) 时的负二项分布。\n记为：\n\\[X \\sim Ge(p)\\]\n\n\n\n不放回的随机抽样，设有\\(N\\)件产品，其中中\\(M\\)件不合格品，从中不放回的随机抽取\\(n\\)件，则其中的不合格的件数服从超几何分布： \\[P(X = k) = \\frac{C_M^K C_{N-M}^{n-k}}{C_N^n} \\]\n记为：\\(X \\sim h(n,N,M)\\) \\[E(X) = n\\frac{M}{N}\\] \\[D(X) = \\frac{nM(N-M)(N-n)}{N^2(N-1)}\\]\n\n\n\n涉及到单位时间，面积，体积的计数过程，数量\\(X\\): \\[ P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}  \\]\n记为： \\[ X \\sim  P(\\lambda) \\] \\[E(X) = \\lambda\\] \\[D(X) = \\lambda\\]"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#二项分布binomial-distribution",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#二项分布binomial-distribution",
    "title": "Probability Distribution",
    "section": "",
    "text": "多重伯努利实验中，已知事件 \\(A\\) 成功的概率为 \\(p\\)，且实验次数 \\(n\\) 固定 ，那么随机变量 \\(X\\) —— 事件 \\(A\\) 发生次数 \\(X\\) ： \\[ P(X = k) = C_n^k p^k(1-p)^{n-k}, k = 0,1,...,n. \\]\n记为： \\[\nX \\sim b(n,p) \\text{ Where } E(X) = np, D(X) = np(1 - p)\n\\]\n\ncurve(dbinom(x, 100, 0.3), 0, 80, col = \"red\")\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 0.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 1.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 2.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 3.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 4.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 5.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 6.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 7.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 8.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 9.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 10.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 11.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 12.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 13.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 14.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 15.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 16.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 17.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 18.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 19.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 20.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 21.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 22.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 23.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 24.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 25.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 26.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 27.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 28.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 29.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 30.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 31.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 32.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 33.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 34.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 35.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 36.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 37.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 38.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 39.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 40.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 41.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 42.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 43.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 44.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 45.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 46.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 47.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 48.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 49.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 50.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 51.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 52.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 53.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 54.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 55.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 56.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 57.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 58.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 59.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 60.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 61.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 62.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 63.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 64.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 65.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 66.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 67.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 68.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 69.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 70.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 71.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 72.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 73.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 74.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 75.200000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 76.800000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 77.600000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 78.400000\n\n\nWarning in dbinom(x, 100, 0.3): non-integer x = 79.200000\n\ncurve(dbinom(x, 100, 0.5), 0, 80, col = \"blue\", add = TRUE)\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 0.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 1.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 2.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 3.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 4.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 5.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 6.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 7.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 8.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 9.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 10.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 11.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 12.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 13.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 14.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 15.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 16.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 17.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 18.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 19.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 20.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 21.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 22.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 23.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 24.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 25.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 26.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 27.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 28.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 29.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 30.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 31.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 32.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 33.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 34.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 35.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 36.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 37.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 38.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 39.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 40.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 41.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 42.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 43.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 44.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 45.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 46.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 47.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 48.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 49.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 50.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 51.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 52.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 53.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 54.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 55.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 56.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 57.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 58.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 59.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 60.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 61.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 62.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 63.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 64.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 65.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 66.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 67.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 68.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 69.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 70.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 71.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 72.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 73.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 74.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 75.200000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 76.800000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 77.600000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 78.400000\n\n\nWarning in dbinom(x, 100, 0.5): non-integer x = 79.200000\n\nlegend(\"topright\",\n  legend = paste0(\"probe = \", c(0.3, 0.5)),\n  text.col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\n\ncurve(pbinom(x, 100, 0.3), 0, 80, col = \"red\")\ncurve(pbinom(x, 100, 0.5), 0, 80, col = \"blue\", add = TRUE)\nlegend(\"topleft\",\n  legend = paste0(\"probe = \", c(0.3, 0.5)),\n  text.col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\n两点分布(Bernoulli Distribution) ，即一重伯努利实验,为二项分布的特殊分布。"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#负二项分布negative-binomial-distribution",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#负二项分布negative-binomial-distribution",
    "title": "Probability Distribution",
    "section": "",
    "text": "多重伯努利实验中，已知事件 \\(A\\) 发生的概率为 \\(p\\)，那么当事件 \\(A\\) 第 \\(r\\) 次发生，那么随机变量 \\(X\\) —— 伯努利实验次数： \\[\nP(X = K) = C_{k-1}^{r-1}p^r(1-p)^{k-r}, k = r,r+1,...\n\\]\n记作： \\[\nX \\sim Nb(r,p), \\text{ Where } E(X) = \\frac{r}{p}, D(X) = \\frac{r(1-p)}{p^2}\n\\]\n几何分布(Geometric Distrirution)为负二项分布的特殊分布 ，即当 \\(r = 1\\) 时的负二项分布。\n记为：\n\\[X \\sim Ge(p)\\]"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#超几何分布",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#超几何分布",
    "title": "Probability Distribution",
    "section": "",
    "text": "不放回的随机抽样，设有\\(N\\)件产品，其中中\\(M\\)件不合格品，从中不放回的随机抽取\\(n\\)件，则其中的不合格的件数服从超几何分布： \\[P(X = k) = \\frac{C_M^K C_{N-M}^{n-k}}{C_N^n} \\]\n记为：\\(X \\sim h(n,N,M)\\) \\[E(X) = n\\frac{M}{N}\\] \\[D(X) = \\frac{nM(N-M)(N-n)}{N^2(N-1)}\\]"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#泊松分布possion-distribution",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#泊松分布possion-distribution",
    "title": "Probability Distribution",
    "section": "",
    "text": "涉及到单位时间，面积，体积的计数过程，数量\\(X\\): \\[ P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}  \\]\n记为： \\[ X \\sim  P(\\lambda) \\] \\[E(X) = \\lambda\\] \\[D(X) = \\lambda\\]"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#正态分布",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#正态分布",
    "title": "Probability Distribution",
    "section": "正态分布",
    "text": "正态分布\n正态分布含有两个参数 \\(\\mu\\)，\\(\\sigma\\), 其中 \\(\\mu\\) 为位置参数，控制曲线在 \\(x\\) 轴上的位置；\\(\\sigma\\)为尺度参数，用于控制曲线的参数。 记为：\n\\[X \\sim N(\\mu,\\sigma)\\] \\[E(X) = \\mu\\] \\[D(X) = \\sigma^2\\]\n概率密度函数： \\[ p(x) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} e^{- \\frac{(x - \\mu)^2}   {2\\sigma^2}} \\]\n\ncurve(dnorm(x), from = -4, 4, col = \"red\")\ncurve(dnorm(x, 0, 2), from = -4, 4, add = TRUE, col = \"blue\")\nlegend(\n  \"topright\",\n  paste0(\"mean = 0, sd = \", c(1, 2)),\n  text.col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\n分布函数： \\[ F(x) = \\int_{-\\infty}^x p(t)dt = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{- \\frac{(t-\\mu^2)}{2\\sigma}}dt \\]\n\ncurve(pnorm(x, 0, 1), from = -4, 4, col = \"red\")\ncurve(pnorm(x, 0, 2), from = -4, 4, add = TRUE, col = \"blue\")\nlegend(\n  \"topright\",\n  paste0(\"mean = 0, sd = \", c(1, 2)),\n  text.col = c(\"red\", \"blue\")\n)"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#均匀分布",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#均匀分布",
    "title": "Probability Distribution",
    "section": "均匀分布",
    "text": "均匀分布\n记为： \\[X \\sim U(a,b)\\] \\[E(X) = \\frac{a+b}{2}\\] \\[D(X) = \\frac{(b-a)^2}{12}\\]\n\\[\nf(x) = \\begin{cases}\n\\frac{1}{b - a} & \\text{for } a \\leq x \\leq b, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]\n\ncurve(dunif(x), -1, 2, col = \"red\")\nlegend(\"topright\",\n  legend = \"min = 0, max = 1\",\n  text.col = \"red\"\n)\n\n\n\n\n\n\n\n\n\\[\nF(x) = \\begin{cases}\n0 & \\text{for } x &lt; a, \\\\\n\\frac{x - a}{b - a} & \\text{for } a \\leq x &lt; b, \\\\\n1 & \\text{for } x \\geq b.\n\\end{cases}\n\\]\n\ncurve(punif(x), -1, 2, col = \"red\")\nlegend(\"topleft\",\n  legend = \"min = 0, max = 1\",\n  text.col = \"red\"\n)"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#指数分布",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#指数分布",
    "title": "Probability Distribution",
    "section": "指数分布",
    "text": "指数分布\n记为： \\[X \\sim Exp(\\lambda)\\]\n\\[E(X) = \\frac{1}{\\lambda}\\]\n\\[D(x) = \\frac{1}{\\lambda^2}\\]\n密度函数\n\\[\nf(x; \\lambda) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\geq 0 \\text{ and } \\lambda &gt; 0.\n\\]\n\ncurve(dexp(x), 0, 5, col = \"red\")\ncurve(dexp(x, rate = 2), 0, 5, col = \"blue\", add = TRUE)\nlegend(\"topright\",\n  legend = paste0(\"rate = \", c(1, 2)),\n  text.col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\n\\[\nF(x; \\lambda) = 1 - e^{-\\lambda x} \\quad \\text{for } x \\geq 0 \\text{ and } \\lambda &gt; 0.\n\\]\n\ncurve(pexp(x), 0, 5, col = \"red\")\ncurve(pexp(x, rate = 2), 0, 5, col = \"blue\", add = TRUE)\nlegend(\"topleft\",\n  legend = paste0(\"rate = \", c(1, 2)),\n  text.col = c(\"red\", \"blue\")\n)"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#gamma-分布",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#gamma-分布",
    "title": "Probability Distribution",
    "section": "\\(\\Gamma\\) 分布",
    "text": "\\(\\Gamma\\) 分布\n记为：\\(X \\sim Ga(\\alpha,\\lambda)\\) \\(E(X) = \\frac{\\alpha}{\\lambda}\\)， \\(D(X) = \\frac{\\alpha}{\\lambda^2}\\)\n密度函数\n\\[\nf(x; k, \\theta) = \\frac{x^{k-1}e^{-\\frac{x}{\\theta}}}{\\theta^k \\Gamma(k)} \\quad \\text{for } x &gt; 0 \\text{ and } k, \\theta &gt; 0.\n\\]\n其中，\\(k\\) 是形状参数（也称为度数），\\(\\theta\\) 是尺度参数（与标准差成比例），而 \\(\\Gamma(k)\\) 是伽马函数。\n\ncurve(dgamma(x, shape = 0), 0, 5, col = \"red\")\ncurve(dgamma(x, shape = 1), 0, 5, col = \"blue\", add = TRUE)\ncurve(dgamma(x, shape = 2), 0, 5, col = \"green\", add = TRUE)\nlegend(\"topright\",\n  legend = paste0(\"shape = \", c(0, 1, 2)),\n  text.col = c(\"red\", \"blue\", \"green\")\n)\n\n\n\n\n\n\n\n\n分布函数\n\\[\nF(x; k, \\theta) = \\int_0^x \\frac{t^{k-1}e^{-\\frac{t}{\\theta}}}{\\theta^k \\Gamma(k)} dt = \\frac{\\gamma(k, \\frac{x}{\\theta})}{\\Gamma(k)} \\quad \\text{for } x &gt; 0 \\text{ and } k, \\theta &gt; 0.\n\\]\n\ncurve(pgamma(x, shape = 0), 0, 5, col = \"red\")\ncurve(pgamma(x, shape = 1), 0, 5, col = \"blue\", add = TRUE)\ncurve(pgamma(x, shape = 2), 0, 5, col = \"green\", add = TRUE)\nlegend(\"bottomright\",\n  legend = paste0(\"shape = \", c(0, 1, 2)),\n  text.col = c(\"red\", \"blue\", \"green\")\n)"
  },
  {
    "objectID": "note/2022-06-10-probability-distribution.qmd/index.html#beta-分布",
    "href": "note/2022-06-10-probability-distribution.qmd/index.html#beta-分布",
    "title": "Probability Distribution",
    "section": "\\(\\beta\\) 分布",
    "text": "\\(\\beta\\) 分布\n记为：\\(X \\sim Be(a,b)\\) \\(E(X) = \\frac{a}{a+b}\\)， \\(D(x) = \\frac{ab}{(a+b)^2 (a+b+1)}\\)\n密度函数\n\\[f(x; \\alpha, \\beta) = \\frac{x^{\\alpha - 1}(1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)} \\quad \\text{for } 0 &lt; x &lt; 1 \\text{ and } \\alpha, \\beta &gt; 0,\\]\n分布函数\n\\[\nF(x; \\alpha, \\beta) = I_x(\\alpha, \\beta) = \\frac{B_x(\\alpha, \\beta)}{B(\\alpha, \\beta)} \\quad \\text{for } 0 \\leq x \\leq 1 \\text{ and } \\alpha, \\beta &gt; 0\n\\]"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html",
    "href": "note/2024-07-01-linear-algebra/index.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "矩阵为矩形数表；\n\\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\n0 & a_{22} & a_{23} \\\\\n0 & 0 & a_{33} \\\\\n\\end{bmatrix}\n\\]\n上三角矩阵 \\(U\\).\n\\[\n\\begin{bmatrix}\na_{11} & 0 & 0 \\\\\na_{21} & a_{22} & 0 \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\n下三角矩阵 \\(L\\).\n上下三角矩阵统称为三角矩阵。\n\\[\n\\begin{bmatrix}\na_{11} & 0 & 0 \\\\\n0 & a_{22} & 0 \\\\\n0 & 0 & a_{33} \\\\\n\\end{bmatrix}\n\\]\n对角矩阵 \\(\\Lambda\\).\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\]\n单位矩阵 \\(E, I\\).\n\n\n\n\\[ {c_{ij}=a_{ij}+b_{ij}} \\]\n\\[ C_{ij} = ac_{ij} \\]\n矩阵加法和数乘统称为矩阵的线性运算。\n\n\n\nThe element \\(C_{ij}\\) of the resulting matrix \\(C\\) is found by taking the dot product of the \\(i\\)th row of matrix \\(A\\) and the \\(j\\)th column of matrix \\(B\\). This can be expressed as:\n\\[C_{ij} = \\sum_{k=1}^{n} A_{ik} \\times B_{kj}\\]\nWhere \\(A_{ik}\\) is the element in the \\(i\\)th row and \\(k\\)th column of matrix \\(A\\), and \\(B_{kj}\\) is the element in the \\(k\\)th row and \\(j\\)th column of matrix \\(B\\).\n\n\\(AB = 0 \\not\\Rightarrow A = 0 \\text{ 或 } B = 0\\)\n\\(A \\cdot B \\neq B \\cdot A\\)\n\\(ABC = (AB)C\\)\n\\(A(B + C) = AB + AC\\)\n\\(k(AB) = A(kB) = ABk\\)\n\n空间位置不能变，时间次序可以变。\n\\[\n\\begin{aligned}\n2x_1 + 3x_2 &= 8 \\\\\nx_1 - x_2 &= 1 \\\\\n\\end{aligned}\n\\]\nCan be represented in matrix form as:\n\\[\n\\begin{bmatrix}\n2 & 3 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n8 \\\\\n1 \\\\\n\\end{bmatrix}\n\\]\n线性方程组的矩阵表示 系数矩阵，变量矩阵，常数矩阵。\n对于矩阵乘法有如下理解：\n\n左边乘一个矩阵为对该矩阵列向量的线性组合，线性组合的系数为右侧矩阵的列，该视角最为常用，可用于判断线性方程组解的情况；右乘一个矩阵为对该矩阵的行向量的线性组合，线性组合的系数为左侧矩阵的行， 该视角可用于消元。\n矩阵乘法也可理解为每一列乘每一行得到多个矩阵，然后相加。\n\n\n\n\n\\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\]\nThe transpose of \\(A\\), denoted as \\(A^T\\) is obtained by flipping \\(A\\) over its main diagonal, and is defined as:\n\\[\nA^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\]\n\n\\((A^T)^T = A\\)\n\\((A + B)^T = A^T + B^T\\)\n\\((kA)^T = kA^T\\)\n\\((AB)^T = B^TA^T\\)\n\n对称矩阵 \\(A^T = A\\), 一定为方阵。\n反对称矩阵 \\(A^T = -A\\) , 一定为方阵，对角线一定为0.\n\n\n\n非分块矩阵可以认为每个元素都是一个只有一个数的块。\n\\[\n\\alpha_1x_1 + \\alpha_2x_2 = b\n\\]\n分块矩阵可用于矩阵的抽象运算，利用分块矩阵表示线性方程组如上。\n\n\n\n\\[\nA_{n}B_{n} = B_{n}A_{n} = E_n\n\\]\n逆只能存在方阵之中，且不一定可逆。逆具有唯一性。\n\\[\n\\begin{aligned}\n(A^{-1})^{-1} &= A \\\\\n(A + B)^{-1} &= ? \\\\\n(kA)^{-1} &= k^{-1}A^{-1} \\\\\n(AB)^{-1} &= B^{-1}A^{-1} \\\\\n(A^{-1})^T &= (A^T)^{-1}\n\\end{aligned}\n\\]\n矩阵逆运算规律如上。矩阵的上标运算可以任意交换位置。\n矩阵初等变换 实际上就是方程组系数消元的抽象过程，初等变换的一系列矩阵称为等价矩阵。\n\\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\xrightarrow{\\text{行变换}}\n\\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\]\n初等方阵 单位矩阵 \\(E\\) 经过一次初等变换所形成的方阵\\(P\\)。 若 \\(P\\) 为初等方阵， \\(PA=B\\)， 则\\(A \\rightarrow B\\)的变化等价于 \\(E \\rightarrow P\\) 的初等行变换。若\\(AP = B\\)， 则 \\(A \\rightarrow B\\) 等价于 \\(E \\rightarrow P\\) 的初等列变换。实际上这里描述的是一个初等变换可由一个初等方阵来定义；从矩阵乘法的行视角来看，就是对右侧矩阵行向量的线性组合。\n那么有初等方阵的逆矩阵：\n\n通过交换单位矩阵的第 \\(i\\) 行和第 \\(j\\) 行得到的初等矩阵 \\(E_{ij}\\)， 其逆矩阵也是通过交换同样的两行得到的，即 \\(E_{ij}^{-1} = E_{ij}\\)。\n将单位矩阵的第 \\(i\\) 行乘以一个非零常数 \\(k\\) 得到的初等矩阵 \\(E_i(k)\\)，其逆矩阵是将第 \\(i\\) 行乘以 \\(1/k\\)，即 \\(E_i(k)^{-1} = E_i(1/k)\\)。\n将单位矩阵的第 \\(i\\) 行加上第 \\(j\\) 行的 \\(k\\) 倍得到的初等矩阵 \\(E_{ij}(k)\\)，其逆矩阵是将第 \\(i\\) 行减去第 \\(j\\) 行的 \\(k\\) 倍， 即 \\(E_{ij}(k)^{-1} = E_{ij}(-k)\\)。\n\n\\[\n\\begin{bmatrix}\na_{11} & a_{12} & 1 & 0 \\\\\na_{21} & a_{22} & 0 & 1\n\\end{bmatrix}\n\\xrightarrow{\\text{初等行变换}}\n\\begin{bmatrix}\n1 & 0 & a_{11}^{-1} & -a_{12}a_{11}^{-1} \\\\\n0 & 1 & -a_{21}a_{11}^{-1} & a_{11}^{-1}(a_{11}a_{22}-a_{12}a_{21})\n\\end{bmatrix}\n\\]\n逆矩阵的求法 将矩阵经过初等变换为单位矩阵，同样的变化作用于单位矩阵的结果就是其逆矩阵。\n\n\n\n逆序数 在一个排列 \\(a_1, a_2, \\ldots, a_n\\) 中，如果存在一对 \\(i, j\\)，满足 \\(i &lt; j\\) 且 \\(a_i &gt; a_j\\)，那么这对 \\((a_i, a_j)\\) 称为一个逆序对。排列中所有逆序对的个数称为该排列的逆序数。\n例如，对于排列 \\(3, 1, 2\\)，它的逆序对有 \\((3, 1)\\) 和 \\((3, 2)\\)，所以它的逆序数是 2。\n形式化地，设排列为 \\(\\sigma = (\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)\\)，则逆序数定义为：\n\\[ \\text{Inv}(\\sigma) = \\sum_{1 \\leq i &lt; j \\leq n} \\mathbf{1}_{\\sigma_i &gt; \\sigma_j} \\]\n其中 \\(\\mathbf{1}_{\\sigma_i &gt; \\sigma_j}\\) 是指示函数，当 \\(\\sigma_i &gt; \\sigma_j\\) 时取值为 1，否则取值为 0。\n余子式 设 \\(A\\) 是一个 \\(n \\times n\\) 的矩阵，\\(A_{ij}\\) 是 \\(A\\) 中去掉第 \\(i\\) 行和第 \\(j\\) 列后剩下的 \\((n-1) \\times (n-1)\\) 子矩阵。那么，\\(A\\) 的 \\((i, j)\\) 元素对应的余子式定义为：\n\\[ M_{ij} = \\det(A_{ij}) \\]\n其中 \\(\\det(A_{ij})\\) 表示 \\(A_{ij}\\) 的行列式。\n代数余子式 设 \\(A\\) 是一个 \\(n \\times n\\) 的矩阵，\\(A_{ij}\\) 是 \\(A\\) 中去掉第 \\(i\\) 行和第 \\(j\\) 列后剩下的 \\((n-1) \\times (n-1)\\) 子矩阵。那么，\\(A\\) 的 \\((i, j)\\) 元素对应的代数余子式定义为：\n\\[ C_{ij} = (-1)^{i+j} \\det(A_{ij}) \\]\n行列式 行列式（Determinant）是一个函数，将方阵映射到标量，其共有 \\(n!\\) 项, 其中每一项为不同行不同列元素的积；每一项的正负由排列的逆序数决定。 记作 \\(\\det(A)\\) 或 \\(|A|\\)。设 \\(A\\) 是一个 \\(n \\times n\\) 的方阵，那么 \\(A\\) 的行列式 \\(\\det(A)\\) 定义为：\n\\[\n\\det(A) = \\sum_{\\sigma \\in S_n} (\\text{sgn}(\\sigma) \\prod_{i=1}^{n} a_{i,\\sigma(i)})\n\\]\n其中，\\(S_n\\) 表示所有 \\(n\\) 个元素的排列，\\(\\sigma\\) 是一个排列，\\(\\text{sgn}(\\sigma)\\) 表示排列 \\(\\sigma\\) 的符号，\\(a_{i,\\sigma(i)}\\) 表示矩阵 \\(A\\) 在第 \\(i\\) 行第 \\(\\sigma(i)\\) 列的元素。对于 n 阶方阵 \\(A\\)，其行列式可通过以下方法计算：\n\n三角矩阵：对于三角矩阵，行列式是对角线上元素的代数余子式。\n展开法：通过任意一行（或列）的元素进行展开。例如，第 \\(i\\) 行的展开公式为：\n\\[\n\\det(A) = \\sum_{j=1}^{n} (-1)^{i+j} a_{ij} \\det(A_{ij})\n\\]\n其中，\\(a_{ij}\\) 是矩阵 \\(A\\) 中第 \\(i\\) 行第 \\(j\\) 列的元素，\\(A_{ij}\\) 是删除了第 \\(i\\) 行和第 \\(j\\) 列的子矩阵。\n如果按第 \\(i\\) 行展开得到代数余子式，分别乘以 第 \\(j (j \\neq i)\\) 行对应的元素，其等于0\n\n行列式的基本性质包括\n\n转置相等 \\(\\det(A) = \\det(A^T)\\)\n换行变号 交换行列式的两行（列），行列式变号。\n乘数乘行 行列式的某一行（列）乘以常数，行列式也乘以该常数。\n倍加相等 行列式中某一行（列）的常数倍加到另一行（列），行列式不变。\n拆分分行 行列式可以按行或列拆分。\n零性质 行列式中某一行（列）全为零，或有行或者列成比例，行列式为零。\n\n行列式运算规律如下\n\n\\(|AB| = |A||B|\\)\n\\(|kA| = k^n|A|\\)\n分块的对角矩阵的行列式等于对角线元素上的分块的行列式的乘积。\n\n行列式计算\n\n对角，三角行列式， 次(副)对角元素乘积 \\(\\rightarrow\\) 对角线乘积（正负由逆序数决定）\n分块矩阵的行列式。\n一杠一星，两杠一星行列式 \\(\\rightarrow\\) 只有对角元素项。\n箭头行列式，弓形行列式 \\(\\rightarrow\\) 列变换为第一列为0， 列变换为对角行列式。\n同行（列）行列式 \\(\\rightarrow\\) 列，行变换为箭头行列式。\n\\(X\\) 形行列式 \\(\\rightarrow\\)\n\\(ab\\) 矩阵。\n范德蒙行列式 \\(\\rightarrow\\) 确定不变行。\n\n\\[\nV_n = \\begin{vmatrix}\n1      & 1      & 1      & \\cdots & 1      \\\\\nx_1    & x_2    & x_3    & \\cdots & x_n    \\\\\nx_1^2  & x_2^2  & x_3^2  & \\cdots & x_n^2  \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_1^{n-1} & x_2^{n-1} & x_3^{n-1} & \\cdots & x_n^{n-1}\n\\end{vmatrix}\n\\]\n\n\n\n伴随矩阵 伴随矩阵是与原矩阵密切相关的一个矩阵，它由原矩阵的代数余子式构成。对于一个 n 阶方阵 \\(A\\)，其伴随矩阵记作 \\(adj(A)\\)，它的每个元素 \\(a_{ij}\\) 都是由原矩阵 \\(A\\) 的代数余子式 \\(C_{ij}\\) 替换得到的。具体来说，伴随矩阵的第 \\(i\\) 行第 \\(j\\) 列的元素是原矩阵第 \\(j\\) 行第 \\(i\\) 列的代数余子式，即 \\(adj(A)_{ij} = C_{ji}\\)。\n伴随矩阵与原矩阵的行列式之间有如下关系：\n\\[\nA \\cdot adj(A) = adj(A) \\cdot A  = \\det(A) \\cdot I\n\\]\n其中，\\(I\\) 是单位矩阵，\\(\\det(A)\\) 是矩阵 \\(A\\) 的行列式。这个关系表明，伴随矩阵可以用于计算矩阵的逆，当 \\(\\det(A) \\neq 0\\) 时，矩阵 \\(A\\) 可逆，且其逆矩阵 \\(A^{-1}\\) 可以通过伴随矩阵来计算：\n\\[\nA^{-1} = \\frac{1}{\\det(A)} \\cdot adj(A)\n\\]\n\n\n\n对角矩阵 \\(A, B\\)\n\n\\(AB\\) = \\(BA\\) \\(\\rightarrow\\) 对角线元素的乘积。\n对角矩阵的幂 \\(\\rightarrow\\) 对角线元素的幂。\n逆 \\(\\rightarrow\\) 对角线元素的倒数。\n行列式 \\(\\rightarrow\\) 对角线元素的乘积。\n\n副对角矩阵\n\n逆 \\(\\rightarrow\\) 副对角元素的倒数，转置。\n行列式 \\(\\rightarrow\\) 副对角线元素乘积，正负由逆序数确定。\n\n分块对角矩阵相关公式 同上。副对角分块矩阵的行列式为 \\(-1^{m \\cdot n}|A||B|\\)\n\n\n\n\n\\((A + B)\\) 的上标运算，只有转置有公式。\n伴随矩阵的运算全都来源于其定义。\n若 \\(AB = BA = E\\), 则 \\((A + B)\\) 的幂次运算可按二项式展开。\n\n\n\n\n矩阵的秩（Rank) 通过其最高阶非零子式来定义。设 \\(A\\) 是一个 \\(m \\times n\\) 的矩阵。矩阵的秩是 \\(A\\) 中阶数最大的非零子式的阶数。具体定义如下：\n设 \\(A\\) 是一个 \\(m \\times n\\) 的矩阵，若 \\(A\\) 中存在一个 \\(k \\times k\\) 的子矩阵（即从 \\(A\\) 中取出 \\(k\\) 行 \\(k\\) 列构成的子矩阵）的行列式不为零，但所有 \\(k+1 \\times k+1\\) 的子矩阵的行列式均为零，则称矩阵 \\(A\\) 的秩为 \\(k\\)。\n换句话说： \\[\n\\text{rank}(A) = k\n\\] 其中 \\(k\\) 是满足矩阵 \\(A\\) 中存在 \\(k \\times k\\) 非零行列式的最大整数。\n行阶梯矩阵（Row Echelon Matrix） 行阶梯型矩阵是一种特殊形式的矩阵，它满足以下条件：\n\n所有零行（全为零的行）都排在非零行的下面。\n每一非零行的首个非零元素（称为主元）位于其前一行的主元的右边。\n\n\\[\n\\begin{bmatrix}\n1 & 2 & 0 & 3 \\\\\n0 & 1 & 4 & 5 \\\\\n0 & 0 & 0 & 6 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\n行最简形矩阵（Reduced Row Echelon Form, RREF） 行最简形矩阵（Reduced Row Echelon Form, RREF）是行阶梯矩阵的进一步简化形式，满足以下条件：\n\n它是一个行阶梯矩阵。\n每个主元所在列的其他元素均为零。\n每个主元为1。\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 2 & 0 \\\\\n0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\n矩阵秩的性质如下\n\n\\(R(AB) &lt;= R(A) \\text{以及}  R(B)\\)\n\\(R(AB) &gt;= R(A) + R(B) - n\\) (若 \\(AB = 0, R(A) + R(B) &lt;= n\\) )\n\\(R(A^*) = n , 1, 0 \\space (R(A) = n, R(A) = n - 1,R(A) &lt; n - 1\\)\n\n初等变化不改变矩阵的秩，将矩阵经过初等变换为最简形矩阵可用于求矩阵的秩\n\n\n\n克莱姆法则（Cramer’s Rule） 克莱姆法则是一种用行列式求解线性方程组的方法。假设我们有一个线性方程组：\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n = b_n\n\\end{cases}\n\\]\n我们可以将其表示为矩阵形式：\n\\[\nA \\mathbf{x} = \\mathbf{b}\n\\]\n其中，\\(A\\) 是系数矩阵，\\(\\mathbf{x}\\) 是未知数向量，\\(\\mathbf{b}\\) 是常数向量：\n\\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix},\n\\quad\n\\mathbf{b} = \\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_n\n\\end{bmatrix}\n\\]\n克莱姆法则指出，如果行列式 \\(\\text{det}(A) \\neq 0\\)，那么线性方程组有唯一解，每个未知数 \\(x_i\\) 的解可以通过以下公式求得：\n\\[\nx_i = \\frac{\\text{det}(A_i)}{\\text{det}(A)}\n\\]\n其中，矩阵 \\(A_i\\) 是将矩阵 \\(A\\) 的第 \\(i\\) 列替换为向量 \\(\\mathbf{b}\\) 后得到的矩阵。\n对于非齐次线性方程组 \\(A\\mathbf{x} = \\mathbf{b}\\)，其解的判定如下：\n\n如果 \\(\\det(A) \\neq 0\\), 则方程组有唯一解。\n如果 \\(\\det(A) = 0\\)，则方程组有无穷多解，或者无解。\n\n对于齐次线性方程组 \\(A\\mathbf{x} = \\mathbf{0}\\)，其解的判定如下：\n\n如果 \\(\\det(A) \\neq 0\\) 则方程组只有零解。\n如果 \\(\\det(A) = 0\\) ，则方程组有无穷多非零解。\n\n高斯消元法（初等变换）解方程组 高斯消元法的抽象就是对矩阵的初等变换。\n对于非齐次方程组，其解的判定如下：\n\n若 \\(rank([A, b]) &gt; rank(A)\\)， 方程组无解 - 方程角度出现了矛盾方程。\n若 \\(rank([A, b]) = rank(A)\\), 若\\(A_{mn}\\), \\(rank(A) &lt; n\\) 则有无穷多解，\\(rank(A) = n\\), 则只有唯一解。 - 方程角度 \\(rank(A)\\) 就是方程组的的约束条件， \\(n\\) 就是未知数的个数\n\n齐次方程组，其解的判定如下：\n\n若 \\(rank(A_{mn}) = n\\), 则只有零解。\n若 \\(rank(A_{mn}) &lt; n\\), 则有非零解。\n\n\n\n\n向量 一个向量完全由其基向量以及各基向量方向上的标量决定，例如向量\\(\\alpha = [a, b]\\), 表示其在两个基 \\(i, j\\) 的倍数。\n矩阵 一个矩阵代表了空间的变化，为了确定一个向量在变换后的位置，只需要追踪基向量的位置。实际上矩阵\\(A = [e_1|e_2]\\)的每个列向量\\(e_1, e_2\\)就是变换后的基向量。将每个基向量\\(e_1, e_2\\)乘以原向量\\(\\alpha\\)每个方向上的标量\\(a, b\\)，再加起来就得到变换后的向量\\(\\beta\\).\n初等变换 向量的线性组合，重新组合后的向量仍然在同一向量空间。\n行列式倍加相等 向量的重新线性组合。\n\\[\\beta = ae_1 + be_2\\]\n行列式 行列式的几何含义空间变换以后由基向量所张成空间的变化比例。\\(|A| = 0\\)，则说明\\(A\\)并不是满秩的，会造成空间的降维，其行列式为0，且由于发生了空间的坍缩，由低维空间返回到原高维空间是不可能得，即其逆变换不存在，即\\(A^{-1}\\)不存在。所以实际上行列式，秩，是对矩阵所张成空间的维度的不同角度的描述。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#定义以及概念",
    "href": "note/2024-07-01-linear-algebra/index.html#定义以及概念",
    "title": "Linear Algebra",
    "section": "",
    "text": "矩阵为矩形数表；\n\\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\n0 & a_{22} & a_{23} \\\\\n0 & 0 & a_{33} \\\\\n\\end{bmatrix}\n\\]\n上三角矩阵 \\(U\\).\n\\[\n\\begin{bmatrix}\na_{11} & 0 & 0 \\\\\na_{21} & a_{22} & 0 \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\n下三角矩阵 \\(L\\).\n上下三角矩阵统称为三角矩阵。\n\\[\n\\begin{bmatrix}\na_{11} & 0 & 0 \\\\\n0 & a_{22} & 0 \\\\\n0 & 0 & a_{33} \\\\\n\\end{bmatrix}\n\\]\n对角矩阵 \\(\\Lambda\\).\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\]\n单位矩阵 \\(E, I\\)."
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#矩阵线性运算",
    "href": "note/2024-07-01-linear-algebra/index.html#矩阵线性运算",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\[ {c_{ij}=a_{ij}+b_{ij}} \\]\n\\[ C_{ij} = ac_{ij} \\]\n矩阵加法和数乘统称为矩阵的线性运算。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#矩阵乘法",
    "href": "note/2024-07-01-linear-algebra/index.html#矩阵乘法",
    "title": "Linear Algebra",
    "section": "",
    "text": "The element \\(C_{ij}\\) of the resulting matrix \\(C\\) is found by taking the dot product of the \\(i\\)th row of matrix \\(A\\) and the \\(j\\)th column of matrix \\(B\\). This can be expressed as:\n\\[C_{ij} = \\sum_{k=1}^{n} A_{ik} \\times B_{kj}\\]\nWhere \\(A_{ik}\\) is the element in the \\(i\\)th row and \\(k\\)th column of matrix \\(A\\), and \\(B_{kj}\\) is the element in the \\(k\\)th row and \\(j\\)th column of matrix \\(B\\).\n\n\\(AB = 0 \\not\\Rightarrow A = 0 \\text{ 或 } B = 0\\)\n\\(A \\cdot B \\neq B \\cdot A\\)\n\\(ABC = (AB)C\\)\n\\(A(B + C) = AB + AC\\)\n\\(k(AB) = A(kB) = ABk\\)\n\n空间位置不能变，时间次序可以变。\n\\[\n\\begin{aligned}\n2x_1 + 3x_2 &= 8 \\\\\nx_1 - x_2 &= 1 \\\\\n\\end{aligned}\n\\]\nCan be represented in matrix form as:\n\\[\n\\begin{bmatrix}\n2 & 3 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n8 \\\\\n1 \\\\\n\\end{bmatrix}\n\\]\n线性方程组的矩阵表示 系数矩阵，变量矩阵，常数矩阵。\n对于矩阵乘法有如下理解：\n\n左边乘一个矩阵为对该矩阵列向量的线性组合，线性组合的系数为右侧矩阵的列，该视角最为常用，可用于判断线性方程组解的情况；右乘一个矩阵为对该矩阵的行向量的线性组合，线性组合的系数为左侧矩阵的行， 该视角可用于消元。\n矩阵乘法也可理解为每一列乘每一行得到多个矩阵，然后相加。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#矩阵转置",
    "href": "note/2024-07-01-linear-algebra/index.html#矩阵转置",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\]\nThe transpose of \\(A\\), denoted as \\(A^T\\) is obtained by flipping \\(A\\) over its main diagonal, and is defined as:\n\\[\nA^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\]\n\n\\((A^T)^T = A\\)\n\\((A + B)^T = A^T + B^T\\)\n\\((kA)^T = kA^T\\)\n\\((AB)^T = B^TA^T\\)\n\n对称矩阵 \\(A^T = A\\), 一定为方阵。\n反对称矩阵 \\(A^T = -A\\) , 一定为方阵，对角线一定为0."
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#分块矩阵",
    "href": "note/2024-07-01-linear-algebra/index.html#分块矩阵",
    "title": "Linear Algebra",
    "section": "",
    "text": "非分块矩阵可以认为每个元素都是一个只有一个数的块。\n\\[\n\\alpha_1x_1 + \\alpha_2x_2 = b\n\\]\n分块矩阵可用于矩阵的抽象运算，利用分块矩阵表示线性方程组如上。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#矩阵的逆与矩阵的初等变化",
    "href": "note/2024-07-01-linear-algebra/index.html#矩阵的逆与矩阵的初等变化",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\[\nA_{n}B_{n} = B_{n}A_{n} = E_n\n\\]\n逆只能存在方阵之中，且不一定可逆。逆具有唯一性。\n\\[\n\\begin{aligned}\n(A^{-1})^{-1} &= A \\\\\n(A + B)^{-1} &= ? \\\\\n(kA)^{-1} &= k^{-1}A^{-1} \\\\\n(AB)^{-1} &= B^{-1}A^{-1} \\\\\n(A^{-1})^T &= (A^T)^{-1}\n\\end{aligned}\n\\]\n矩阵逆运算规律如上。矩阵的上标运算可以任意交换位置。\n矩阵初等变换 实际上就是方程组系数消元的抽象过程，初等变换的一系列矩阵称为等价矩阵。\n\\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\xrightarrow{\\text{行变换}}\n\\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\]\n初等方阵 单位矩阵 \\(E\\) 经过一次初等变换所形成的方阵\\(P\\)。 若 \\(P\\) 为初等方阵， \\(PA=B\\)， 则\\(A \\rightarrow B\\)的变化等价于 \\(E \\rightarrow P\\) 的初等行变换。若\\(AP = B\\)， 则 \\(A \\rightarrow B\\) 等价于 \\(E \\rightarrow P\\) 的初等列变换。实际上这里描述的是一个初等变换可由一个初等方阵来定义；从矩阵乘法的行视角来看，就是对右侧矩阵行向量的线性组合。\n那么有初等方阵的逆矩阵：\n\n通过交换单位矩阵的第 \\(i\\) 行和第 \\(j\\) 行得到的初等矩阵 \\(E_{ij}\\)， 其逆矩阵也是通过交换同样的两行得到的，即 \\(E_{ij}^{-1} = E_{ij}\\)。\n将单位矩阵的第 \\(i\\) 行乘以一个非零常数 \\(k\\) 得到的初等矩阵 \\(E_i(k)\\)，其逆矩阵是将第 \\(i\\) 行乘以 \\(1/k\\)，即 \\(E_i(k)^{-1} = E_i(1/k)\\)。\n将单位矩阵的第 \\(i\\) 行加上第 \\(j\\) 行的 \\(k\\) 倍得到的初等矩阵 \\(E_{ij}(k)\\)，其逆矩阵是将第 \\(i\\) 行减去第 \\(j\\) 行的 \\(k\\) 倍， 即 \\(E_{ij}(k)^{-1} = E_{ij}(-k)\\)。\n\n\\[\n\\begin{bmatrix}\na_{11} & a_{12} & 1 & 0 \\\\\na_{21} & a_{22} & 0 & 1\n\\end{bmatrix}\n\\xrightarrow{\\text{初等行变换}}\n\\begin{bmatrix}\n1 & 0 & a_{11}^{-1} & -a_{12}a_{11}^{-1} \\\\\n0 & 1 & -a_{21}a_{11}^{-1} & a_{11}^{-1}(a_{11}a_{22}-a_{12}a_{21})\n\\end{bmatrix}\n\\]\n逆矩阵的求法 将矩阵经过初等变换为单位矩阵，同样的变化作用于单位矩阵的结果就是其逆矩阵。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#行列式",
    "href": "note/2024-07-01-linear-algebra/index.html#行列式",
    "title": "Linear Algebra",
    "section": "",
    "text": "逆序数 在一个排列 \\(a_1, a_2, \\ldots, a_n\\) 中，如果存在一对 \\(i, j\\)，满足 \\(i &lt; j\\) 且 \\(a_i &gt; a_j\\)，那么这对 \\((a_i, a_j)\\) 称为一个逆序对。排列中所有逆序对的个数称为该排列的逆序数。\n例如，对于排列 \\(3, 1, 2\\)，它的逆序对有 \\((3, 1)\\) 和 \\((3, 2)\\)，所以它的逆序数是 2。\n形式化地，设排列为 \\(\\sigma = (\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)\\)，则逆序数定义为：\n\\[ \\text{Inv}(\\sigma) = \\sum_{1 \\leq i &lt; j \\leq n} \\mathbf{1}_{\\sigma_i &gt; \\sigma_j} \\]\n其中 \\(\\mathbf{1}_{\\sigma_i &gt; \\sigma_j}\\) 是指示函数，当 \\(\\sigma_i &gt; \\sigma_j\\) 时取值为 1，否则取值为 0。\n余子式 设 \\(A\\) 是一个 \\(n \\times n\\) 的矩阵，\\(A_{ij}\\) 是 \\(A\\) 中去掉第 \\(i\\) 行和第 \\(j\\) 列后剩下的 \\((n-1) \\times (n-1)\\) 子矩阵。那么，\\(A\\) 的 \\((i, j)\\) 元素对应的余子式定义为：\n\\[ M_{ij} = \\det(A_{ij}) \\]\n其中 \\(\\det(A_{ij})\\) 表示 \\(A_{ij}\\) 的行列式。\n代数余子式 设 \\(A\\) 是一个 \\(n \\times n\\) 的矩阵，\\(A_{ij}\\) 是 \\(A\\) 中去掉第 \\(i\\) 行和第 \\(j\\) 列后剩下的 \\((n-1) \\times (n-1)\\) 子矩阵。那么，\\(A\\) 的 \\((i, j)\\) 元素对应的代数余子式定义为：\n\\[ C_{ij} = (-1)^{i+j} \\det(A_{ij}) \\]\n行列式 行列式（Determinant）是一个函数，将方阵映射到标量，其共有 \\(n!\\) 项, 其中每一项为不同行不同列元素的积；每一项的正负由排列的逆序数决定。 记作 \\(\\det(A)\\) 或 \\(|A|\\)。设 \\(A\\) 是一个 \\(n \\times n\\) 的方阵，那么 \\(A\\) 的行列式 \\(\\det(A)\\) 定义为：\n\\[\n\\det(A) = \\sum_{\\sigma \\in S_n} (\\text{sgn}(\\sigma) \\prod_{i=1}^{n} a_{i,\\sigma(i)})\n\\]\n其中，\\(S_n\\) 表示所有 \\(n\\) 个元素的排列，\\(\\sigma\\) 是一个排列，\\(\\text{sgn}(\\sigma)\\) 表示排列 \\(\\sigma\\) 的符号，\\(a_{i,\\sigma(i)}\\) 表示矩阵 \\(A\\) 在第 \\(i\\) 行第 \\(\\sigma(i)\\) 列的元素。对于 n 阶方阵 \\(A\\)，其行列式可通过以下方法计算：\n\n三角矩阵：对于三角矩阵，行列式是对角线上元素的代数余子式。\n展开法：通过任意一行（或列）的元素进行展开。例如，第 \\(i\\) 行的展开公式为：\n\\[\n\\det(A) = \\sum_{j=1}^{n} (-1)^{i+j} a_{ij} \\det(A_{ij})\n\\]\n其中，\\(a_{ij}\\) 是矩阵 \\(A\\) 中第 \\(i\\) 行第 \\(j\\) 列的元素，\\(A_{ij}\\) 是删除了第 \\(i\\) 行和第 \\(j\\) 列的子矩阵。\n如果按第 \\(i\\) 行展开得到代数余子式，分别乘以 第 \\(j (j \\neq i)\\) 行对应的元素，其等于0\n\n行列式的基本性质包括\n\n转置相等 \\(\\det(A) = \\det(A^T)\\)\n换行变号 交换行列式的两行（列），行列式变号。\n乘数乘行 行列式的某一行（列）乘以常数，行列式也乘以该常数。\n倍加相等 行列式中某一行（列）的常数倍加到另一行（列），行列式不变。\n拆分分行 行列式可以按行或列拆分。\n零性质 行列式中某一行（列）全为零，或有行或者列成比例，行列式为零。\n\n行列式运算规律如下\n\n\\(|AB| = |A||B|\\)\n\\(|kA| = k^n|A|\\)\n分块的对角矩阵的行列式等于对角线元素上的分块的行列式的乘积。\n\n行列式计算\n\n对角，三角行列式， 次(副)对角元素乘积 \\(\\rightarrow\\) 对角线乘积（正负由逆序数决定）\n分块矩阵的行列式。\n一杠一星，两杠一星行列式 \\(\\rightarrow\\) 只有对角元素项。\n箭头行列式，弓形行列式 \\(\\rightarrow\\) 列变换为第一列为0， 列变换为对角行列式。\n同行（列）行列式 \\(\\rightarrow\\) 列，行变换为箭头行列式。\n\\(X\\) 形行列式 \\(\\rightarrow\\)\n\\(ab\\) 矩阵。\n范德蒙行列式 \\(\\rightarrow\\) 确定不变行。\n\n\\[\nV_n = \\begin{vmatrix}\n1      & 1      & 1      & \\cdots & 1      \\\\\nx_1    & x_2    & x_3    & \\cdots & x_n    \\\\\nx_1^2  & x_2^2  & x_3^2  & \\cdots & x_n^2  \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_1^{n-1} & x_2^{n-1} & x_3^{n-1} & \\cdots & x_n^{n-1}\n\\end{vmatrix}\n\\]"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#伴随矩阵",
    "href": "note/2024-07-01-linear-algebra/index.html#伴随矩阵",
    "title": "Linear Algebra",
    "section": "",
    "text": "伴随矩阵 伴随矩阵是与原矩阵密切相关的一个矩阵，它由原矩阵的代数余子式构成。对于一个 n 阶方阵 \\(A\\)，其伴随矩阵记作 \\(adj(A)\\)，它的每个元素 \\(a_{ij}\\) 都是由原矩阵 \\(A\\) 的代数余子式 \\(C_{ij}\\) 替换得到的。具体来说，伴随矩阵的第 \\(i\\) 行第 \\(j\\) 列的元素是原矩阵第 \\(j\\) 行第 \\(i\\) 列的代数余子式，即 \\(adj(A)_{ij} = C_{ji}\\)。\n伴随矩阵与原矩阵的行列式之间有如下关系：\n\\[\nA \\cdot adj(A) = adj(A) \\cdot A  = \\det(A) \\cdot I\n\\]\n其中，\\(I\\) 是单位矩阵，\\(\\det(A)\\) 是矩阵 \\(A\\) 的行列式。这个关系表明，伴随矩阵可以用于计算矩阵的逆，当 \\(\\det(A) \\neq 0\\) 时，矩阵 \\(A\\) 可逆，且其逆矩阵 \\(A^{-1}\\) 可以通过伴随矩阵来计算：\n\\[\nA^{-1} = \\frac{1}{\\det(A)} \\cdot adj(A)\n\\]"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#对角矩阵副对角相关公式",
    "href": "note/2024-07-01-linear-algebra/index.html#对角矩阵副对角相关公式",
    "title": "Linear Algebra",
    "section": "",
    "text": "对角矩阵 \\(A, B\\)\n\n\\(AB\\) = \\(BA\\) \\(\\rightarrow\\) 对角线元素的乘积。\n对角矩阵的幂 \\(\\rightarrow\\) 对角线元素的幂。\n逆 \\(\\rightarrow\\) 对角线元素的倒数。\n行列式 \\(\\rightarrow\\) 对角线元素的乘积。\n\n副对角矩阵\n\n逆 \\(\\rightarrow\\) 副对角元素的倒数，转置。\n行列式 \\(\\rightarrow\\) 副对角线元素乘积，正负由逆序数确定。\n\n分块对角矩阵相关公式 同上。副对角分块矩阵的行列式为 \\(-1^{m \\cdot n}|A||B|\\)"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#矩阵运算规律总结",
    "href": "note/2024-07-01-linear-algebra/index.html#矩阵运算规律总结",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\((A + B)\\) 的上标运算，只有转置有公式。\n伴随矩阵的运算全都来源于其定义。\n若 \\(AB = BA = E\\), 则 \\((A + B)\\) 的幂次运算可按二项式展开。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#矩阵的秩",
    "href": "note/2024-07-01-linear-algebra/index.html#矩阵的秩",
    "title": "Linear Algebra",
    "section": "",
    "text": "矩阵的秩（Rank) 通过其最高阶非零子式来定义。设 \\(A\\) 是一个 \\(m \\times n\\) 的矩阵。矩阵的秩是 \\(A\\) 中阶数最大的非零子式的阶数。具体定义如下：\n设 \\(A\\) 是一个 \\(m \\times n\\) 的矩阵，若 \\(A\\) 中存在一个 \\(k \\times k\\) 的子矩阵（即从 \\(A\\) 中取出 \\(k\\) 行 \\(k\\) 列构成的子矩阵）的行列式不为零，但所有 \\(k+1 \\times k+1\\) 的子矩阵的行列式均为零，则称矩阵 \\(A\\) 的秩为 \\(k\\)。\n换句话说： \\[\n\\text{rank}(A) = k\n\\] 其中 \\(k\\) 是满足矩阵 \\(A\\) 中存在 \\(k \\times k\\) 非零行列式的最大整数。\n行阶梯矩阵（Row Echelon Matrix） 行阶梯型矩阵是一种特殊形式的矩阵，它满足以下条件：\n\n所有零行（全为零的行）都排在非零行的下面。\n每一非零行的首个非零元素（称为主元）位于其前一行的主元的右边。\n\n\\[\n\\begin{bmatrix}\n1 & 2 & 0 & 3 \\\\\n0 & 1 & 4 & 5 \\\\\n0 & 0 & 0 & 6 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\n行最简形矩阵（Reduced Row Echelon Form, RREF） 行最简形矩阵（Reduced Row Echelon Form, RREF）是行阶梯矩阵的进一步简化形式，满足以下条件：\n\n它是一个行阶梯矩阵。\n每个主元所在列的其他元素均为零。\n每个主元为1。\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 2 & 0 \\\\\n0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\n矩阵秩的性质如下\n\n\\(R(AB) &lt;= R(A) \\text{以及}  R(B)\\)\n\\(R(AB) &gt;= R(A) + R(B) - n\\) (若 \\(AB = 0, R(A) + R(B) &lt;= n\\) )\n\\(R(A^*) = n , 1, 0 \\space (R(A) = n, R(A) = n - 1,R(A) &lt; n - 1\\)\n\n初等变化不改变矩阵的秩，将矩阵经过初等变换为最简形矩阵可用于求矩阵的秩"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#矩阵视角下的线性方程组",
    "href": "note/2024-07-01-linear-algebra/index.html#矩阵视角下的线性方程组",
    "title": "Linear Algebra",
    "section": "",
    "text": "克莱姆法则（Cramer’s Rule） 克莱姆法则是一种用行列式求解线性方程组的方法。假设我们有一个线性方程组：\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n = b_n\n\\end{cases}\n\\]\n我们可以将其表示为矩阵形式：\n\\[\nA \\mathbf{x} = \\mathbf{b}\n\\]\n其中，\\(A\\) 是系数矩阵，\\(\\mathbf{x}\\) 是未知数向量，\\(\\mathbf{b}\\) 是常数向量：\n\\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix},\n\\quad\n\\mathbf{b} = \\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_n\n\\end{bmatrix}\n\\]\n克莱姆法则指出，如果行列式 \\(\\text{det}(A) \\neq 0\\)，那么线性方程组有唯一解，每个未知数 \\(x_i\\) 的解可以通过以下公式求得：\n\\[\nx_i = \\frac{\\text{det}(A_i)}{\\text{det}(A)}\n\\]\n其中，矩阵 \\(A_i\\) 是将矩阵 \\(A\\) 的第 \\(i\\) 列替换为向量 \\(\\mathbf{b}\\) 后得到的矩阵。\n对于非齐次线性方程组 \\(A\\mathbf{x} = \\mathbf{b}\\)，其解的判定如下：\n\n如果 \\(\\det(A) \\neq 0\\), 则方程组有唯一解。\n如果 \\(\\det(A) = 0\\)，则方程组有无穷多解，或者无解。\n\n对于齐次线性方程组 \\(A\\mathbf{x} = \\mathbf{0}\\)，其解的判定如下：\n\n如果 \\(\\det(A) \\neq 0\\) 则方程组只有零解。\n如果 \\(\\det(A) = 0\\) ，则方程组有无穷多非零解。\n\n高斯消元法（初等变换）解方程组 高斯消元法的抽象就是对矩阵的初等变换。\n对于非齐次方程组，其解的判定如下：\n\n若 \\(rank([A, b]) &gt; rank(A)\\)， 方程组无解 - 方程角度出现了矛盾方程。\n若 \\(rank([A, b]) = rank(A)\\), 若\\(A_{mn}\\), \\(rank(A) &lt; n\\) 则有无穷多解，\\(rank(A) = n\\), 则只有唯一解。 - 方程角度 \\(rank(A)\\) 就是方程组的的约束条件， \\(n\\) 就是未知数的个数\n\n齐次方程组，其解的判定如下：\n\n若 \\(rank(A_{mn}) = n\\), 则只有零解。\n若 \\(rank(A_{mn}) &lt; n\\), 则有非零解。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#几何直观",
    "href": "note/2024-07-01-linear-algebra/index.html#几何直观",
    "title": "Linear Algebra",
    "section": "",
    "text": "向量 一个向量完全由其基向量以及各基向量方向上的标量决定，例如向量\\(\\alpha = [a, b]\\), 表示其在两个基 \\(i, j\\) 的倍数。\n矩阵 一个矩阵代表了空间的变化，为了确定一个向量在变换后的位置，只需要追踪基向量的位置。实际上矩阵\\(A = [e_1|e_2]\\)的每个列向量\\(e_1, e_2\\)就是变换后的基向量。将每个基向量\\(e_1, e_2\\)乘以原向量\\(\\alpha\\)每个方向上的标量\\(a, b\\)，再加起来就得到变换后的向量\\(\\beta\\).\n初等变换 向量的线性组合，重新组合后的向量仍然在同一向量空间。\n行列式倍加相等 向量的重新线性组合。\n\\[\\beta = ae_1 + be_2\\]\n行列式 行列式的几何含义空间变换以后由基向量所张成空间的变化比例。\\(|A| = 0\\)，则说明\\(A\\)并不是满秩的，会造成空间的降维，其行列式为0，且由于发生了空间的坍缩，由低维空间返回到原高维空间是不可能得，即其逆变换不存在，即\\(A^{-1}\\)不存在。所以实际上行列式，秩，是对矩阵所张成空间的维度的不同角度的描述。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#定义以及定理",
    "href": "note/2024-07-01-linear-algebra/index.html#定义以及定理",
    "title": "Linear Algebra",
    "section": "定义以及定理",
    "text": "定义以及定理\n线性组合 线性组合是指通过对向量进行加权求和得到一个新的向量。假设有向量 \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) 和系数 \\(c_1, c_2, \\ldots, c_n\\)，它们的线性组合表示为：\n\\[\\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n\\]\n线性表示 线性表示是指一个向量可以表示为其他向量的线性组合。假设向量 \\(\\mathbf{v}\\) 可以表示为向量 \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) 的线性组合，则有：\n\\[\\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n\\]\n线性相关 如果存在一组不全为零的标量 \\(c_1, c_2, \\ldots, c_n\\)，使得这些向量的线性组合为零向量：\n\\[c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n = \\mathbf{0}\\]\n换句话说，如果至少有一个向量可以表示为其他向量的线性组合，则这些向量是线性相关的。对应于齐次方程组有非零解。\n线性无关 如果只有当所有标量 \\(c_1, c_2, \\ldots, c_n\\) 均为零时，这些向量的线性组合才为零向量：\n\\[c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n = \\mathbf{0} \\implies c_1 = c_2 = \\cdots = c_n = 0\\]\n换句话说，任何一个向量都不能表示为其他向量的线性组合，则这些向量是线性无关的。对应于齐次方程组只有零解。\n假设我们有一组线性无关的向量 \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\)。这些向量的线性组合只有在所有系数都为零时，才会等于零向量：\n\\[c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n = \\mathbf{0} \\implies c_1 = c_2 = \\cdots = c_n = 0\\]\n现在，假设我们引入一个新向量 \\(\\mathbf{u}\\)。如果将这个新向量加到上述向量组中后变得线性相关，则有：\n\\[c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n + c_{n+1} \\mathbf{u} = \\mathbf{0}\\]\n并且存在不全为零的系数 \\(c_1, c_2, \\ldots, c_n, c_{n+1}\\) 使得上式成立。\n由于 \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\) 是线性无关的，因此 \\(\\mathbf{u}\\) 可以唯一地表示为这些向量的线性组合。\n向量组的部分与整体定理 整体线性无关，那么部分线性无关；部分线性相关，那么整体线性相关。\n向量组的伸长与缩短定理 短线性无关，那么长线性无关；长线性相关，那么短线性相关。\n极大无关组与向量组的秩 向量组中无关向量的数量，数量上等于矩阵的秩。\n向量个数与向量维数\n等价向量组 向量组与它的任意极大无关组等价。\n向量组的秩与向量组的线性表示定理 如果向量组 \\(\\alpha_1, \\alpha_2 ...\\) , 可由向量组 \\(\\beta_1, \\beta_2 ..\\) 表示，那么 \\(rank(B) &gt;= rank(A)\\) .\n向量组的臃肿与紧凑定理\n向量空间 是一个由向量构成的集合，这个集合对于向量的加法和标量乘法是封闭的。更正式地说，一个集合 \\(V\\) 配备了一对操作：加法（记作 \\(+\\)）和标量乘法（记作 \\(\\cdot\\)），如果满足以下性质，则称 \\(V\\) 为一个向量空间：\n\n封闭性, 对于所有 \\(\\mathbf{u}, \\mathbf{v} \\in V\\)，有 \\(\\mathbf{u} + \\mathbf{v} \\in V\\) 和 \\(c \\cdot \\mathbf{u} \\in V\\)，其中 \\(c\\) 是任意标量。\n结合律, 对于所有 \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\)，有 \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)。\n交换律, 对于所有 \\(\\mathbf{u}, \\mathbf{v} \\in V\\)，有 \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)。\n存在零向量, 存在一个向量 \\(\\mathbf{0} \\in V\\)，使得对于所有 \\(\\mathbf{v} \\in V\\)，有 \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)。\n存在加法的逆元, 对于每个 \\(\\mathbf{v} \\in V\\)，存在一个向量 \\(-\\mathbf{v} \\in V\\)，使得 \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)。\n标量乘法的分配律, 对于所有 \\(c, d\\) 是标量，和所有 \\(\\mathbf{u}, \\mathbf{v} \\in V\\)，有 \\(c \\cdot (\\mathbf{u} + \\mathbf{v}) = c \\cdot \\mathbf{u} + c \\cdot \\mathbf{v}\\) 和 \\((c + d) \\cdot \\mathbf{u} = c \\cdot \\mathbf{u} + d \\cdot \\mathbf{u}\\)。\n标量乘法的单位元, 对于所有 \\(\\mathbf{v} \\in V\\)，有 \\(1 \\cdot \\mathbf{v} = \\mathbf{v}\\)，其中 \\(1\\) 是标量乘法的单位元。\n\n向量在基下的坐标 在向量空间中，一个向量 \\(\\mathbf{v}\\) 可以通过基 \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\) 下的坐标来唯一表示。如果 \\(\\mathbf{v}\\) 可以表示为基向量的线性组合，那么存在一组系数 \\(x_1, x_2, \\ldots, x_n\\) 使得：\n\\[\\mathbf{v} = x_1 \\mathbf{e}_1 + x_2 \\mathbf{e}_2 + \\cdots + x_n \\mathbf{e}_n\\]\n系数 \\(x_1, x_2, \\ldots, x_n\\) 就是向量 \\(\\mathbf{v}\\) 在基 \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\) 下的坐标。\n过渡矩阵 过渡矩阵是一个与基变换相关的矩阵。如果有两组基 \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\) 和 \\(\\{\\mathbf{f}_1, \\mathbf{f}_2, \\ldots, \\mathbf{f}_n\\}\\)，过渡矩阵 \\(P\\) 是一个矩阵，它的列是第二个基向量在第一个基下的坐标。如果 \\(\\mathbf{f}_i\\) 在基 \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\) 下的坐标是列向量 \\(\\mathbf{p}_i\\)，那么过渡矩阵 \\(P\\) 可以表示为：\n\\[P = [\\mathbf{p}_1 | \\mathbf{p}_2 | \\ldots | \\mathbf{p}_n]\\]\n过渡矩阵可以用来转换一个向量在两个基下的坐标。\n向量的内积 两个向量 \\(\\mathbf{u}\\) 和 \\(\\mathbf{v}\\) 的内积（点积）是一个标量，定义为：\n\\[\\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n\\]\n内积也可以表示为：\n\\[\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos \\theta\\]\n其中 \\(\\theta\\) 是向量 \\(\\mathbf{u}\\) 和 \\(\\mathbf{v}\\) 之间的夹角。\n向量的长度（范数） 向量 \\(\\mathbf{v}\\) 的长度或范数是向量的内积的平方根，通常指欧几里得范数：\n\\[\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\\]\n向量的夹角 两个非零向量 \\(\\mathbf{u}\\) 和 \\(\\mathbf{v}\\) 之间的夹角 \\(\\theta\\) 可以通过它们的内积和范数来计算：\n\\[\\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\]\n正交基 如果一组基向量中的任意两个都是正交的，即它们的内积为零，那么这组基称为正交基。对于正交基 \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\)，满足：\n\\[\\mathbf{e}_i \\cdot \\mathbf{e}_j = 0 \\quad \\text{for } i \\neq j\\]\n标准正交基 如果正交基中的每个基向量的长度都是 1，那么这组基称为标准正交基。对于标准正交基 \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\)，满足：\n\\[\\mathbf{e}_i \\cdot \\mathbf{e}_j = \\delta_{ij}\\]\n其中 \\(\\delta_{ij}\\) 是克罗内克 delta 函数。\n正交矩阵 正交矩阵是一个方阵，其列向量构成一个标准正交基。对于一个 \\(n \\times n\\) 的正交矩阵 \\(Q\\)，\\(Q^TQ = E\\) ;它满足以下条件：\n\n列向量是线性无关的。\n列向量的长度都是 1。\n列向量与自身正交，即对于任意两个不同的列向量 \\(\\mathbf{q}_i\\) 和 \\(\\mathbf{q}_j\\)（\\(i \\neq j\\)），满足：\n\n\\[\n\\mathbf{q}_i \\cdot \\mathbf{q}_j = \\delta_{ij}\n\\]\n其中 \\(\\delta_{ij}\\) 是克罗内克 delta 函数。\n正交矩阵的性质\n\n正交矩阵上标运算也是正交矩阵。\n正交矩阵的转置等于其逆矩阵，即 \\(Q^T = Q^{-1}\\)。\n若 \\(A, B\\) 都为正交矩阵， 则 \\(AB\\) 及 \\(BA\\) 都是正交矩阵。\n\\((A\\alpha, A\\beta) = (\\alpha, \\beta), ||A\\alpha|| = ||\\alpha||, &lt;A|\\alpha, A\\beta&gt; = &lt;\\alpha, \\beta&gt;\\)\n正交矩阵的行列式等于 ±1，即 \\(det(Q) = \\pm 1\\)。\n正交矩阵的奇异值都是 1 或 -1。\n正交矩阵的特征值是 ±1。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#向量空间视角下的线性方程组",
    "href": "note/2024-07-01-linear-algebra/index.html#向量空间视角下的线性方程组",
    "title": "Linear Algebra",
    "section": "向量空间视角下的线性方程组",
    "text": "向量空间视角下的线性方程组\n方程组表示 (System of Equations) 线性方程组可以直接以方程的形式表示。假设有 \\(n\\) 个方程和 \\(m\\) 个未知数 \\(x_1, x_2, \\ldots, x_m\\)，方程组形式如下：\n\\[\n\\begin{cases}\na_{11} x_1 + a_{12} x_2 + \\cdots + a_{1m} x_m = b_1 \\\\\na_{21} x_1 + a_{22} x_2 + \\cdots + a_{2m} x_m = b_2 \\\\\n\\vdots \\\\\na_{n1} x_1 + a_{n2} x_2 + \\cdots + a_{nm} x_m = b_n \\\\\n\\end{cases}\n\\]\n矩阵表示 (Matrix Form) 使用矩阵表示，可以将上述线性方程组表示为：\n\\[ A \\mathbf{x} = \\mathbf{b} \\]\n其中，矩阵 \\(A\\) 和向量 \\(\\mathbf{x}\\)、\\(\\mathbf{b}\\) 分别为：\n\\[ A = \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix},\n\\quad\n\\mathbf{x} = \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_m\n\\end{pmatrix},\n\\quad\n\\mathbf{b} = \\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_n\n\\end{pmatrix}\n\\]\n向量表示 (Vector Form) 向量表示法强调线性组合的观点，可以写成：\n\\[a_{1} \\mathbf{x}_1 + a_{2} \\mathbf{x}_2 + \\cdots + a_{m} \\mathbf{x}_m = \\mathbf{b}\\]\n其中 \\(a_{i}\\) 表示向量 \\(\\mathbf{x}_i\\) 的系数。\n齐次方程组的解\n齐次方程组是由线性方程组成的方程组，其形式如下：\n\\[Ax = 0\\]\n假设 \\(\\xi_1\\) , \\(\\xi_2\\) 是其解向量， 那么 \\(k\\xi_1\\), \\(\\xi_1 + \\xi_2\\), \\(k\\xi_1 + k\\xi_2\\)也是其解向量, 这些所有的向量构成解空间，其中任意一组解称为其基础解系，也就是解空间的一组基。\n其中 \\(A\\) 是一个 \\(m \\times n\\) 的矩阵，\\(x\\) 是一个 \\(n \\times 1\\) 的列向量。齐次方程组的通解是指满足方程组的解的集合。对于齐次方程组，如果 \\(A\\) 的秩 \\(r(A) &lt; n\\)，则齐次方程组有非零解。这些解可以表示为 \\(A\\) 的零空间的基向量的线性组合，其解的向量个数为 \\(n - r(A)\\) 即：\n\\[x = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k\\]\n其中 \\(c_1, c_2, \\ldots, c_k\\) 是任意常数，\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) 是 \\(A\\) 的零空间的基向量。\n非齐次方程组的通解\n导出组，非齐次方程组对应的齐次方程组。\n\n若 \\(\\xi_1, \\xi_2\\) 是 非齐次方程组的两个解， 则 \\(\\xi_1 - \\xi_2\\) 是其导出组 \\(Ax = 0\\) 的解。\n若 \\(n\\) 是非齐次线性方程组 \\(Ax = b\\)的的解， \\(\\xi\\) 是其导出组的解， 那么 \\(\\xi + n\\) 是 \\(Ax=b\\) 的解。\n\n非齐次方程组是由线性方程组成的方程组，其形式如下：\n\\[Ax = b\\]\n其中 \\(A\\) 是一个 \\(m \\times n\\) 的矩阵，\\(x\\) 是一个 \\(n \\times 1\\) 的列向量，\\(b\\) 是一个 \\(m \\times 1\\) 的列向量。非齐次方程组的通解是指满足方程组的解的集合。对于非齐次方程组，如果 \\(A\\) 的秩 \\(r(A) = n\\)，则方程组有唯一解。这个解可以表示为 \\(A\\) 的零空间的基向量的线性组合加上 \\(A\\) 的零空间的补空间的基向量的线性组合，即：\n\\[x = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k + d_1 \\mathbf{w}_1 + d_2 \\mathbf{w}_2 + \\cdots + d_m \\mathbf{w}_m\\]\n其中 \\(c_1, c_2, \\ldots, c_k, d_1, d_2, \\ldots, d_m\\) 是任意常数，\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) 是 \\(A\\) 的零空间的基向量，\\(\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_m\\) 是 \\(A\\) 的零空间的补空间的基向量。\n非齐次线性方程组的通解就是：导出组的通解 + 其一个特解。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#几何直观-1",
    "href": "note/2024-07-01-linear-algebra/index.html#几何直观-1",
    "title": "Linear Algebra",
    "section": "几何直观",
    "text": "几何直观\n向量 空间中有方向的箭头。\n点积 向量\\(\\alpha\\)投影到\\(\\beta\\)上的长度与与向量 \\(\\beta\\) 长度的乘积。对于点积运算，向量总是对应一个对偶向量，其为\\(\\alpha^T\\)，也就是说点积也可以理解为向量\\(\\beta\\)经过线性变换\\(\\alpha^T\\)后，压缩为一个一维的向量。如果\\(||\\alpha|| = 1\\)，则\\(\\alpha, \\beta\\)的点积为 \\(\\beta\\) 在单位基向量\\(\\alpha\\)的表示。这个应用在PCA等线性降维方法中。\n齐次方程组的自由变量 列向量的个数 - 列向量的极大无关组。多余的向量的系数可以任意取，最后由极大无关组，即这个空间的基向量，决定最终的系数。\n线性方程组的向量张成的空间 无解，则说明 \\(\\alpha_1, \\alpha_2, ..., \\alpha_n\\) 与 \\(b\\) 不在同一个向量空间， 若有解则说明 \\(\\alpha_1, \\alpha_2, ..., \\alpha_n\\) 与 \\(b\\) 在同一个向量空间。进一步地， 若 \\(\\alpha_1, \\alpha_2, ..., \\alpha_n\\) 的数量大于 \\(A\\) 空间维数，则有多余的向量，那么 构成 \\(b\\) 的方式则不止一个； 若 \\(\\alpha_1, \\alpha_2, ..., \\alpha_n\\) 的数量等于 \\(A\\) 空间维数，则只有一种表示方法。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#特征值eigenvalues特征向量eigenvectors",
    "href": "note/2024-07-01-linear-algebra/index.html#特征值eigenvalues特征向量eigenvectors",
    "title": "Linear Algebra",
    "section": "特征值（Eigenvalues）特征向量（Eigenvectors）",
    "text": "特征值（Eigenvalues）特征向量（Eigenvectors）\n特征值是线性代数中矩阵理论的一个基本概念。对于一个给定的方阵 \\(A\\)，特征值是满足方程 \\(Av = \\lambda v\\) 的数 \\(\\lambda\\)，其中 \\(v\\) 是非零向量，称为特征向量。特征值揭示了矩阵的某些代数属性，比如矩阵的行列式和迹（对角线元素之和）与特征值有关。\n特征向量 是与特征值相关联的非零向量，它表示了矩阵 \\(A\\) 在某个方向上的拉伸或压缩的比例。当一个矩阵作用在它的特征向量上时，结果是该特征向量的标量倍，这个标量就是相应的特征值。\n由定义可得：\n\\[(A - \\lambda E)v = 0\\]\n由特征值的的定义，\\(v \\neq 0\\), 则该齐次方程组有非零解，即 \\(|A - \\lambda E| = 0\\), 可求得特征值，将特征值代入 \\((A - \\lambda E)v = 0\\) 即可得到该特征值对应的特征向量。\n关于特征值的定理\n\n特征值的和等于矩阵的迹 \\(tr(A)\\) 。\n特征值的积等于矩阵行列式。\n考虑一个变换 \\(f\\) 其作用于矩阵 \\(A\\), 那么其对应的特征值有同样的变换，但是 \\(f(\\lambda)\\) 对应的特征向量不变。\n矩阵互不相等的特征值对应的特征向量是线性无关的。\n几何重数不大于代数重数。\n转置矩阵的特征值相等。\n若 \\(n\\) 阶矩阵 \\(g(A) = 0\\)， 那么其所有特征值 \\(g(\\lambda) = 0\\)\n\n关于实对称矩阵特征值的定理\n\n特征值都是实数，对应的特征向量也是实向量。\n不同特征值对应特征向量两两正交。\n几何重数等于其代数重数。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#相似矩阵-矩阵对角相似化",
    "href": "note/2024-07-01-linear-algebra/index.html#相似矩阵-矩阵对角相似化",
    "title": "Linear Algebra",
    "section": "相似矩阵 矩阵对角相似化",
    "text": "相似矩阵 矩阵对角相似化\n矩阵 \\(A\\) 与 \\(B\\) 相似 定义为:\n\\[B = P^{-1}AP\\]\n相似矩阵的性质：\n\n\\(A\\) 与 \\(B\\) 等价。\n\\(R(A) = R(B)\\)\n\\(\\lambda_A = \\lambda_B\\)\n\\(determinant(A) = determinant(B)\\)\n\\(|A| = |B|\\)\n\\(tr(A) = tr(B)\\)\n\\(f(A) ~ f(B)\\)\n\n相似对角化 将矩阵 \\(A\\) 与 一个对角矩阵 \\(\\Lambda\\) 进行相似。实际上， 对于形如 \\(P^{-1}AP\\) 的矩阵变化或者相似的过程，其几何含义为将 \\(A\\) 所表示的线性变化 转换为以矩阵 \\(P\\) 的列向量的为基向量的坐标系下的线性变化。相似矩阵为同一线性变化在不同基下的描述。特别地，为了得到与 \\(\\Lambda\\) 的相似，那么选择\\(A\\)的特征向量构成的基，则该线性变化在特征基的描述下即为对角矩阵。\n\\[P^{-1}AP = \\Lambda\\]\n相似对角化充要条件：\n\n\\(n\\) 个线性无关的特征向量\n代数重数等于几何重数。\n\n求解 矩阵\\(P\\)就是\\(A\\)的特征向量按列构成的矩阵。\n实对称矩阵的相似对角化 对于实对称矩阵\\(A\\)，总可以找到其正交的特征向量，并将其单位化为正交矩阵 \\(Q\\)，使得其相似对角化：\n\\(Q^{-1}AQ = \\Lambda \\rightarrow Q^{T}AQ = \\Lambda\\)"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#二次型",
    "href": "note/2024-07-01-linear-algebra/index.html#二次型",
    "title": "Linear Algebra",
    "section": "二次型",
    "text": "二次型\n二次型定义为二次多项式函数的矩阵形式，其中\\(A\\)为实对称矩阵。\n\\[\nf = x^TAx\n\\]\n合同矩阵 对于合同矩阵，矩阵\\(A\\)实际上为一个 \\((0, 2)\\)的张量，其是空间或流形几何一个量，称之为度规，形如\\(C^TAC\\)的形式实际上是将某个度规，转化为以矩阵\\(C\\)的列向量为基的坐标下的描述。而对于二次型而言，该矩阵为实对称矩阵， 形似上其相似与合同一致。\n\\(B = C^TAC\\)\n\n\\(r(A) = r(B)\\)\n若 \\(A\\) 为对称矩阵，那么 \\(B\\) 也为对称矩阵。\n\n二次型化为标准形：\n\n令 \\(x = Qy\\), 则 \\(x^TAx = y^TQ^TAQy\\)，其中矩阵 \\(Q\\) 为矩阵 \\(A\\) 特征向量对应的单位正交特征向量。(实际上为实对称矩阵相似对角化的过程， 实对称矩阵的相似和合同一致)\n配方法。\n\n惯性定理：无论怎样的可逆的线性变换使得其为标准二次型，其正，负平方项的个数一样。\n正定（负定）矩阵：假设有二次型\\(f\\), 若\\(f &gt; 0\\)， 则为正定矩阵 \\(f &gt;= 0\\)则为负正定矩阵。\n正定矩阵的性质如下\n负定矩阵的性质如下\n等价，相似，合同的判定\n\n秩相等，则等价；反之，亦成立。\n特征值相等，则可与同一个对角矩阵相似，则相似。\n正负惯性指数，或者正负特征值一致，则合同。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#几何直观-2",
    "href": "note/2024-07-01-linear-algebra/index.html#几何直观-2",
    "title": "Linear Algebra",
    "section": "几何直观",
    "text": "几何直观\n等价矩阵 对于形如\\(PAQ = B\\)形式的变换，其中\\(P, Q\\)为可逆矩阵。左乘一个可逆矩阵等价于进行初等行变换，右乘一个可逆矩阵等价于进行初等列变换。几何上，\\(B\\)是\\(A\\)经过初等变换得到的矩阵，两者等价意味着个矩阵所张成的行空间与列空间是等价的。\n相似矩阵 对于形如 \\(P^{-1}AP\\) 的矩阵变化或者相似的过程，其几何含义为将 \\(A\\) 所表示的线性变化 转换为以矩阵 \\(P\\) 的列向量的为基向量的坐标系下的线性变化。相似矩阵为同一线性变化在不同基下的描述。特别地，为了得到与 \\(\\Lambda\\) 的相似，那么选择\\(A\\)的特征向量构成的基，则该线性变化在特征基的描述下即为对角矩阵。\n合同矩阵 对于形如 \\(P^TAP\\) 的形式，矩阵\\(A\\)实际上为一个 \\((0, 2)\\)的张量，其是空间或流形几何一个量，称之为度规，形如\\(C^TAC\\)的形式实际上是将某个度规，转化为以矩阵\\(C\\)的列向量为基的坐标下的描述。特别地，对于二次型而言，该矩阵为实对称矩阵， 形似上其相似与合同一致。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#a-lu",
    "href": "note/2024-07-01-linear-algebra/index.html#a-lu",
    "title": "Linear Algebra",
    "section": "\\(A = LU\\)",
    "text": "\\(A = LU\\)\n\\(LU\\) 分解将矩阵分解为一个下三角以及上三角矩阵乘积；实际上，对于线性方程组进行高斯消元时，在不设计行交换时，每一次的行变换都可以用一个下三角矩阵表示，那么其乘积以及逆都是下三角矩阵 \\(L\\)，该下三角矩阵，的每一个系数记录了每次操作的消元系数。\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\na_{21} & 1 & 0 \\\\\na_{31} & a_{32} & 1 \\\\\n\\end{bmatrix}\n\\]\n如上矩阵表示在对一列进行消元时，进行的操作\\(r_2 = r_2 + a_{21}r_1\\), \\(r_3 = a_{31}r_1\\)；在第二列进行消元时， \\(r_3 = a_{32}r_2\\)"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#a-plambda-p-1",
    "href": "note/2024-07-01-linear-algebra/index.html#a-plambda-p-1",
    "title": "Linear Algebra",
    "section": "\\(A = P\\Lambda P^{-1}\\)",
    "text": "\\(A = P\\Lambda P^{-1}\\)\n考虑方阵\\(A\\)，\\(A\\)代表了一个空间变换，空间的变换只对该矩阵的特征向量起到缩放作用。那么\\(A\\)所代表的空间变换，我们可以考虑先把向量\\(x\\)转化为用矩阵\\(A\\)的特征向量构成的基底表示，即\\(Q^{-1}x\\)，其中\\(Q\\)为方阵\\(A\\)的特征向量构成的矩阵（为什么是\\(Q^{-1}\\)而不是\\(Q\\)?因为特征向量是从原矩阵求解而来，它们都是在同一视角下的表示，要变化到特征向量的视角则是其逆\\(Q^{-1}\\))。那么矩阵\\(A\\)对这个新的\\(x\\)，即\\(Q^{-1}x\\)的作用就仅仅是缩放（在每个维度上进行缩放），这个缩放用一个新的矩阵\\(\\Sigma\\)表示，其为对角矩阵，对角线上即是对应的特征值，那么现在整个变化记为\\(\\Sigma\\)\\(Q^{-1}x\\)。完成变化后，我们回到原来的基的视角，即左乘\\(Q\\)，整个变化即如下： \\[\nAx = Q\\Sigma{Q^{-1}}{x} \\longrightarrow A = Q\\Sigma{Q^{-1}}\n\\]"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#a-usigma-v",
    "href": "note/2024-07-01-linear-algebra/index.html#a-usigma-v",
    "title": "Linear Algebra",
    "section": "\\(A = U\\Sigma V^*\\)",
    "text": "\\(A = U\\Sigma V^*\\)\n考虑向量 \\(\\begin{bmatrix} V_1 & V_2 \\end{bmatrix}\\)，其为一组正交的向量，如果其在经历空间变换 \\(M\\) 以后仍然映射为一组正交的向量 \\(\\begin{bmatrix} U_1 & U_2 \\end{bmatrix}\\) 。那么，我们直接在向量 \\(\\begin{bmatrix} U_1 & U_2 \\end{bmatrix}\\)的方向上选择一组基 \\(\\begin{bmatrix} u_1 & u_2 \\end{bmatrix}\\)，那么向量\\(\\begin{bmatrix} v_1 & v_2 \\end{bmatrix}\\)，在经历空间变化以后，在基\\(\\begin{bmatrix} u_1 & u_2 \\end{bmatrix}\\) 上的表示为，\\(\\begin{bmatrix} u_1 & u_2 \\end{bmatrix} * \\begin{bmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{bmatrix}\\)。即：\n\\[\nM[v_1, v_2] = [u_1, u_2] * \\begin{bmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{bmatrix} \\rightarrow\nMV = U\\Sigma \\rightarrow  M = U\\Sigma V^T\n\\]\n如此，对于奇异值分解，我们有如下的直观理解：对于矩阵\\(M\\),其对一组向量\\(V\\)在变换以后仍然为正交的向量。在进行变换的时候考虑直接转换到向量\\(V\\)的视角之下，然后进行缩放\\(\\Sigma\\)，以及其它变换\\(E\\)（旋转，投影），最后变换回原来的视角。即：\n\\[\nM = VE\\Sigma V^T \\rightarrow M = U\\Sigma V^T\n\\]\n可以看到奇异值与特征值分解的区别在于，选择的视角不同。当选择特征向量视角时，变换只会有缩放即\\(\\Sigma\\)变换，当选择任意的正交向量视角时，变换不仅包含缩放还含有旋转以及投影等即\\(E\\Sigma\\)。\n需要注意的是，虽然对于特征分解以及奇异值分解从视角转换的角度去解释了。但是，矩阵分解同样可以从空间变换的角度理解。例如对于奇异值分解有如下解释：\n第一个变换\\(V^{T}\\)将单位正交向量\\(v_1, v_2\\)转到水平和垂直方向、\\(\\Sigma\\)相当于对\\(v_1, v_2\\)进行放缩、\\(U\\)将放缩后的向量旋转到最后的位置。"
  },
  {
    "objectID": "note/2024-07-01-linear-algebra/index.html#线性降维",
    "href": "note/2024-07-01-linear-algebra/index.html#线性降维",
    "title": "Linear Algebra",
    "section": "线性降维",
    "text": "线性降维\n线性降维方法主要就是PCA(principal componet analysis)以及SVD(sigular value decomposition),以及PCA的扩展MDS(Multtidimensional Scaling)。PCA与MDS的差别在于PCA考虑的是样本的特征，寻求在低维空间下保留方差较大的特征的信息，所以通过对特征之间的协方差矩阵进行特征值分解矩阵，得到在低维空间下能够保留方差较大特征的正交基，是对特征的线性组合，而MDS考虑的是样本之间的相似度矩阵，通过对相似度矩阵矩阵进行特征分解找到在低维空间下能够保留样本最大距离的正交基。\n\nPCA\n样本矩阵\\(m*n\\)，\\(n\\)个样本，\\(m\\)个特征。\n\n计算样本矩阵协方差矩阵。\n协方差矩阵特征分解，找到使得协方差最大的方向。\n特征向量单位化(使得其长度为1)，特征向量与样本做点积，因为特征向量长度为1，其点积就是样本在特征向量方向上的投影长度，代表样本在这个方向上的坐标，选择多少个特征向量就为降到多少维。\n\n\n\nMDS\nMDS是一类基于样本距离进行映射的方法。MDS针对不同类型的数据有不同的方法。PCoA是MDS针对于数值数据分析的方法。\n\n\nPCoA\n考虑一个样本的dissimilarity matrix,也就是一个含有样本之间距离或不相似度量的矩阵,记为D。\n\nThe Torgerson method\n\n首先对D进行double centering得到double-centered matrix,记为B。然后对矩阵B进行奇异值分解。\ndouble centering1): \\[\nD^2 = \\left[d_{ij}^2 \\right]\n\\] double centering2): \\[\nB = -\\frac{1}{2}CD^{2}C\n\\] 其中\\(C = I - \\frac{1}{n}J_n\\)\n\nThe iterative method\n该方法更加实用，可以用于非欧式距离矩阵。\n\n\\[\nStress_D(x_1, x_2, ..., x_N) = \\left(\\sum_{i\\neq j\\neq 1, ..., N} \\left(d_{ij} - ||x_i - x_j|| \\right)^2 \\right)^{\\frac{1}{2}}\n\\]"
  },
  {
    "objectID": "note/2024-10-12-survivial-analysis/index.html",
    "href": "note/2024-10-12-survivial-analysis/index.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Current Life Table\n现时寿命表是加上同时出生的一代人，按照某年某地实际人口年龄别死亡率计算出的年龄别死亡率计算出的年龄别死亡率陆续死亡，直到全部人口死完为止，用寿命表的方法计算出这一代人在不同年龄组的“死亡人数”和刚满某年龄时的“尚存人数”以及“预期寿命”等寿命表指标。\n每一组是是左闭右开的区间 \\([x, x + n)\\). 考虑从横断面数据获得以下指标：\n\n\\(x\\): exact age.\n\\(_{n}P_x\\): \\(x\\) 岁组的平均人口数, \\(x\\) 表示年龄组下限，\\(n\\) 表示组距。\n\\(_{n}D_x\\): \\(x\\) 岁组实际死亡人数。\n\\(_{n}m_x\\): \\(\\frac{_{n}D_x}{_{n}P_x}\\).\n\n假定同时出生的一代人为10万人，那么其初期人数 \\(l_0 = 100000\\). 则有：\n\n0岁组年龄别死亡概率为 \\(q_0 = 婴儿死亡率 = \\frac{某年婴儿死亡数}{某年活产数}\\)。中间组年龄别死亡概率 \\(_{n}q_x = \\frac{2n \\cdot {_{n}m_x}}{2 + {n \\cdot _{n}m_x}}\\)。最后一组死亡概率为1.\nperson-year of survival: \\(L_0 = l_1 + a_0 \\cdot d_0\\); 中间组 \\(_{n}L_x = \\frac{l_x + l_{x+n}}{2} \\cdot n\\); 最后一组 \\(L_{\\omega} = \\frac{l_\\omega}{m_\\omega}\\)\n\n由初期的10万人，以及每个组的年龄别死亡概率可计算得到每个组的理论死亡人数，进而得到每个组的初期人数 \\(l_{x+n} = l_x - d_x\\) ；由此，可计算得到每个组的人年数，从最后一组开始累加即得到每个组生存人总年数 \\(T_x = \\sum{_{n}L_x}\\), 进而求得 预期寿命 \\(e_x = \\frac{T_x}{l_x}\\).\n去死因寿命表与一般寿命表的差异在于其根据去死因生存概率计算每组的期初人数。\n\n\\(_{n}r_x^{-i} = \\frac{_{n}D_x - _{n}D_x^{-i}}{_{n}D_x}\\).\n\\(_{n}P_x^{-i} = (_{n}P_x)^{_{n}r_{x}^{-i}}\\).\n\\(l_{x + n}^{-i} = l_x^{-i} \\cdot _{n}p_{x}^{-i}\\).\n\\(_{n}d_{x}^{-i} = l_x^{-i} - l_{n+x}^{-i}\\)\n\n\n\nSurvival Analysis\n直接观察满 \\(n\\) 年的人数中活满 n 年的比率。其计算公式为：\n\\(n年生存率 = \\frac{活满n年的人数}{观察满n年的人数} \\cdot 100 \\%\\)\n考虑 \\(x, L_x, W_x, D_x\\) 分别为随访年数，期初观察人数，期内失访人数，期内死亡人数，那么有：\n\n\\(L_{x + 1} = L_x - W_x - D_x\\).\n\\(N_x = L_x - \\frac{W_x}{2}\\). 校正人数，该公式假设每个失访者在这个区间内被随访的时间是区间长度的一半，那么 \\(N_x\\) 相当于是实际随访的人年数。\n\\(q_x = \\frac{D_x}{N_x}\\). 死亡概率，表示活满 \\(x\\) 年的人在第二年死亡的概率。\n\\(p_x = 1 - q_x\\). 生存概率，表示活满 \\(x\\) 年的人再活一年的概率。\n\n那么，其 \\(n\\) 年生存率：\n\\[\n\\begin{aligned}\n_{n}P_{0} =& \\prod_{x = 0}^{n-1} p_x \\\\\nS_{_{n}P_{0}} =& _{n}P_{0} \\cdot \\sqrt{\\sum_{i = 0}^{n-1}{\\frac{q_i}{p_i \\cdot N_i}}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html",
    "title": "Highlights of Calculus",
    "section": "",
    "text": "Danger case:\n\\[\n\\begin{aligned}\n\\infty - \\infty \\\\\n0 \\cdot \\infty \\\\\n\\frac{0}{0} \\\\\n0^0 \\text{ or } 1^\\infty\n\\end{aligned}\n\\]\nL’Hospital Rule:\n\\[\n\\frac{f(x)}{g(x)} \\rightarrow \\frac{\\frac{\\Delta{f}}{\\Delta{x}}}{\\frac{\\Delta{g}}{\\Delta{x}}} \\rightarrow \\frac{f'}{g'}\n\\]\nFor any small \\(\\epsilon\\) chosen, we can find \\(\\delta  &gt; 0\\), so that if \\(|f(x) - f(a)| &lt; \\epsilon\\), then \\(|f(x) - f(a)| &lt; \\delta\\)"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#exponential-function",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#exponential-function",
    "title": "Highlights of Calculus",
    "section": "Exponential Function",
    "text": "Exponential Function\nKey: Which function’s derivatives are equal to the function itself?\n\\[\n\\frac{df}{dx} = y \\rightarrow \\text{first differential equation}\n\\]\nConstruction:\n\\[\n\\begin{aligned}\ny(x) = 1 + x + \\frac{1}{2}x^2 + \\frac{1}{3 \\cdot{2} \\cdot{1}}x^3 + \\dots + \\frac{1}{n!}x^n + \\dots \\\\\n\\frac{df}{dx} =  1 + x + \\frac{1}{2}x^2 + \\frac{1}{3 \\cdot{2} \\cdot{1}}x^3 + \\dots + \\frac{1}{n!}x^n + \\dots\n\\end{aligned}\n\\]\n这里思想在于当 \\(\\text{when} \\space x = 0, e^x = 1\\), 那么其导数也为 \\(1\\); 导数为 \\(1\\)，原函数为什么其导数才为 \\(1\\) 呢？如此反复迭代；显然当 \\(n \\rightarrow \\infty\\)， 两式才相等。该级数称之为指数级数。\n\\[\ne^x =  1 + x + \\frac{1}{2}x^2 + \\frac{1}{3 \\cdot{2} \\cdot{1}}x^3 + \\dots + \\frac{1}{n!}x^n + \\dots\n\\]\nset \\(x = 0\\), \\(e = 1 + 1 + \\dots = 2.71828... \\rightarrow \\text{Euler's Number}\\)\n用指数级数可证明指数函数下面的性质\n\\[\ne^{a} \\cdot e^{b} = e^{a + b}\n\\]\nEuler’s Number 也可以通过如下方式计算得到\n\\[\ne = (1 + \\frac{1}{N + 1})^N, \\text{When}  \\space N \\rightarrow \\infty\n\\]\n对于该式子的展开基于二项式定理(Binomial Theorem).\n\\[\n\\frac{dy}{dx} = y\n\\]\n\\[\ny = f(x) = 1 + x + \\frac{1}{2}x^2 + ... + \\frac{1}{n!}x^n + ... = e\n\\]"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#trigonometric-function",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#trigonometric-function",
    "title": "Highlights of Calculus",
    "section": "Trigonometric Function",
    "text": "Trigonometric Function\n三角函数起源于勾股定理\n\\[\n\\begin{aligned}\na^2 + b^2 &= c^2 \\\\\n(\\frac{a}{c})^2 + (\\frac{b}{c})^2 &= 1\\\\\n(\\sin{\\theta})^2 + (\\cos{\\theta})^2 &= 1\n\\end{aligned}\n\\]\n三角函数求导关键在于用半径为1的圆描述周期运动，以及其中的三角形。\n下面给两个重要的极限\n\\[\n\\begin{aligned}\n\\sin{\\theta} &&lt; \\theta  \\rightarrow \\frac{\\sin{\\theta}}{\\theta} &lt; 1 \\\\\n\\tan{\\theta} &&gt; \\theta \\rightarrow \\frac{\\sin{\\theta}}{\\theta} &gt; \\cos{\\theta} \\\\\n\\frac{\\sin{\\theta}}{\\theta} &= 1, \\text{when} \\space \\theta \\rightarrow 0\n\\end{aligned}\n\\]\n前两个式子可由弧度制的弧长和面积证明，该极限可认为是 \\(\\sin{0}\\) 处的导数, 由上面两个式子夹逼准则定义。\n下面给出另一个重要的极限。\n\\[\n\\frac{\\cos{\\theta} - 1}{\\theta}  = 1, \\text{when} \\space \\theta \\rightarrow 0\n\\]\n该极限可认为是 \\(\\cos{0}\\) 处的导数。\n\\[\n\\begin{aligned}\n\\frac{\\Delta{\\sin{x}}}{\\Delta{x}} &= \\frac{\\sin{(x + \\Delta{x})} - \\sin {x}}{\\Delta{x}} \\\\\n&= \\frac{\\sin{x}(\\cos{\\Delta{x} - 1})}{\\Delta{x}} + \\frac{\\sin\\Delta{x} \\cos{x}}{\\Delta{x}} \\\\\n&= \\cos{x}\n\\end{aligned}\n\\]\n仿照上例子可得到 \\(\\cos{\\theta}\\) 的导数；下面不加证明地给出 \\(\\cos{x}\\) 的导数\n\\[\n\\frac{d\\cos{x}}{dx} = - \\sin{x}\n\\]"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#product-rule-quotient-rule-derivaitives-to-power-function",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#product-rule-quotient-rule-derivaitives-to-power-function",
    "title": "Highlights of Calculus",
    "section": "Product Rule, Quotient Rule, Derivaitives to Power Function",
    "text": "Product Rule, Quotient Rule, Derivaitives to Power Function\n\\(q(x) = f(x)g(x)\\)\n考虑边长分别为 \\(f(x), g(x)\\), 的长方形，当两边分别改变 \\(\\Delta x\\)， 其面积的变化：\n\\[\n\\Delta\\text{area} = f(x)g(x + \\Delta{x}) - g(x)) + g(x)(f(x + \\Delta{x} - f(x))) + \\Delta{x}^2\n\\]\nWhen \\(\\Delta{x} \\rightarrow 0\\),\n\\[\n\\begin{aligned}\ndq &= f(x)dg + g(x)df \\\\\n\\frac{dq}{dx} &= f(x)\\frac{dg}{dx} + g(x)\\frac{df}{dx}\n\\end{aligned}\n\\]\nQuation rule 可由乘法法则推导得到。\n\\[\n\\frac{f(x)}{g(x)} = \\frac{f(x)g' - g(x)f'}{g(x)^2}\n\\]"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#chain-rule",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#chain-rule",
    "title": "Highlights of Calculus",
    "section": "Chain Rule",
    "text": "Chain Rule\n\\[\nf'(y(x)) = \\frac{df}{dx} = \\frac{df}{dy}\\frac{dy}{dx}\n\\]\n对于偶函数，其导数为奇函数。对于奇函数，其导数为偶函数。\n\\[\ny = f(x) \\rightarrow x = f^{-1}(y)\n\\]\n需要注意的是只有在单调区间内，才有逆函数，且 \\(f\\) 与 \\(f^{-1}\\) 的函数图像关于原点对称。"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#logarithmic-function",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#logarithmic-function",
    "title": "Highlights of Calculus",
    "section": "Logarithmic Function",
    "text": "Logarithmic Function\n指数函数的逆函数为对数函数，其求的是指数的值。\n\\[\nx = \\ln{y}\n\\]\n其具有如下性质\n\\[\n\\begin{aligned}\n\\ln{ab} &= \\ln{a} + \\ln{b} \\\\\n\\ln{y^n} &= n\\ln{y}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#derivatives-for-lnx-sin-1x-cos-1x",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#derivatives-for-lnx-sin-1x-cos-1x",
    "title": "Highlights of Calculus",
    "section": "Derivatives for \\(\\ln{x}, \\sin^{-1}{x}, \\cos^{-1}{x}\\)",
    "text": "Derivatives for \\(\\ln{x}, \\sin^{-1}{x}, \\cos^{-1}{x}\\)\nset\n\\[\n\\begin{aligned}\ny &= e^x \\\\\nx &= \\ln{y}\n\\end{aligned}\n\\]\nThen \\[\n\\begin{aligned}\ny = e^x \\rightarrow e^{\\ln{y}} = y \\\\\ne^{\\ln{y}} \\cdot \\frac{d\\ln{y}}{dy} = 1, \\text{Where} \\space e^{\\ln{y}} = y\\\\\n\\end{aligned}\n\\]\nset\n\\[\n\\begin{aligned}\ny &= \\sin{x} \\\\\nx &= \\sin^{-1}{y}\n\\end{aligned}\n\\]\nThen\n\\[\n\\begin{aligned}\n\\sin{\\sin^{-1}{y}}  &= y\\\\\n\\cos{\\sin^{-1}{y}} \\cdot \\frac{d \\sin^{-1}y}{y} &= 1, \\text{Where} \\cos{\\sin^{-1}{y}} = \\frac{1}{\\sqrt{1 - y^2}}\n\\end{aligned}\n\\]\nNote that the \\(\\sin^{-1}y\\) is an angle.\nGive the \\(\\frac{d\\cos^{-1}y}{dy}\\) without proof.\n\\[\n\\frac{d\\cos^{-1}y}{dy} = -\\frac{1}{\\sqrt{1 - y^2}}\n\\]\nNote that:\n\\[\n\\frac{d\\cos^{-1}y}{dy}  + \\frac{d\\sin^{-1}y}{dy} = 0\n\\]\nWhere \\(\\theta + \\alpha = \\frac{\\pi}{2}\\) is a constant.\nSome other deritivites:\n\\[\n\\begin{aligned}\n\\frac{d\\arctan{x}}{x} = \\frac{1}{1 + x^2} \\\\\n\\frac{d \\space \\text{acrcot} \\space {x}}{x} = -\\frac{1}{1 + x^2} \\\\\n\\frac{d{a^x}}{x} = a^{x} \\ln{a}\n\\end{aligned}\n\\]\nConverion between different base. \\[\n\\begin{aligned}\n\\log_a{|x|} &= \\frac{1}{x\\ln{a}} \\\\\n\\log_{a}{b} &= \\frac{\\ln{b}}{\\ln{a}} = \\frac{\\log_n{b}}{\\log_n{a}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#differential-equations-of-motion",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#differential-equations-of-motion",
    "title": "Highlights of Calculus",
    "section": "Differential Equations of Motion",
    "text": "Differential Equations of Motion\nLinear, and Second order equation.\n\\[\nm\\frac{d^2y}{dt^2} + 2r\\frac{dy}{dt} + ky = 0\n\\]\nWhen \\(m = 0\\)\n\\[\n\\frac{dy}{dt} = ay \\rightarrow y = ce^{at}\n\\]\nWhen \\(r = 0\\)\n\\[\n\\frac{d^2y}{dt^2} = \\frac{k}{m}y  = -\\omega^2y \\rightarrow y = C\\cos{\\omega{t}} + D\\sin{\\omega{t}}\n\\]\nWhen \\(m = r = 0\\)\n\\[\n\\frac{d^2y}{dt^2} = 0 \\rightarrow y = C + Dt\n\\]\nGeneral solutaion - Try \\(y = e^{\\lambda{t}}\\)\n\\[\nm\\lambda^2 + 2r\\lambda + K = 0\n\\]\nThree Cases:\n\\[\n\\begin{aligned}\ny'' + 6y' + 8y = 0 &\\rightarrow y(t) = Ce^{-2t} + De^{-4t} \\\\\ny'' + 6y' + 10y = 0 &\\rightarrow y(t) = Ce^{(-3 - i)t} + De^{(-3 + i)t} \\\\\ny'' + 6y' + 9 = 0 &\\rightarrow y(t) = Ce^{-3t} + Dte^{-3t}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "note/2024-08-04-Hilights-of-calculas/index.html#differential-equations-of-growth",
    "href": "note/2024-08-04-Hilights-of-calculas/index.html#differential-equations-of-growth",
    "title": "Highlights of Calculus",
    "section": "Differential Equations of Growth",
    "text": "Differential Equations of Growth\nThe growth rate proportional to itself. \\[\n\\begin{aligned}\n\\frac{dy}{dt} &= cy \\\\\ny(0) &\\rightarrow \\text{Given start} \\\\\ny(t) &= y(0)e^{ct}\n\\end{aligned}\n\\]\nAdd source term:\n\\[\n\\begin{aligned}\n\\frac{dy}{dt} &= cy + s \\space \\text{Where} \\space s \\space \\text{is source term} \\\\\n\\frac{d{(y + \\frac{s}{c}})}{dt} &= c(y + \\frac{s}{c}) \\\\\ny + \\frac{s}{c} &= (y(0) + \\frac{s}{c})e^{ct}\n\\end{aligned}\n\\]\nFor Linear eq, the solutions to eq have form below\n\\[\ny(t) = y_{\\text{particular}}(t) + y_{\\text{right side 0}}(t)\n\\]\nSpecially for \\(\\frac{dy}{dt} = cy + s\\)\n\\[\n\\begin{aligned}\ny_{\\text{particular}} = -\\frac{s}{c} \\\\\ny_{\\text{set s = 0}} = Ae^{ct}\n\\end{aligned}\n\\]\nThen\n\\[\ny = -\\frac{s}{c} + Ae^{ct}\n\\]\nTo find \\(A\\), put \\(t = 0\\), \\(y(0) = \\frac{s}{c} + A\\)\nNon-linear equation for population:\n\\[\n\\frac{dp}{dt} = cp - sp^2\n\\]\nTo solve this equation, set \\(y = \\frac{1}{p}\\) to turn this equation to linear equation.\nEquation for predators and prey\n\\[\n\\begin{aligned}\n\\frac{du}{dt} &= - cu + suv \\\\\n\\frac{dv}{dt} &= cv - suv\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "note/2022-06-25-rna-seq/index.html",
    "href": "note/2022-06-25-rna-seq/index.html",
    "title": "RNA Seq",
    "section": "",
    "text": "组学分析包括基因组学(全外显子组学，全基因组学)，转录组学（有参，无参），表观组学（chip-seq，甲基化等）。\nRNA-seq的重要在于，很多情况下疾病的原因是蛋白的原因，而蛋白是由mRNA翻译而来，通过RNA我们可以了解到究竟是哪些蛋白发生发生了变化，或者转录出现了问题。对于组学分析而言，你需要知道哪些物种是已经被测序的，对于已测序的动物，你可以在Ensembl这个网站找到，植物在Ensembl Plant查询。\n基因组包括细胞核基因组以及细胞质基因组（主要是线粒体），人基因组大概3.1Gbp，其中coding- area 1.1%，RNA genes以及 regulatory area 3%。线粒体16.6Kb。\n基因在染色上分布不均匀。人类大概有20000蛋白基因，7000RNA基因，6号染色体基因数目最多。\nrRNA 18 / 28 5.8 5 - 40/ 60。可以从NCBI下载到rRNA序列信息。\ntRNA mapping时的参考序列需要加CCA, 组氨酸的参考序列+ G。有的tRNA含有内含子。可以从GtRNAdb下载tRNA序列\nmRNA 序列信息可以从Uniprot下载。\nmicroRNA 可以从miRbase中下载。\n数量：rRNA(80-90%) tRNA(10%)，mRNA(1-5%)，直接测序大部分都是tRNA，提取mRNA(通过其poly A)，或者去除tRNA再测序。"
  },
  {
    "objectID": "note/2022-06-25-rna-seq/index.html#序-生物背景",
    "href": "note/2022-06-25-rna-seq/index.html#序-生物背景",
    "title": "RNA Seq",
    "section": "",
    "text": "组学分析包括基因组学(全外显子组学，全基因组学)，转录组学（有参，无参），表观组学（chip-seq，甲基化等）。\nRNA-seq的重要在于，很多情况下疾病的原因是蛋白的原因，而蛋白是由mRNA翻译而来，通过RNA我们可以了解到究竟是哪些蛋白发生发生了变化，或者转录出现了问题。对于组学分析而言，你需要知道哪些物种是已经被测序的，对于已测序的动物，你可以在Ensembl这个网站找到，植物在Ensembl Plant查询。\n基因组包括细胞核基因组以及细胞质基因组（主要是线粒体），人基因组大概3.1Gbp，其中coding- area 1.1%，RNA genes以及 regulatory area 3%。线粒体16.6Kb。\n基因在染色上分布不均匀。人类大概有20000蛋白基因，7000RNA基因，6号染色体基因数目最多。\nrRNA 18 / 28 5.8 5 - 40/ 60。可以从NCBI下载到rRNA序列信息。\ntRNA mapping时的参考序列需要加CCA, 组氨酸的参考序列+ G。有的tRNA含有内含子。可以从GtRNAdb下载tRNA序列\nmRNA 序列信息可以从Uniprot下载。\nmicroRNA 可以从miRbase中下载。\n数量：rRNA(80-90%) tRNA(10%)，mRNA(1-5%)，直接测序大部分都是tRNA，提取mRNA(通过其poly A)，或者去除tRNA再测序。"
  },
  {
    "objectID": "note/2022-06-25-rna-seq/index.html#实验篇",
    "href": "note/2022-06-25-rna-seq/index.html#实验篇",
    "title": "RNA Seq",
    "section": "实验篇",
    "text": "实验篇\n\nRNA 测序文库\nmRNA 建库\n\nmRNA or total RNA\nremove contaminant DNA (remove rRNA or select mRNA?)\nfragment RNA\nreverse transcrible into cDNA（strand specefic?)\nligate sequence adaptors\n\n建库时的几个问题。\n\n是去除tRNA还是特异性保留mRNA？\npoly A+ RNA-seq方法通过与mRNA的poly A尾巴特异性结合选择mRNA(无法区分正负链，基因overlap ，正负链都可能是基因)，但是组蛋白RNA没有poly A尾巴。\nrRNA - RNA-seq方法通过酶降解tRNA。\n两种建库方式的原始数据还是有差异的。\n如何保留链特异性？\n通过dUTP method，加入dUTP合成，切割该链条。\n\n\n\n测序技术\nillumina测序原理可以参考这里。\nreads1/reads2（/ - 互补） + adapater = fragment\nreads1/reads)（/ - 互补) = insert distance\n\nillumina测不长的原因？\n每测一轮都有可能有错误（同一簇不同步），后面越来越差，杂色参杂。\n片段为什么要均一？\n短片段更容易在flow cell 结合，导致测出来的都是短片段\n为什么需要保持碱基平衡？\n150次flow cell快照，每簇都同一色不好解方程。\nadapter序列出现在结果中？ insert短，没有150bp（只会出现在3端），fasta从左到右边，只有右边有接头；测序是从5端到3端进行测序。\n\n\n\n实验数据质控\ntotal RNA提取的质控标准：RIN(RNA Integrity Number)，根据5S, 18, 28S的峰值进行评估，范围0-10。（6-6.5，7）"
  },
  {
    "objectID": "note/2022-06-25-rna-seq/index.html#rnaseq上游分析篇",
    "href": "note/2022-06-25-rna-seq/index.html#rnaseq上游分析篇",
    "title": "RNA Seq",
    "section": "RNASeq上游分析篇",
    "text": "RNASeq上游分析篇\n\nQuality Control\n质控主要需要考虑一下几个方面：\n\n去除adapter\n去除低质量reads\n去除reads部分低质量区域\n为下一步分析做准备；例如研究可变剪切，需要把reads修剪成等长。\n\n关于fasta文件：\n第一行，@reads名字（测序仪编号:lane:tail:x:y)\n第二行，序列\n第三行，+（reads名字/没有）\n第四行，phred value + 33 对应的ASSCI编码\n\\(Phred(\"F\") = 70 - 33 = 37\\)(Sanger 标准)\n\\(Phred = -10 * log_{10}(error\\ probability)\\)\nPhred40 = 0.0001 error rate\nPhred30 = 0.001 error rate\nPhred20 = 0.01 error rate\nPhred10 = 0.1 error rate\n质检报告html图表:\nper base quality：总reads每个位置碱基的犯错概率（下限q30）。\nper tile sequence quality：如果有花的，那么整个tile可能都有问题。\nGCcontent：不同物种GC含量不一样，样品可能被污染。\nper base N content: 杂色，未测出碱基是什么。\nsequence duplicate level：重复reads展示。\nadapter content：序列的接头检测\n质控参数：\nquality -10\n\nPhred - 10\n数据从右向左累加\n在累加的最小值处进行切割，保留前面的。\n\n\n概览\n\nFASTQ(QC, mapping-找reads在基因组的位置)- SAM（sequence alignment)- BAM（压缩的sam文件）\nbam-质控（比对情况），计数，标准化，找差异表达\n改进：翻译效率的问题，降解未结合核糖体的mRNA区域。 RNA结合蛋白鉴定：fastq-bam-chip-seq 待写。\n\n\nmapping\n\nalignment（低通量，两条/多序列） pairwise multiple\npairwise: 全局，局部（needleman-wunch, smith-waterman）\nblast: one vs many 序列切短，相似再扩展。\nmapping（reads 回溯基因组） many vs one(bwt算法-bwa bowtie)\n\nmapping回成熟的mRNA参考序列，不用处理可变剪切，不能发现新的转录本。\nmapping到参考基因组，可以发现新的转录本，进行isoform层次的定量，但是不能使用之前的DNA mapping软件。\n\n\nmapping的可变剪切问题：\n\nexon-first approach(pseudogene-mRNA逆转录cDNA插入基因组，贴到假基因区)\nseed-extend approch\npotential limitations of exon-first approches。\n\n参考基因组(序列）下载：\nUCSC genome broswer/download/human/sequence data by chromsome\n合并染色体序列信息：\nchr2.fa.gz cat chr1.fa.gz chr2.fa.gz &gt; ref_hg38.fa\n&gt;chr1\nNNNNNNNNNN(端粒的占位符)\nATCGGGGGGGG\n&gt;chr2\nNNNNNNNNNN\nATCGGGGGGGG\nEnsembl: ensembl species list/human/gene assemble/download DNA sequence\nNCBI: refseq/\n参考基因注释文件（GTF,GFF-序列哪些位置是基因等)：\nussc/tools/table browser\n注释文件每列含义：待写\nNote：注释文件和参考基因组match\nBWT(burrows wheeler transform)\n假定：ref- ACAACG\nACAACG& 循环\n\n&ACAACG\nG&ACAAC\nCG&ACAA\nACG&ACA\nAACG&AC\nCAACG&A\n得到的字符矩阵根据第一列首字母排序（&ACGT)，得到排序后的字符矩阵。\nindex就是排序的字符矩阵的最后一列\n\n排序后的字符矩阵有下面两个性质：\n\n每一行的第一个字符和最后一个字符一定在原始字符串的相连；且后一个字符在序列中在第一字符前。\n第一列和最后一列字母（例如同一个A..)的相对次序不变；也就是第一列中第一个A和最后一列的第一个A实际上是序列中的同一个A。\n\nIndex在mapping中的作用如下：\n\n根据index还原排序后的字符矩阵矩阵的第一列\n根据上面两条性质反推出reference。\n根据排序矩阵第一列和最后一列比对，从序列后面，矩阵第一列开始搜索\n\n后缀树算法- suffix tree\n\n同bwt得到未排序的字符矩阵。\n按第一列首字符排序得到排序的字符矩阵。\n构建suffix tree（存储树信息以及相对位置 信息，index索引非常大）\n从树的根节点出发。\n\n实际上后缀树存储了序列某个字符后的所有可能性，例如某一字符为A，后缀树就穷举了序列中A后面所有字符的可能并保存下来，这种做法就是用空间换时间。\n一些mapping软件：\ntophat/tophat2 构建索引： bowtie2-build --threds 6 ref_hg38.fa ref_hg38.fa\nstar-基于后缀树\n\nreads切成小的seed，找到seed位置\n符合的seed拼一起\n\nhisat2(tophat的升级版) - 全基因组Index，切割为55000份建立index。先定位在哪个小的index，然后在短的index搜索。（两层的bwt结构；其优点在于考虑了SNP信息（mapping时可以替换）\n构建hisat2 index： SNP信息：dbSNP commom（ucsc genome/annotation/sql/common.txt.gz)\n可变剪切信息：GTF\n参考基因组信息：Genome FASTA\nsam flag信息\nsamtool sort PG bam（哪些操作）\nbam 文件建立索引方便查看mapping信息。 samtool index，任意一段 samtool view -h\n\n\nRNA seq 定量\n\n通过参考基因组进行定量\n样本内标准方法\n除以测序深度（reads数目），基因长度（bp)，进行单位化，分子乘以\\(10^6\\)（每百万reads)， 乘以\\(10^3\\)(基因长度1000bp)，1单位rpkm含义就是每百万reads中长度为1kb的基因的reads数目为1。\nr-reads, f-fragments, p-per, k-kilobase, m-million。\nRPM or CPM：仅对测序深度进行单位化。 \\[\n\\frac{Number\\ of\\ reads\\ mapped\\ to\\ gene *10^6}{Total\\ number\\ of\\ mapped\\ reads}\n\\] RPKM, FPKM：同时测序深度以及基因长度进行单位化。对于双端测序，fragment就是同一对reads。单端测序FPKM等于RPKM。 \\[\n\\frac{Number\\ of\\ reads\\ mapped\\ to\\ gene* 10^6 *10^3}{Total\\ number\\ of\\ mapped\\ reads* gene\\ length\\ in\\ bp}\n\\]\nTPM:\nTPM假设每个样本的基因数值加和相同，其简单的为样本内基因Fpkm 的百分数*\\(10^6\\)。\n样本间标准化\n直接计算比例： \\[\nC_j = \\frac{10^6}{D_j}\n\\] 其中\\(D_j\\)为样本\\(j\\)测序深度，\\(C_j\\)为样本\\(j\\)校正系数。这种方法容易受到极端值的影响。\nquantile： \\[\nC_j = \\frac{exp\\left( \\frac{1}{N}\\sum_{l = 1}^{N}log(D_{l}Q_{l}^{(p)})\\right)}{D_{j}Q_{j}^{(p)}}\n\\]\n样本的分位数均值与某一个样本的分位数比值。其中\\(D_j\\)为样本\\(j\\)的测序深度，\\(Q_j^{(p)}\\)为样本\\(j\\)的\\(p\\)分位数，\\(C_j\\)为样本\\(j\\)的校正系数。\nRLE(relative log expression) - cufdiff；Deseq2默认方法：\n假定行为基因，列为样本。\n\n每行的几何平均数。\n每行除以该行的几何平均数，得到新矩阵。\n新矩阵每列的中位数就是该列样本的校正因子。\n\n\\[\nC_j = median_g\\left( \\frac{K_{gj}}{(\\prod_{l=1}^{N}K_{gl})^{\\frac{1}{N}}} \\right)\n\\]\nTMM(edge R) Trimmed mean of M-values：\n假设前提：total reads受高表达基因影响。大多数基因的表达量不变。\nRNA-seq的定量在MA plot反应为，1）大多数点应该贴近于M = 0该直线附近。2）高表达基因的应该贴近于M = 0该直线附近———横轴是按基因的几何平均由低（左）到高（右边）排布的。\nMA plot： X-A：两组样本的几何平均数 \\[\nA = \\frac{1}{2}log_2{(RG)}\n\\] M-Y：两组样本的fold change。 \\[\nM = log_2{(R/G)}\n\\]\n变化基因以及高表达的基因的阈值可选。\n\n\n\nRSEM转录水平定量\nreads直接mapping到mRNA上，解决可变剪切的问题。reads来自于哪个isoform?从极大似然估计到EM算法，可以得出每个isoform的count数目。"
  },
  {
    "objectID": "note/2022-06-25-rna-seq/index.html#rnaseq下游分析篇",
    "href": "note/2022-06-25-rna-seq/index.html#rnaseq下游分析篇",
    "title": "RNA Seq",
    "section": "RNASeq下游分析篇",
    "text": "RNASeq下游分析篇\n\n差异分析\n基因的定量是一个抽样的结果RNA-cDNA-RNA\nRNA-Seq的前提：\n\n绝大多数基因的表达量不变\n高表达基因的表达量不变\n如果需要绝对定量，使用提前绝对定量的内参（spike-ins)，ERCC countrol。\n\n对于cell line进行差异分析，需要2-3个repeat，对于生物体进行差异分析需要3-个repeat，对于群体而言，需要成百上千个repeate。\n\np-value校正：\n\np排序\\(\\rightarrow\\) 新p = p* 检验次数m \\(\\rightarrow\\)保持原有顺序（第一次排序的结果）调整顺序\nBH校正 ：p排序\\(\\rightarrow\\)新p = p * 检验次数 / 序号\\(\\rightarrow\\)保持原有顺序调整p值。\n\nRNA-Seq定量的分布模型\n\nRNA-Seq定量的二项分布：考虑一个基因，其它所有基因为一部分。那么基因A的出现次数二项分布。\nRNA-Seq定量的泊松分布：因为一个基因出现的概率很低，p很低，二项分布接近于泊松分布。\nRNA-Seq定量的负二项分布：\n实际上RNA-seq的数据并不服从泊松分布，泊松分布期望与方差相等，然而RNA-Seq图像是over-dispersion的。随均值增加，方差也变大。这种现象称为short noise。\n泊松分布： \\[\nE(K_g) = Var(K_g) = \\lambda\n\\]\nRNA-seq short noise现像通过对泊松分布的修正描述。\n\\[\nE(K_g) = \\lambda, \\ Var(K_g) = \\lambda + \\phi\\lambda^2\n\\]\n负二项分布具有此特性。所有RNA-seq基因的分布修正为负二项分布。\n差异检验的零假设： \\[\n\\lambda_{g}A = \\lambda_{g}B\n\\]\n问题变成了估计\\(\\lambda_g\\)，\\(\\phi_g\\)，这个过程叫做estimate dispersion，不同软件的估算方法不同。\n以前的统计检验思路，列联表卡方检验或fisher检验。基因同分布检验，fisher对大数字敏感，对小数字不敏感。\n\nRNA-seq的绝对定量\nERCC 建库加入定量的已知ERCC spike-in mRNA\nhouse kepping(3000-4000）基因定量 输入文件是 未经过校正的count数据，认为不变的基因list(spike in) 输出就是校正过的数据。"
  },
  {
    "objectID": "note/2022-06-25-rna-seq/index.html#基因注释与富集分析",
    "href": "note/2022-06-25-rna-seq/index.html#基因注释与富集分析",
    "title": "RNA Seq",
    "section": "基因注释与富集分析",
    "text": "基因注释与富集分析\n检验：GO kegg注释就是列联表的卡方检验。\n输入：一个感兴趣基因集，基因注释信息。\n输出：基因基是否在某一类注释信息中。\n需要认为设定感兴趣基因集的阈值。\nGESA：解决人为设定基因集的问题。 输入：全部的基因变化信息，一个感兴趣的通路的基因集合（GESA msi)。\n输出：是否与整个感兴趣的通路相关。\n图片认知（待写）\n非模式生物的富集分析：有参，无参。\nannotationhub。\n\n多样本数据分析\nTCGA是肿瘤数据库，可以通过firebrowser下载其中的数据。GTex是正常人的数据。\n关于pearson相关系数以及spearman相关系数：spearman相关系数仅仅是对数据的排序序号进行pearson相关分析的结果。\nWGCNA：只是简单对基因进行聚类，通过最终聚类效果的评价指标选取距离计算的指数。\n多样本差异表达基因：\n将负二项模型通过对数连接函数写作广义线性模型，并对其参数进行似然比检验：\n\\[\nlog\\mu_{gi} = x_{i}^{t}\\beta_g + logN_i\n\\]\n其中\\(log\\mu_{gi}\\)是基因\\(g\\)在样本\\(i\\)的观测值，\\(x_{i}^{t}\\)设计矩阵，\\(N_i\\)是样本的测序深度。差异检验就是对\\(\\beta_g\\)是否全为0的检验。"
  },
  {
    "objectID": "note.html",
    "href": "note.html",
    "title": "Note",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nCategories\n\n\nModified\n\n\n\n\n\n\nSurvival Analysis\n\n\n \n\n\n2024-11-08\n\n\n\n\nHighlights of Calculus\n\n\n \n\n\n2024-11-08\n\n\n\n\nLinear Algebra\n\n\n \n\n\n2024-11-08\n\n\n\n\nData Tricks\n\n\n \n\n\n2024-11-08\n\n\n\n\nHealth Statistics\n\n\n \n\n\n2024-11-08\n\n\n\n\nXgboost\n\n\n \n\n\n2024-11-08\n\n\n\n\nLiner Dimension Reduction\n\n\n \n\n\n2024-11-08\n\n\n\n\nStatistics\n\n\n \n\n\n2024-11-08\n\n\n\n\nCluster Algorithm\n\n\n \n\n\n2024-11-08\n\n\n\n\nRNA Seq\n\n\n \n\n\n2024-11-08\n\n\n\n\nProbability Distribution\n\n\n \n\n\n2024-11-08\n\n\n\n\n\nNo matching items"
  }
]